{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv2d, self).__init__()\n",
    "        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels)\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return x\n",
    "\n",
    "class MobileNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(MobileNetEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            # nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(8, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        return x1, x2, x3, x4, x5, x6\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, output_act = nn.Sigmoid(), find_threshold = False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.lane_threshold = 0.5\n",
    "        self.output_act = output_act\n",
    "        self.roc_flag = find_threshold\n",
    "        # Encoder\n",
    "        self.encoder = MobileNetEncoder(in_channels=n_channels)\n",
    "        \n",
    "        # Decoder\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.iconv5 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.decoder_block5_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block5_2 = nn.Sequential(\n",
    "            nn.Conv2d(128*2, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.iconv4 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "        self.decoder_block4_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block4_2 = nn.Sequential(\n",
    "            nn.Conv2d(64*2, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.iconv3 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.decoder_block3_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(32)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block3_2 = nn.Sequential(\n",
    "            nn.Conv2d(32*2, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.iconv2 = nn.Conv2d(32, 16, kernel_size=1)\n",
    "        self.decoder_block2_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block2_2 = nn.Sequential(\n",
    "            nn.Conv2d(16*2, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)\n",
    "        # print(self.upconv1.shape)\n",
    "        self.iconv1 = nn.Conv2d(16, 8, kernel_size=1)\n",
    "        self.decoder_block1_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(16, 8, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(8)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block1_2 = nn.Sequential(\n",
    "            nn.Conv2d(8*2, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.C2_layer = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=1), \n",
    "            nn.BatchNorm2d(256))\n",
    "        self.C3_layer = nn.Conv2d(8, 8, kernel_size=1)\n",
    "        self.C3_layer = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=1),\n",
    "            nn.BatchNorm2d(8))\n",
    "        self.output_layer = nn.Conv2d(8, n_classes, kernel_size=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape: \", x.shape)\n",
    "        x1, x2, x3, x4, x5, x6 = self.encoder(x)\n",
    "        # print(\"Encoder output shapes: \", x1.shape, x2.shape, x3.shape, x4.shape, x5.shape, x6.shape)\n",
    "        y6 = self.C2_layer(x6)\n",
    "        # print(\"C2 layer shape: \", y6.shape)\n",
    "        # UpSample\n",
    "        y5 = self.decoder_block5_1(y6)\n",
    "        # Concatenation with skip connection\n",
    "        y5 = torch.cat([x5, y5], dim=1)\n",
    "        # Conv2\n",
    "        y5 = self.decoder_block5_2(y5)\n",
    "        # print(\"1 - Decoder layer shape: \", y5.shape)\n",
    "        # UpSample\n",
    "        y4 = self.decoder_block4_1(y5)\n",
    "        # Concatenation with skip connection\n",
    "        y4 = torch.cat([x4, y4], dim=1)\n",
    "        # Conv2\n",
    "        y4 = self.decoder_block4_2(y4)\n",
    "        # print(\"2 - Decoder layer shape: \", y4.shape)\n",
    "        # UpSample\n",
    "        y3 = self.decoder_block3_1(y4)\n",
    "        # Concatenation with skip connection\n",
    "        y3 = torch.cat([x3, y3], dim=1)\n",
    "        # Conv2\n",
    "        y3 = self.decoder_block3_2(y3)\n",
    "        # print(\"3 - Decoder layer shape: \", y3.shape)\n",
    "        # UpSample\n",
    "        y2 = self.decoder_block2_1(y3)\n",
    "        # Concatenation with skip connection\n",
    "        y2 = torch.cat([x2, y2], dim=1)\n",
    "        # Conv2\n",
    "        y2 = self.decoder_block2_2(y2)\n",
    "        # print(\"4 - Decoder layer shape: \", y2.shape)\n",
    "\n",
    "        # UpSample\n",
    "        y1 = self.decoder_block1_1(y2)\n",
    "        # Concatenation with skip connection\n",
    "        y1 = torch.cat([x1, y1], dim=1)\n",
    "        # Conv2\n",
    "        y1 = self.decoder_block1_2(y1)\n",
    "        # print(\"5 - Decoder layer shape: \", y1.shape)\n",
    "\n",
    "        out = self.output_layer(y1)\n",
    "        # Training time\n",
    "        if self.training:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            return out, predictions\n",
    "            # return class_prob_masks\n",
    "        elif self.roc_flag and not self.training:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            return class_prob_masks\n",
    "        # Evaluation time\n",
    "        else:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            # Flatten the tensor into a 1D array\n",
    "            x_flat = class_prob_masks.detach().cpu().numpy().flatten()\n",
    "            # Plot a histogram of the tensor values\n",
    "            plt.hist(x_flat, bins=100)\n",
    "            plt.show()\n",
    "            # print(class_prob_masks)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            # return predictions\n",
    "            return predictions\n",
    "        # print(\"6 - Output layer shape: \", out.shape)\n",
    "        # return out\n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "import torchvision.transforms as transforms\n",
    "# from torch_poly_lr_decay import PolynomialLRDecay\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "import utils\n",
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.2, val_size= 0.01)\n",
    "# Create train and validation splits / Always use del dataset to free memory after this\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)\n",
    "print(pos_weight.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_img_pred(train_set[2][0], train_set[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the CNN pipeline with schedule and SGD optimizer\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.1, momentum=0.9, weight_decay=0.001, lr_scheduler=True, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        scheduler = PolynomialLRDecay(optimizer, max_decay_steps=100, end_learning_rate=0.0001, power=0.9)\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    train_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "    \n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        \n",
    "            \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = train_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)].to(device)\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "            \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "        \n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "        \n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 793249\n",
      "Epoch: 1 - Train Loss: 7.1729 - Train_IoU: 0.07658 - Train_F1: 0.14157\n",
      "Epoch: 2 - Train Loss: 5.5787 - Train_IoU: 0.10714 - Train_F1: 0.19340\n",
      "Epoch: 3 - Train Loss: 5.0263 - Train_IoU: 0.12023 - Train_F1: 0.21446\n",
      "Epoch: 4 - Train Loss: 4.8811 - Train_IoU: 0.12330 - Train_F1: 0.21932\n",
      "Epoch: 5 - Train Loss: 4.5605 - Train_IoU: 0.13127 - Train_F1: 0.23191\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=8, shuffle= True, drop_last= True)\n",
    "validation_loader = DataLoader(validation_set,batch_size=8, shuffle= True, drop_last= True)  \n",
    "model = UNet()\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "train(model, train_loader, val_loader = None, num_epochs= 5, lr_scheduler=False, lane_weight = pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Train Loss: 5.2155 - Learning Rate: 0.100000 - Train_IoU: 0.00332 - Train_F1: 0.00662\n",
      "Epoch: 2 - Train Loss: 4.9140 - Learning Rate: 0.100000 - Train_IoU: 0.00352 - Train_F1: 0.00702\n",
      "Epoch: 3 - Train Loss: 4.6149 - Learning Rate: 0.100000 - Train_IoU: 0.00373 - Train_F1: 0.00743\n",
      "Epoch: 4 - Train Loss: 4.5529 - Learning Rate: 0.100000 - Train_IoU: 0.00375 - Train_F1: 0.00748\n",
      "Epoch: 5 - Train Loss: 4.2586 - Learning Rate: 0.100000 - Train_IoU: 0.00403 - Train_F1: 0.00802\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader,val_loader= validation_loader,num_epochs= 5, lane_weight = pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD SAVED MODEL FROM COLAB\n",
    "model.load_state_dict(torch.load('CNN_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1969800889492035\n"
     ]
    }
   ],
   "source": [
    "model.lane_threshold = 0.1969800889492035\n",
    "print(model.lane_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n",
      "torch.Size([1, 3, 640, 640])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "model.to(device='cuda').eval()\n",
    "img_tns, gt = validation_set[1]\n",
    "print(img_tns.shape)\n",
    "img_tns_ = img_tns.unsqueeze(0)\n",
    "print(img_tns_.shape)\n",
    "img_tns_ = img_tns_.to('cuda')\n",
    "pred_mask = model(img_tns_)\n",
    "print(pred_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640)\n",
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "pred_mask = torch.squeeze(pred_mask)\n",
    "pred_mask = utils.toImagearr(pred_mask)\n",
    "img_tns = utils.toImagearr(img_tns)\n",
    "print(pred_mask.shape)\n",
    "print(img_tns.shape)\n",
    "utils.disp_img(image = img_tns, name = 'Original Image')\n",
    "utils.disp_img(image = pred_mask, name = 'Original Image')\n",
    "# img_tns.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
