{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv2d, self).__init__()\n",
    "        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels)\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return x\n",
    "\n",
    "class MobileNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(MobileNetEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(8, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        return x1, x2, x3, x4, x5, x6\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=2):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = MobileNetEncoder(in_channels=n_channels)\n",
    "        \n",
    "        # Decoder\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.iconv5 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.decoder_block5_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block5_2 = nn.Sequential(\n",
    "            nn.Conv2d(128*2, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.iconv4 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "        self.decoder_block4_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block4_2 = nn.Sequential(\n",
    "            nn.Conv2d(64*2, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.iconv3 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.decoder_block3_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block3_2 = nn.Sequential(\n",
    "            nn.Conv2d(32*2, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.iconv2 = nn.Conv2d(32, 16, kernel_size=1)\n",
    "        self.decoder_block2_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block2_2 = nn.Sequential(\n",
    "            nn.Conv2d(16*2, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)\n",
    "        # print(self.upconv1.shape)\n",
    "        self.iconv1 = nn.Conv2d(16, 8, kernel_size=1)\n",
    "        self.decoder_block1_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(16, 8, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block1_2 = nn.Sequential(\n",
    "            nn.Conv2d(8*2, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.C2_layer = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.C3_layer = nn.Conv2d(8, 8, kernel_size=1)\n",
    "        self.C3_layer = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=1),\n",
    "            nn.BatchNorm2d(8))\n",
    "        self.output_layer = nn.Conv2d(8, n_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape: \", x.shape)\n",
    "        x1, x2, x3, x4, x5, x6 = self.encoder(x)\n",
    "        # print(\"Encoder output shapes: \", x1.shape, x2.shape, x3.shape, x4.shape, x5.shape, x6.shape)\n",
    "        y6 = self.C2_layer(x6)\n",
    "        # print(\"C2 layer shape: \", y6.shape)\n",
    "        # UpSample\n",
    "        y5 = self.decoder_block5_1(y6)\n",
    "        # Concatenation with skip connection\n",
    "        y5 = torch.cat([x5, y5], dim=1)\n",
    "        # Conv2\n",
    "        y5 = self.decoder_block5_2(y5)\n",
    "        # print(\"1 - Decoder layer shape: \", y5.shape)\n",
    "        # UpSample\n",
    "        y4 = self.decoder_block4_1(y5)\n",
    "        # Concatenation with skip connection\n",
    "        y4 = torch.cat([x4, y4], dim=1)\n",
    "        # Conv2\n",
    "        y4 = self.decoder_block4_2(y4)\n",
    "        # print(\"2 - Decoder layer shape: \", y4.shape)\n",
    "        # UpSample\n",
    "        y3 = self.decoder_block3_1(y4)\n",
    "        # Concatenation with skip connection\n",
    "        y3 = torch.cat([x3, y3], dim=1)\n",
    "        # Conv2\n",
    "        y3 = self.decoder_block3_2(y3)\n",
    "        # print(\"3 - Decoder layer shape: \", y3.shape)\n",
    "        # UpSample\n",
    "        y2 = self.decoder_block2_1(y3)\n",
    "        # Concatenation with skip connection\n",
    "        y2 = torch.cat([x2, y2], dim=1)\n",
    "        # Conv2\n",
    "        y2 = self.decoder_block2_2(y2)\n",
    "        # print(\"4 - Decoder layer shape: \", y2.shape)\n",
    "\n",
    "        # UpSample\n",
    "        y1 = self.decoder_block1_1(y2)\n",
    "        # Concatenation with skip connection\n",
    "        y1 = torch.cat([x1, y1], dim=1)\n",
    "        # Conv2\n",
    "        y1 = self.decoder_block1_2(y1)\n",
    "        # print(\"5 - Decoder layer shape: \", y1.shape)\n",
    "\n",
    "        out = self.output_layer(y1)\n",
    "        # print(\"6 - Output layer shape: \", out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "\n",
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.07 GiB already allocated; 248.65 MiB free; 2.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\MobileNet_UNet.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# inputs = inputs.unsqueeze(0)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print(inputs.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Forward pass the input images through the model to get the predictions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m outputs\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\MobileNet_UNet.ipynb Cell 4\u001b[0m in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m y2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_block2_2(y2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m \u001b[39m# print(\"4 - Decoder layer shape: \", y2.shape)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m \u001b[39m# UpSample\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m y1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder_block1_1(y2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39m# Concatenation with skip connection\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#W3sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m y1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x1, y1], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.07 GiB already allocated; 248.65 MiB free; 2.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Instantiate the UNet model and move it to the device\n",
    "model = UNet().to(device)\n",
    "# print(model)\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# summary(model, input_size=(3, 640, 640), device='cpu')\n",
    "# Loop through the training dataset and perform the training\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        # Move the input images and the target masks to the same device as the model\n",
    "        inputs, targets = inputs.to(device), targets[\"gt_tensor\"].to(device)\n",
    "        # Zero the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # inputs = inputs.unsqueeze(0)\n",
    "        # print(inputs.shape)\n",
    "        # Forward pass the input images through the model to get the predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        outputs = torch.argmax(outputs, dim=1).unsqueeze(1).repeat(1,3,1,1).float()\n",
    "        outputs.requires_grad = True\n",
    "        # Compute the loss between the predictions and the target masks\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward propagate the loss through the model to compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    #Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{10} - Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n",
      "torch.Size([1, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "img_tns, gt = dataset[0]\n",
    "print(img_tns.shape)\n",
    "img_tns_ = img_tns.unsqueeze(0)\n",
    "print(img_tns_.shape)\n",
    "img_tns_ = img_tns_.to('cuda')\n",
    "pred_mask = model(img_tns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "pred_mask = torch.argmax(pred_mask, dim=1).unsqueeze(1).repeat(1,3,1,1).float()\n",
    "# print(pred_mask[0].unique())\n",
    "print(pred_mask.shape)\n",
    "# print(gt[\"gt_tensor\"])\n",
    "# print(gt[\"gt_tensor\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n",
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "pred_mask = torch.squeeze(pred_mask)\n",
    "print(pred_mask.shape)\n",
    "pred_mask = dataset.toImagearr(pred_mask)\n",
    "print(pred_mask.shape)\n",
    "dataset.plot_img_gt(img_tns, pred_mask)\n",
    "# img_tns.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
