{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv2d, self).__init__()\n",
    "        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels)\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return x\n",
    "\n",
    "class MobileNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(MobileNetEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            # nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(8, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        return x1, x2, x3, x4, x5, x6\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, output_act = nn.Sigmoid(), find_threshold = False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.lane_threshold = 0.5\n",
    "        self.output_act = output_act\n",
    "        self.roc_flag = find_threshold\n",
    "        # Encoder\n",
    "        self.encoder = MobileNetEncoder(in_channels=n_channels)\n",
    "        \n",
    "        # Decoder\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.iconv5 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.decoder_block5_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block5_2 = nn.Sequential(\n",
    "            nn.Conv2d(128*2, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.iconv4 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "        self.decoder_block4_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block4_2 = nn.Sequential(\n",
    "            nn.Conv2d(64*2, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.iconv3 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.decoder_block3_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(32)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block3_2 = nn.Sequential(\n",
    "            nn.Conv2d(32*2, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.iconv2 = nn.Conv2d(32, 16, kernel_size=1)\n",
    "        self.decoder_block2_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block2_2 = nn.Sequential(\n",
    "            nn.Conv2d(16*2, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)\n",
    "        # print(self.upconv1.shape)\n",
    "        self.iconv1 = nn.Conv2d(16, 8, kernel_size=1)\n",
    "        self.decoder_block1_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(16, 8, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(8)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block1_2 = nn.Sequential(\n",
    "            nn.Conv2d(8*2, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.C2_layer = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=1), \n",
    "            nn.BatchNorm2d(256))\n",
    "        # self.C3_layer = nn.Conv2d(8, 8, kernel_size=1)\n",
    "        self.C3_layer = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=1),\n",
    "            nn.BatchNorm2d(8))\n",
    "        self.output_layer = nn.Conv2d(8, n_classes, kernel_size=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape: \", x.shape)\n",
    "        x1, x2, x3, x4, x5, x6 = self.encoder(x)\n",
    "        # print(\"Encoder output shapes: \", x1.shape, x2.shape, x3.shape, x4.shape, x5.shape, x6.shape)\n",
    "        y6 = self.C2_layer(x6)\n",
    "        # print(\"C2 layer shape: \", y6.shape)\n",
    "        # UpSample\n",
    "        y5 = self.decoder_block5_1(y6)\n",
    "        # Concatenation with skip connection\n",
    "        y5 = torch.cat([x5, y5], dim=1)\n",
    "        # Conv2\n",
    "        y5 = self.decoder_block5_2(y5)\n",
    "        # print(\"1 - Decoder layer shape: \", y5.shape)\n",
    "        # UpSample\n",
    "        y4 = self.decoder_block4_1(y5)\n",
    "        # Concatenation with skip connection\n",
    "        y4 = torch.cat([x4, y4], dim=1)\n",
    "        # Conv2\n",
    "        y4 = self.decoder_block4_2(y4)\n",
    "        # print(\"2 - Decoder layer shape: \", y4.shape)\n",
    "        # UpSample\n",
    "        y3 = self.decoder_block3_1(y4)\n",
    "        # Concatenation with skip connection\n",
    "        y3 = torch.cat([x3, y3], dim=1)\n",
    "        # Conv2\n",
    "        y3 = self.decoder_block3_2(y3)\n",
    "        # print(\"3 - Decoder layer shape: \", y3.shape)\n",
    "        # UpSample\n",
    "        y2 = self.decoder_block2_1(y3)\n",
    "        # Concatenation with skip connection\n",
    "        y2 = torch.cat([x2, y2], dim=1)\n",
    "        # Conv2\n",
    "        y2 = self.decoder_block2_2(y2)\n",
    "        # print(\"4 - Decoder layer shape: \", y2.shape)\n",
    "\n",
    "        # UpSample\n",
    "        y1 = self.decoder_block1_1(y2)\n",
    "        # Concatenation with skip connection\n",
    "        y1 = torch.cat([x1, y1], dim=1)\n",
    "        # Conv2\n",
    "        y1 = self.decoder_block1_2(y1)\n",
    "        y1 = self.C3_layer(y1)\n",
    "        # print(\"5 - Decoder layer shape: \", y1.shape)\n",
    "\n",
    "        out = self.output_layer(y1)\n",
    "        # Training time\n",
    "        if self.training:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            return out, predictions\n",
    "            # return class_prob_masks\n",
    "        elif self.roc_flag and not self.training:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            return class_prob_masks\n",
    "        # Evaluation time\n",
    "        else:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            # Flatten the tensor into a 1D array\n",
    "            x_flat = class_prob_masks.detach().cpu().numpy().flatten()\n",
    "            # Plot a histogram of the tensor values\n",
    "            plt.hist(x_flat, bins=100)\n",
    "            plt.show()\n",
    "            # print(class_prob_masks)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            # return predictions\n",
    "            return predictions\n",
    "        # print(\"6 - Output layer shape: \", out.shape)\n",
    "        # return out\n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "import torchvision.transforms as transforms\n",
    "# from torch_poly_lr_decay import PolynomialLRDecay\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "import utils\n",
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.2, val_size= 0.01)\n",
    "# Create train and validation splits / Always use del dataset to free memory after this\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)\n",
    "print(pos_weight.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_img_pred(train_set[2][0], train_set[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the CNN pipeline with schedule and SGD optimizer\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.1, momentum=0.9, weight_decay=0.001, lr_scheduler=True, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        scheduler = PolynomialLRDecay(optimizer, max_decay_steps=100, end_learning_rate=0.0001, power=0.9)\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    train_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "    \n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        \n",
    "            \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = train_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)].to(device)\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "            \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "        \n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "        \n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 793249\n"
     ]
    }
   ],
   "source": [
    "# train_loader = DataLoader(train_set, batch_size=8, shuffle= True, drop_last= True)\n",
    "# validation_loader = DataLoader(validation_set,batch_size=8, shuffle= True, drop_last= True)  \n",
    "model = UNet()\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "# train(model, train_loader, val_loader = None, num_epochs= 5, lr_scheduler=False, lane_weight = pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Train Loss: 5.2155 - Learning Rate: 0.100000 - Train_IoU: 0.00332 - Train_F1: 0.00662\n",
      "Epoch: 2 - Train Loss: 4.9140 - Learning Rate: 0.100000 - Train_IoU: 0.00352 - Train_F1: 0.00702\n",
      "Epoch: 3 - Train Loss: 4.6149 - Learning Rate: 0.100000 - Train_IoU: 0.00373 - Train_F1: 0.00743\n",
      "Epoch: 4 - Train Loss: 4.5529 - Learning Rate: 0.100000 - Train_IoU: 0.00375 - Train_F1: 0.00748\n",
      "Epoch: 5 - Train Loss: 4.2586 - Learning Rate: 0.100000 - Train_IoU: 0.00403 - Train_F1: 0.00802\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader,val_loader= validation_loader,num_epochs= 5, lane_weight = pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD SAVED MODEL FROM COLAB\n",
    "model.load_state_dict(torch.load('CNN_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1969800889492035\n"
     ]
    }
   ],
   "source": [
    "model.lane_threshold = 0.1969800889492035\n",
    "print(model.lane_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n",
      "torch.Size([1, 3, 640, 640])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUUElEQVR4nO3df6zd9X3f8edrdkNJM4iBC2I2nenw2gJq1OARr92qbN6Cm1Q1k0By1xYrs2SVsS6bJi3QSUNqZAm0abRogwoFhmFRwHKz4a2jqWWWZVPB5NKk4Vcpd6GDO7z4pmaUpYLO5L0/zudGxzf3fnx9z/3B9X0+pKPzPe/v5/O9n4+udV738/1+z3GqCkmS5vLnVnoAkqT3NoNCktRlUEiSugwKSVKXQSFJ6lq/0gNYbBdddFFt3rx5pYchSavKM888862qGptt31kXFJs3b2Z8fHylhyFJq0qS/znXPk89SZK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSus66T2aPavOtv/Xd7T+64xMrOBJJem9wRSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6ThsUSR5IcjzJc0O1f5HkD5J8Pcm/T/LBoX23JZlI8lKS64bq1yR5tu27O0la/Zwkj7b60SSbh/rsTvJye+xerElLkuZvPiuKB4EdM2qHgaur6seAPwRuA0hyJbALuKr1uSfJutbnXmAvsKU9po+5B3ijqq4A7gLubMe6ALgd+AhwLXB7kg1nPkVJ0ihOGxRV9WXgxIza71TVyfbyKWBT294JPFJV71TVK8AEcG2SS4HzqurJqirgIeD6oT772/ZBYHtbbVwHHK6qE1X1BoNwmhlYkqQlthjXKP4e8Hjb3gi8NrRvstU2tu2Z9VP6tPB5E7iwcyxJ0jIaKSiS/DPgJPC56dIszapTX2ifmePYm2Q8yfjU1FR/0JKkM7LgoGgXl38G+Pl2OgkGf/VfNtRsE/B6q2+apX5KnyTrgfMZnOqa61jfo6ruq6qtVbV1bGxsoVOSJM1iQUGRZAfwaeBnq+pPh3YdAna1O5kuZ3DR+umqOga8lWRbu/5wE/DYUJ/pO5puAJ5owfNF4GNJNrSL2B9rNUnSMlp/ugZJPg98FLgoySSDO5FuA84BDre7XJ+qql+qqueTHABeYHBK6paqercd6mYGd1Cdy+CaxvR1jfuBh5NMMFhJ7AKoqhNJPgN8pbX71ao65aK6JGnpnTYoqurnZinf32m/D9g3S30cuHqW+tvAjXMc6wHggdONUZK0dPxktiSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqOm1QJHkgyfEkzw3VLkhyOMnL7XnD0L7bkkwkeSnJdUP1a5I82/bdnSStfk6SR1v9aJLNQ312t5/xcpLdizZrSdK8zWdF8SCwY0btVuBIVW0BjrTXJLkS2AVc1frck2Rd63MvsBfY0h7Tx9wDvFFVVwB3AXe2Y10A3A58BLgWuH04kCRJy+O0QVFVXwZOzCjvBPa37f3A9UP1R6rqnap6BZgArk1yKXBeVT1ZVQU8NKPP9LEOAtvbauM64HBVnaiqN4DDfG9gSZKW2EKvUVxSVccA2vPFrb4ReG2o3WSrbWzbM+un9Kmqk8CbwIWdY32PJHuTjCcZn5qaWuCUJEmzWeyL2ZmlVp36QvucWqy6r6q2VtXWsbGxeQ1UkjQ/Cw2Kb7bTSbTn460+CVw21G4T8Hqrb5qlfkqfJOuB8xmc6prrWJKkZbTQoDgETN+FtBt4bKi+q93JdDmDi9ZPt9NTbyXZ1q4/3DSjz/SxbgCeaNcxvgh8LMmGdhH7Y60mSVpG60/XIMnngY8CFyWZZHAn0h3AgSR7gFeBGwGq6vkkB4AXgJPALVX1bjvUzQzuoDoXeLw9AO4HHk4ywWAlsasd60SSzwBfae1+tapmXlSXJC2x0wZFVf3cHLu2z9F+H7Bvlvo4cPUs9bdpQTPLvgeAB043RknS0vGT2ZKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0jBUWSf5zk+STPJfl8ku9PckGSw0lebs8bhtrflmQiyUtJrhuqX5Pk2bbv7iRp9XOSPNrqR5NsHmW8kqQzt+CgSLIR+IfA1qq6GlgH7AJuBY5U1RbgSHtNkivb/quAHcA9Sda1w90L7AW2tMeOVt8DvFFVVwB3AXcudLySpIUZ9dTTeuDcJOuB9wOvAzuB/W3/fuD6tr0TeKSq3qmqV4AJ4NoklwLnVdWTVVXAQzP6TB/rILB9erUhSVoeCw6KqvpfwL8EXgWOAW9W1e8Al1TVsdbmGHBx67IReG3oEJOttrFtz6yf0qeqTgJvAhfOHEuSvUnGk4xPTU0tdEqSpFmMcuppA4O/+C8H/gLwA0l+oddlllp16r0+pxaq7quqrVW1dWxsrD9wSdIZGeXU098CXqmqqar6f8AXgJ8AvtlOJ9Gej7f2k8BlQ/03MThVNdm2Z9ZP6dNOb50PnBhhzJKkMzRKULwKbEvy/nbdYDvwInAI2N3a7AYea9uHgF3tTqbLGVy0frqdnnorybZ2nJtm9Jk+1g3AE+06hiRpmaxfaMeqOprkIPB7wEngq8B9wAeAA0n2MAiTG1v755McAF5o7W+pqnfb4W4GHgTOBR5vD4D7gYeTTDBYSexa6HglSQuz4KAAqKrbgdtnlN9hsLqYrf0+YN8s9XHg6lnqb9OCRpK0MvxktiSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUtdIQZHkg0kOJvmDJC8m+atJLkhyOMnL7XnDUPvbkkwkeSnJdUP1a5I82/bdnSStfk6SR1v9aJLNo4xXknTmRl1R/Drw21X1I8CHgBeBW4EjVbUFONJek+RKYBdwFbADuCfJunace4G9wJb22NHqe4A3quoK4C7gzhHHK0k6QwsOiiTnAT8F3A9QVX9WVf8H2Ansb832A9e37Z3AI1X1TlW9AkwA1ya5FDivqp6sqgIemtFn+lgHge3Tqw1J0vIYZUXxQ8AU8G+TfDXJZ5P8AHBJVR0DaM8Xt/YbgdeG+k+22sa2PbN+Sp+qOgm8CVw4cyBJ9iYZTzI+NTU1wpQkSTONEhTrgQ8D91bVjwPfpp1mmsNsK4Hq1Ht9Ti1U3VdVW6tq69jYWH/UkqQzMkpQTAKTVXW0vT7IIDi+2U4n0Z6PD7W/bKj/JuD1Vt80S/2UPknWA+cDJ0YYsyTpDC04KKrqfwOvJfnhVtoOvAAcAna32m7gsbZ9CNjV7mS6nMFF66fb6am3kmxr1x9umtFn+lg3AE+06xiSpGWyfsT+vwx8Lsn7gG8An2QQPgeS7AFeBW4EqKrnkxxgECYngVuq6t12nJuBB4FzgcfbAwYXyh9OMsFgJbFrxPFKks7QSEFRVV8Dts6ya/sc7fcB+2apjwNXz1J/mxY0kqSV4SezJUldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSukYOiiTrknw1yX9qry9IcjjJy+15w1Db25JMJHkpyXVD9WuSPNv23Z0krX5Okkdb/WiSzaOOV5J0ZhZjRfEp4MWh17cCR6pqC3CkvSbJlcAu4CpgB3BPknWtz73AXmBLe+xo9T3AG1V1BXAXcOcijFeSdAZGCookm4BPAJ8dKu8E9rft/cD1Q/VHquqdqnoFmACuTXIpcF5VPVlVBTw0o8/0sQ4C26dXG5Kk5THqiuLXgH8KfGeodklVHQNozxe3+kbgtaF2k622sW3PrJ/Sp6pOAm8CF84cRJK9ScaTjE9NTY04JUnSsAUHRZKfAY5X1TPz7TJLrTr1Xp9TC1X3VdXWqto6NjY2z+FIkuZj/Qh9fxL42SQfB74fOC/JvwO+meTSqjrWTisdb+0ngcuG+m8CXm/1TbPUh/tMJlkPnA+cGGHMkqQztOAVRVXdVlWbqmozg4vUT1TVLwCHgN2t2W7gsbZ9CNjV7mS6nMFF66fb6am3kmxr1x9umtFn+lg3tJ/xPSsKSdLSGWVFMZc7gANJ9gCvAjcCVNXzSQ4ALwAngVuq6t3W52bgQeBc4PH2ALgfeDjJBIOVxK4lGK8kqWNRgqKqvgR8qW3/MbB9jnb7gH2z1MeBq2epv00LGknSyvCT2ZKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0LDooklyX5L0leTPJ8kk+1+gVJDid5uT1vGOpzW5KJJC8luW6ofk2SZ9u+u5Ok1c9J8mirH02yeYS5SpIWYJQVxUngn1TVjwLbgFuSXAncChypqi3Akfaatm8XcBWwA7gnybp2rHuBvcCW9tjR6nuAN6rqCuAu4M4RxitJWoAFB0VVHauq32vbbwEvAhuBncD+1mw/cH3b3gk8UlXvVNUrwARwbZJLgfOq6smqKuChGX2mj3UQ2D692pAkLY9FuUbRTgn9OHAUuKSqjsEgTICLW7ONwGtD3SZbbWPbnlk/pU9VnQTeBC5cjDFLkuZn5KBI8gHgN4F/VFV/0ms6S6069V6fmWPYm2Q8yfjU1NTphixJOgMjBUWS72MQEp+rqi+08jfb6STa8/FWnwQuG+q+CXi91TfNUj+lT5L1wPnAiZnjqKr7qmprVW0dGxsbZUqSpBlGuespwP3Ai1X1r4Z2HQJ2t+3dwGND9V3tTqbLGVy0frqdnnorybZ2zJtm9Jk+1g3AE+06hiRpmawfoe9PAr8IPJvka632K8AdwIEke4BXgRsBqur5JAeAFxjcMXVLVb3b+t0MPAicCzzeHjAIooeTTDBYSewaYbySpAVYcFBU1X9n9msIANvn6LMP2DdLfRy4epb627SgkSStDD+ZLUnqGuXU01lv862/9d3tP7rjEys4EklaOa4oJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLr/CYJ7/OQ9Ja5YpCktTlimIBXF1IWktcUUiSulxRjMjVhaSznUHxHmT4SHovMSgW0fAb/FIc09CQtBIMilVkriBa7gAxvKS1xaA4CyzFSmYxf7ZhIq1uBoWW3HIGmaEkLT6DQmeVUUNpOGiWIuAMsrVttZ62TVWt9BhOK8kO4NeBdcBnq+qOudpu3bq1xsfHF/yzVvI0jiTNZamDJckzVbV1tn3v+RVFknXAvwH+NjAJfCXJoap6YWVHJknLZyVXI+/5oACuBSaq6hsASR4BdgIGhaQ1abnvgFwNQbEReG3o9STwkeEGSfYCe9vL/5vkpRF+3kXAt0bovxo557XBOZ/lciew8Dn/xbl2rIagyCy1Uy6sVNV9wH2L8sOS8bnO052tnPPa4JzXhqWY82r4UsBJ4LKh15uA11doLJK05qyGoPgKsCXJ5UneB+wCDq3wmCRpzXjPn3qqqpNJ/gHwRQa3xz5QVc8v4Y9clFNYq4xzXhuc89qw6HNeFZ+jkCStnNVw6kmStIIMCklS15oMiiQ7kryUZCLJrbPsT5K72/6vJ/nwSoxzMc1jzj/f5vr1JL+b5EMrMc7FdLo5D7X7K0neTXLDco5vKcxnzkk+muRrSZ5P8l+Xe4yLbR7/ts9P8h+T/H6b8ydXYpyLKckDSY4neW6O/Yv7HlZVa+rB4IL4/wB+CHgf8PvAlTPafBx4nMFnOLYBR1d63Msw558ANrTtn14Lcx5q9wTwn4EbVnrcy/B7/iCDbzX4wfb64pUe9zLM+VeAO9v2GHACeN9Kj33Eef8U8GHguTn2L+p72FpcUXz3K0Gq6s+A6a8EGbYTeKgGngI+mOTS5R7oIjrtnKvqd6vqjfbyKQafV1nN5vN7Bvhl4DeB48s5uCUynzn/XeALVfUqQFWt9nnPZ84F/PkkAT7AIChOLu8wF1dVfZnBPOayqO9hazEoZvtKkI0LaLOanOl89jD4a2Q1O+2ck2wE/g7wG8s4rqU0n9/zXwY2JPlSkmeS3LRso1sa85nzvwZ+lMEHdZ8FPlVV31me4a2YRX0Pe89/jmIJnPYrQebZZjWZ93yS/A0GQfHXlnRES28+c/414NNV9e7gj81Vbz5zXg9cA2wHzgWeTPJUVf3hUg9uicxnztcBXwP+JvCXgMNJ/ltV/ckSj20lLep72FoMivl8JcjZ9rUh85pPkh8DPgv8dFX98TKNbanMZ85bgUdaSFwEfDzJyar6D8sywsU333/b36qqbwPfTvJl4EPAag2K+cz5k8AdNTh5P5HkFeBHgKeXZ4grYlHfw9biqaf5fCXIIeCmdufANuDNqjq23ANdRKedc5IfBL4A/OIq/uty2GnnXFWXV9XmqtoMHAT+/ioOCZjfv+3HgL+eZH2S9zP4JuYXl3mci2k+c36VwQqKJJcAPwx8Y1lHufwW9T1sza0oao6vBEnyS23/bzC4A+bjwATwpwz+Ilm15jnnfw5cCNzT/sI+Wav4WzfnOeezynzmXFUvJvlt4OvAdxj8j5Gz3mK5Gszz9/wZ4MEkzzI4JfPpqlrVXz2e5PPAR4GLkkwCtwPfB0vzHuZXeEiSutbiqSdJ0hkwKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6/j8Yh2loeNlL/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "model.to(device='cuda').eval()\n",
    "img_tns, gt = validation_set[5]\n",
    "print(img_tns.shape)\n",
    "img_tns_ = img_tns.unsqueeze(0)\n",
    "print(img_tns_.shape)\n",
    "img_tns_ = img_tns_.to('cuda')\n",
    "pred_mask = model(img_tns_)\n",
    "print(pred_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640)\n",
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "pred_mask = torch.squeeze(pred_mask)\n",
    "pred_mask = utils.toImagearr(pred_mask)\n",
    "img_tns = utils.toImagearr(img_tns)\n",
    "print(pred_mask.shape)\n",
    "print(img_tns.shape)\n",
    "utils.disp_img(image = img_tns, name = 'Original Image')\n",
    "utils.disp_img(image = pred_mask, name = 'Original Image')\n",
    "# img_tns.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
