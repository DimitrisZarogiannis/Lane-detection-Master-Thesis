{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv2d, self).__init__()\n",
    "        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels)\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return x\n",
    "\n",
    "class MobileNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(MobileNetEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            # nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(8, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        return x1, x2, x3, x4, x5, x6\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, output_act = nn.Sigmoid()):\n",
    "        super(UNet, self).__init__()\n",
    "        self.lane_threshold = 0.5\n",
    "        self.output_act = output_act\n",
    "        # Encoder\n",
    "        self.encoder = MobileNetEncoder(in_channels=n_channels)\n",
    "        \n",
    "        # Decoder\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.iconv5 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.decoder_block5_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block5_2 = nn.Sequential(\n",
    "            nn.Conv2d(128*2, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.iconv4 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "        self.decoder_block4_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block4_2 = nn.Sequential(\n",
    "            nn.Conv2d(64*2, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.iconv3 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.decoder_block3_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(32)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block3_2 = nn.Sequential(\n",
    "            nn.Conv2d(32*2, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.iconv2 = nn.Conv2d(32, 16, kernel_size=1)\n",
    "        self.decoder_block2_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block2_2 = nn.Sequential(\n",
    "            nn.Conv2d(16*2, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)\n",
    "        # print(self.upconv1.shape)\n",
    "        self.iconv1 = nn.Conv2d(16, 8, kernel_size=1)\n",
    "        self.decoder_block1_1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(16, 8, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(8)\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.decoder_block1_2 = nn.Sequential(\n",
    "            nn.Conv2d(8*2, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.C2_layer = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=1), \n",
    "            nn.BatchNorm2d(256))\n",
    "        self.C3_layer = nn.Conv2d(8, 8, kernel_size=1)\n",
    "        self.C3_layer = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=1),\n",
    "            nn.BatchNorm2d(8))\n",
    "        self.output_layer = nn.Conv2d(8, n_classes, kernel_size=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape: \", x.shape)\n",
    "        x1, x2, x3, x4, x5, x6 = self.encoder(x)\n",
    "        # print(\"Encoder output shapes: \", x1.shape, x2.shape, x3.shape, x4.shape, x5.shape, x6.shape)\n",
    "        y6 = self.C2_layer(x6)\n",
    "        # print(\"C2 layer shape: \", y6.shape)\n",
    "        # UpSample\n",
    "        y5 = self.decoder_block5_1(y6)\n",
    "        # Concatenation with skip connection\n",
    "        y5 = torch.cat([x5, y5], dim=1)\n",
    "        # Conv2\n",
    "        y5 = self.decoder_block5_2(y5)\n",
    "        # print(\"1 - Decoder layer shape: \", y5.shape)\n",
    "        # UpSample\n",
    "        y4 = self.decoder_block4_1(y5)\n",
    "        # Concatenation with skip connection\n",
    "        y4 = torch.cat([x4, y4], dim=1)\n",
    "        # Conv2\n",
    "        y4 = self.decoder_block4_2(y4)\n",
    "        # print(\"2 - Decoder layer shape: \", y4.shape)\n",
    "        # UpSample\n",
    "        y3 = self.decoder_block3_1(y4)\n",
    "        # Concatenation with skip connection\n",
    "        y3 = torch.cat([x3, y3], dim=1)\n",
    "        # Conv2\n",
    "        y3 = self.decoder_block3_2(y3)\n",
    "        # print(\"3 - Decoder layer shape: \", y3.shape)\n",
    "        # UpSample\n",
    "        y2 = self.decoder_block2_1(y3)\n",
    "        # Concatenation with skip connection\n",
    "        y2 = torch.cat([x2, y2], dim=1)\n",
    "        # Conv2\n",
    "        y2 = self.decoder_block2_2(y2)\n",
    "        # print(\"4 - Decoder layer shape: \", y2.shape)\n",
    "\n",
    "        # UpSample\n",
    "        y1 = self.decoder_block1_1(y2)\n",
    "        # Concatenation with skip connection\n",
    "        y1 = torch.cat([x1, y1], dim=1)\n",
    "        # Conv2\n",
    "        y1 = self.decoder_block1_2(y1)\n",
    "        # print(\"5 - Decoder layer shape: \", y1.shape)\n",
    "\n",
    "        out = self.output_layer(y1)\n",
    "        # Training time\n",
    "        if self.training:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            return out, predictions\n",
    "        # Evaluation time\n",
    "        else:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(out)\n",
    "            # print(class_prob_masks)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            return predictions\n",
    "        # print(\"6 - Output layer shape: \", out.shape)\n",
    "        # return out\n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "import utils\n",
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.15, val_size= 0.01)\n",
    "# Create train and validation splits / Always use del dataset to free memory after this\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1935, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)\n",
    "print(pos_weight.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the CNN pipeline with schedule and SGD optimizer\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.1, momentum=0.9, weight_decay=0.001, lr_scheduler=True, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    # set threshold to identify lane pixels to calculate eval metrics\n",
    "    # LANE_THRESHOLD = 0.5\n",
    "    # sigm = nn.Sigmoid()\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train for one epoch\n",
    "        # model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "                   \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "            \n",
    "        if val_loader:\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader): \n",
    "                model.eval()\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                val_iou += iou_score(outputs.to(device), targets)\n",
    "                val_f1 += f1_score(outputs.to(device), targets)\n",
    "        \n",
    "            val_iou /= len(val_loader)\n",
    "            val_f1 /= len(val_loader)\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "        \n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,scheduler.get_last_lr()[0], train_iou, train_f1))\n",
    "            scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f}'.format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 793249\n",
      "Epoch: 1 - Train Loss: 7.7040 - Learning Rate: 0.100000 - Train_IoU: 0.00192 - Train_F1: 0.00382\n",
      "Epoch: 2 - Train Loss: 6.1285 - Learning Rate: 0.100000 - Train_IoU: 0.00273 - Train_F1: 0.00544\n",
      "Epoch: 3 - Train Loss: 5.3863 - Learning Rate: 0.100000 - Train_IoU: 0.00316 - Train_F1: 0.00630\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=8, shuffle= True, drop_last= True)\n",
    "validation_loader = DataLoader(validation_set,batch_size=8, shuffle= True, drop_last= True)  \n",
    "model = UNet()\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "train(model, train_loader,val_loader= validation_loader,num_epochs= 3, lane_weight = pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Train Loss: 5.2155 - Learning Rate: 0.100000 - Train_IoU: 0.00332 - Train_F1: 0.00662\n",
      "Epoch: 2 - Train Loss: 4.9140 - Learning Rate: 0.100000 - Train_IoU: 0.00352 - Train_F1: 0.00702\n",
      "Epoch: 3 - Train Loss: 4.6149 - Learning Rate: 0.100000 - Train_IoU: 0.00373 - Train_F1: 0.00743\n",
      "Epoch: 4 - Train Loss: 4.5529 - Learning Rate: 0.100000 - Train_IoU: 0.00375 - Train_F1: 0.00748\n",
      "Epoch: 5 - Train Loss: 4.2586 - Learning Rate: 0.100000 - Train_IoU: 0.00403 - Train_F1: 0.00802\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader,val_loader= validation_loader,num_epochs= 5, lane_weight = pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n",
      "torch.Size([1, 3, 640, 640])\n",
      "torch.Size([1, 1, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "img_tns, gt = validation_set[2]\n",
    "print(img_tns.shape)\n",
    "img_tns_ = img_tns.unsqueeze(0)\n",
    "print(img_tns_.shape)\n",
    "img_tns_ = img_tns_.to('cuda')\n",
    "pred_mask = model(img_tns_)\n",
    "print(pred_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640)\n",
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "pred_mask = torch.squeeze(pred_mask)\n",
    "pred_mask = utils.toImagearr(pred_mask)\n",
    "img_tns = utils.toImagearr(img_tns)\n",
    "print(pred_mask.shape)\n",
    "print(img_tns.shape)\n",
    "utils.disp_img(image = img_tns, name = 'Original Image')\n",
    "utils.disp_img(image = pred_mask, name = 'Original Image')\n",
    "# img_tns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 4 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\MobileNet_UNet.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# plot image and prediction simultaneously\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/MobileNet_UNet.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m utils\u001b[39m.\u001b[39;49mplot_img_pred(img_tns\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39;49m),pred_mask)\n",
      "File \u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\../resources\\utils.py:29\u001b[0m, in \u001b[0;36mplot_img_pred\u001b[1;34m(tensor, pred_mask)\u001b[0m\n\u001b[0;32m     27\u001b[0m img \u001b[39m=\u001b[39m toImagearr(tensor)\n\u001b[0;32m     28\u001b[0m rgb_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack((pred_mask,)\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \n\u001b[1;32m---> 29\u001b[0m pred_mask \u001b[39m=\u001b[39m toImagearr(rgb_tensor)\n\u001b[0;32m     30\u001b[0m hori \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((img, pred_mask), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m disp_img(hori,\u001b[39m'\u001b[39m\u001b[39mImage/Predicted Mask\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\../resources\\utils.py:11\u001b[0m, in \u001b[0;36mtoImagearr\u001b[1;34m(img_tens)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoImagearr\u001b[39m(img_tens):\n\u001b[0;32m     10\u001b[0m     convert \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([transforms\u001b[39m.\u001b[39mToPILImage()])\n\u001b[1;32m---> 11\u001b[0m     im_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(convert(img_tens))\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m im_array\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\SOTA\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\SOTA\\lib\\site-packages\\torchvision\\transforms\\transforms.py:226\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    218\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\SOTA\\lib\\site-packages\\torchvision\\transforms\\functional.py:263\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m}:\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndimension()\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m     \u001b[39melif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    266\u001b[0m         \u001b[39m# if 2D image, add channel dimension (CHW)\u001b[39;00m\n\u001b[0;32m    267\u001b[0m         pic \u001b[39m=\u001b[39m pic\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 4 dimensions."
     ]
    }
   ],
   "source": [
    "# plot image and prediction simultaneously\n",
    "utils.plot_img_pred(img_tns.squeeze(0),pred_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
