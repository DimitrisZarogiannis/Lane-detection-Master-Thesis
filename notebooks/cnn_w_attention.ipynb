{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from mlp_decoder import DecoderMLP\n",
    "import segnet_backbone as cnn\n",
    "import utils\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "test_annotated = os.listdir(annotated_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.1)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 29443587\n",
      "Loaded state dict succesfully!\n"
     ]
    }
   ],
   "source": [
    "model = cnn.SegNet()\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "model.load_weights('../models/best_segnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "img_tns,gt = validation_set[0]\n",
    "\n",
    "print(img_tns.unsqueeze(0).shape)\n",
    "\n",
    "model.eval()\n",
    "test_pred, test_features = model.predict(img_tns.unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 448, 448])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel Attention module\n",
    "class TransformerChannelAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, depth, in_channels):\n",
    "        super(TransformerChannelAttention, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_channels, in_channels)\n",
    "        \n",
    "        # define the pooling layer to reduce spatial dimensions\n",
    "        self.pool = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        # define the unpooling layer to recover original spatial dimensions\n",
    "        self.unpool = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        \n",
    "        # Pass through the pooling layer to reduce spatial dimensions for computational reasons\n",
    "        x = self.pool(x)\n",
    "        height = height//4\n",
    "        width = width//4\n",
    "\n",
    "        \n",
    "        # Reformat the inputs to the appropriate format for the transformer\n",
    "        x = x.reshape(height*width, batch_size, num_channels)\n",
    "        x = x.permute(1, 0, 2)   # shape: (batch_size, seq_length, num_channels)\n",
    "\n",
    "        # Perform channel wise attention to the reduced feature maps\n",
    "        x = self.transformer(x)  # shape: (batch_size, seq_length, num_channels)\n",
    "        x = x.permute(1, 0, 2)  # swap back seq_length and batch_size dimensions\n",
    "        x = x.reshape(batch_size, num_channels, height, width)  # shape: (batch_size, num_channels, height, width)\n",
    "        # Reshape back to original spatial dimensions\n",
    "        x = self.unpool(x)\n",
    "        height,width = height * 4, width*4\n",
    "        \n",
    "        # Average pooling to get one value per channel and calculate channel attention weights \n",
    "        attn = self.avg_pool(x)  # shape: (batch_size, num_channels, 1, 1)\n",
    "        attn = attn.view(batch_size, num_channels)  # shape: (batch_size, num_channels)\n",
    "        attn = self.fc(attn)  # shape: (batch_size, num_channels)\n",
    "        attn = attn.unsqueeze(-1).unsqueeze(-1)  # shape: (batch_size, num_channels, 1, 1)\n",
    "        \n",
    "        # Weight the transformed features based on channel attention and keep best features\n",
    "        trans_features = (x * attn).sum(dim=1, keepdim=True)  # shape: (batch_size, 1, height, width)\n",
    "\n",
    "        # Pass through sigmoid to get attention-weighted probabilities\n",
    "        mask_probs = torch.sigmoid(trans_features)\n",
    "        return trans_features, mask_probs\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_module = TransformerChannelAttention(dim = 64 ,heads=8,mlp_dim=2048,depth=3,in_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0210]],\n",
      "\n",
      "         [[ 0.0364]],\n",
      "\n",
      "         [[-0.0477]],\n",
      "\n",
      "         [[-0.0623]],\n",
      "\n",
      "         [[ 0.0154]],\n",
      "\n",
      "         [[ 0.1016]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[ 0.1099]],\n",
      "\n",
      "         [[ 0.0209]],\n",
      "\n",
      "         [[ 0.0811]],\n",
      "\n",
      "         [[ 0.0278]],\n",
      "\n",
      "         [[-0.0859]],\n",
      "\n",
      "         [[ 0.1085]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[ 0.0291]],\n",
      "\n",
      "         [[-0.0712]],\n",
      "\n",
      "         [[ 0.0882]],\n",
      "\n",
      "         [[ 0.1091]],\n",
      "\n",
      "         [[-0.0731]],\n",
      "\n",
      "         [[ 0.0474]],\n",
      "\n",
      "         [[ 0.1186]],\n",
      "\n",
      "         [[-0.0240]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[ 0.0183]],\n",
      "\n",
      "         [[ 0.0715]],\n",
      "\n",
      "         [[-0.0595]],\n",
      "\n",
      "         [[-0.1111]],\n",
      "\n",
      "         [[-0.1076]],\n",
      "\n",
      "         [[ 0.0951]],\n",
      "\n",
      "         [[-0.0507]],\n",
      "\n",
      "         [[-0.0190]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         [[ 0.0918]],\n",
      "\n",
      "         [[ 0.0891]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0657]],\n",
      "\n",
      "         [[-0.0582]],\n",
      "\n",
      "         [[ 0.0768]],\n",
      "\n",
      "         [[ 0.1118]],\n",
      "\n",
      "         [[-0.0830]],\n",
      "\n",
      "         [[ 0.0653]],\n",
      "\n",
      "         [[-0.0554]],\n",
      "\n",
      "         [[-0.0246]],\n",
      "\n",
      "         [[ 0.0447]],\n",
      "\n",
      "         [[ 0.0163]],\n",
      "\n",
      "         [[ 0.0753]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         [[ 0.0988]],\n",
      "\n",
      "         [[-0.0537]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0109]],\n",
      "\n",
      "         [[-0.0667]],\n",
      "\n",
      "         [[-0.0316]],\n",
      "\n",
      "         [[ 0.0005]],\n",
      "\n",
      "         [[ 0.0986]],\n",
      "\n",
      "         [[ 0.1244]],\n",
      "\n",
      "         [[-0.0548]],\n",
      "\n",
      "         [[ 0.1007]],\n",
      "\n",
      "         [[ 0.0449]],\n",
      "\n",
      "         [[-0.0979]],\n",
      "\n",
      "         [[-0.1078]],\n",
      "\n",
      "         [[ 0.0742]],\n",
      "\n",
      "         [[-0.0382]]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits,probs = ca_module(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.6450e-12,\n",
       "           8.7872e-13, 8.7872e-13],\n",
       "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.6450e-12,\n",
       "           8.7872e-13, 8.7872e-13],\n",
       "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.5414e-10,\n",
       "           7.2772e-11, 7.2772e-11],\n",
       "          ...,\n",
       "          [9.9799e-01, 9.9799e-01, 9.6293e-01,  ..., 9.9999e-01,\n",
       "           1.0000e+00, 1.0000e+00],\n",
       "          [6.3819e-01, 6.3819e-01, 1.9970e-01,  ..., 9.9999e-01,\n",
       "           1.0000e+00, 1.0000e+00],\n",
       "          [6.3819e-01, 6.3819e-01, 1.9970e-01,  ..., 9.9999e-01,\n",
       "           1.0000e+00, 1.0000e+00]]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_pipe(nn.Module):\n",
    "    def __init__(self, feat_extractor, ca_module, image_size = (448,448)):\n",
    "        super().__init__()\n",
    "        self.cnn = feat_extractor\n",
    "        self.ca_transformer = ca_module\n",
    "        self.image_size = image_size\n",
    "        self.lane_threshold = 0.5\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # CNN branch for feature extraction\n",
    "        x,_ = self.cnn(im)\n",
    "        \n",
    "        # Transform standardized feature maps using the ViT\n",
    "        logits, probs = self.ca_transformer(x)\n",
    "        \n",
    "        return logits, probs\n",
    "        \n",
    "        \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Load trained model\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = attention_pipe(model,ca_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.0384, -1.0384, -0.9430,  ...,  0.0671,  0.1409,  0.1409],\n",
       "           [-1.0384, -1.0384, -0.9430,  ...,  0.0671,  0.1409,  0.1409],\n",
       "           [-0.8334, -0.8334, -0.7689,  ...,  0.2270,  0.3021,  0.3021],\n",
       "           ...,\n",
       "           [ 0.2861,  0.2861,  0.2942,  ..., -0.6637, -0.7097, -0.7097],\n",
       "           [ 0.3824,  0.3824,  0.3845,  ..., -0.8456, -0.9243, -0.9243],\n",
       "           [ 0.3824,  0.3824,  0.3845,  ..., -0.8456, -0.9243, -0.9243]]]],\n",
       "        grad_fn=<SumBackward1>),\n",
       " tensor([[[[0.2615, 0.2615, 0.2803,  ..., 0.5168, 0.5352, 0.5352],\n",
       "           [0.2615, 0.2615, 0.2803,  ..., 0.5168, 0.5352, 0.5352],\n",
       "           [0.3029, 0.3029, 0.3167,  ..., 0.5565, 0.5750, 0.5750],\n",
       "           ...,\n",
       "           [0.5710, 0.5710, 0.5730,  ..., 0.3399, 0.3297, 0.3297],\n",
       "           [0.5945, 0.5945, 0.5950,  ..., 0.3004, 0.2841, 0.2841],\n",
       "           [0.5945, 0.5945, 0.5950,  ..., 0.3004, 0.2841, 0.2841]]]],\n",
       "        grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(img_tns.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
