{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626\n",
      "[[-2, 570, 554, 538, 522, 505, 489, 473, 456, 440, 424, 407, 391, 375, 359, 342, 326, 310, 293, 277, 261, 244, 228, 212, 196, 179, 163, 147, 130, 114, 98, 81, 65, 49, 33, 16, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2], [-2, -2, 601, 606, 611, 616, 621, 625, 630, 635, 640, 645, 650, 655, 660, 665, 670, 675, 680, 684, 689, 694, 699, 704, 709, 714, 719, 724, 729, 734, 738, 743, 748, 753, 758, 763, 768, 773, 778, 783, 788, 793, 797, 802, 807, 812, 817, 822], [-2, -2, 516, 482, 449, 415, 382, 348, 314, 281, 247, 214, 180, 147, 113, 80, 46, 13, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2], [-2, -2, 647, 673, 698, 724, 750, 775, 801, 826, 852, 877, 903, 928, 954, 980, 1005, 1031, 1056, 1082, 1107, 1133, 1158, 1184, 1210, 1235, 1261, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]]\n"
     ]
    }
   ],
   "source": [
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "print(len(annotations))\n",
    "print(annotations[2]['lanes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = np.zeros((720,1280,3),dtype= np.int32)\n",
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(730, 280), (705, 290), (681, 300), (661, 310), (640, 320), (619, 330), (598, 340), (577, 350), (556, 360), (535, 370), (514, 380), (493, 390), (472, 400), (451, 410), (433, 420), (416, 430), (398, 440), (381, 450), (363, 460), (346, 470), (328, 480), (311, 490), (293, 500), (276, 510), (258, 520), (241, 530), (223, 540), (205, 550), (188, 560), (170, 570), (153, 580), (135, 590), (118, 600), (100, 610), (83, 620), (65, 630), (48, 640), (30, 650), (12, 660)]\n",
      "[(884, 260), (848, 270), (831, 280), (816, 290), (816, 300), (815, 310), (815, 320), (814, 330), (814, 340), (814, 350), (817, 360), (820, 370), (823, 380), (826, 390), (829, 400), (832, 410), (835, 420), (838, 430), (842, 440), (845, 450), (848, 460), (851, 470), (854, 480), (857, 490), (860, 500), (863, 510), (866, 520), (869, 530), (872, 540), (875, 550), (878, 560), (881, 570), (885, 580), (888, 590), (891, 600), (894, 610), (897, 620), (900, 630), (903, 640), (906, 650), (909, 660), (912, 670), (915, 680), (918, 690), (921, 700), (924, 710)]\n",
      "[(683, 270), (630, 280), (585, 290), (539, 300), (493, 310), (448, 320), (405, 330), (361, 340), (318, 350), (275, 360), (232, 370), (189, 380), (146, 390), (102, 400), (59, 410), (16, 420)]\n",
      "[(923, 260), (915, 270), (923, 280), (930, 290), (947, 300), (967, 310), (986, 320), (1005, 330), (1024, 340), (1043, 350), (1065, 360), (1088, 370), (1111, 380), (1135, 390), (1158, 400), (1181, 410), (1204, 420), (1227, 430), (1250, 440), (1274, 450)]\n"
     ]
    }
   ],
   "source": [
    "# Generate segmentation masks functionality/ Useful for resizing ground truth masks NOTE: dims = (H,W,C)\n",
    "img_path = annotations[21]['raw_file']\n",
    "image = cv2.imread(os.path.join(clips_dir, img_path))\n",
    "\n",
    "# print(image)\n",
    "def generate_seg_mask(ground_truth: dict, image, image_dims = (720,1280,3)):\n",
    "    masks = np.zeros_like(image[:,:,0])\n",
    "    nolane_token = -2 \n",
    "    h_vals = ground_truth['h_samples']\n",
    "    lanes = ground_truth['lanes']\n",
    "    lane_val = 255\n",
    "    # NOT WORKING AS IT SHOULD \n",
    "    lane_markings_list = []\n",
    "    for lane in lanes:\n",
    "        x_coords = []\n",
    "        y_coords = []\n",
    "        for i in range(0,len(lane)):             \n",
    "            if lane[i] != nolane_token:\n",
    "                x_coords.append(lane[i])\n",
    "                y_coords.append(h_vals[i])\n",
    "                lane_markings = list(zip(x_coords, y_coords))\n",
    "                # print(lane_markings)\n",
    "        lane_markings_list.append(lane_markings)          \n",
    "        # lane_pix = np.array(lane_pix)                      \n",
    "        # height,width,chan = zip(*lane_pix)\n",
    "        # print(lane_pix)\n",
    "        # np.put(masks,lane_pix,255)\n",
    "    for z in lane_markings_list:\n",
    "        print(z)\n",
    "        for x,y in z:\n",
    "            masks[y,x] = 1\n",
    "    lane_markings_img = cv2.bitwise_and(image, image, mask=masks)\n",
    "    return lane_markings_img  \n",
    "\n",
    "# print(annotations[0])\n",
    "masked_image = generate_seg_mask(annotations[21], image)\n",
    "# masks.shape\n",
    "cv2.imshow('Original Image',image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()        \n",
    "cv2.imshow('Masked Image', masked_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "# Loading/Resizing image test scenario\n",
    "\n",
    "img_path = annotations[0]['raw_file']\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize(size=(640,640))])\n",
    "image = cv2.imread(os.path.join(clips_dir, img_path))\n",
    "print(image.shape)\n",
    "image_tensor = train_transforms(image)\n",
    "print(image_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "# EZ convert from normalized tensor to np.array (0,255) scale for RGB images\n",
    "convert = transforms.Compose([transforms.ToPILImage()])\n",
    "array = np.array(convert(image_tensor))\n",
    "print(array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuSimple Dataset loader and pre-processing class\n",
    "# Full Size: Train(3626 clips/ 20 frames per clip/ 20th only is annotated), Test(2782 clips/ 20 frames per clip/ 20th only annotated)\n",
    "# Link: https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection\n",
    "class TuSimple(Dataset):  \n",
    "    def __init__(self, train_annotations : list, train_img_dir: str, resize_to : tuple , subset_size = 0.2, image_size = (1280,720), val_size = 0.15):\n",
    "        self.image_size = image_size\n",
    "        self.resize = resize_to\n",
    "        self.val_size = val_size\n",
    "        self.subset = subset_size\n",
    "        self.train_dir = train_img_dir\n",
    "        self.complete_gt = train_annotations\n",
    "        self.complete_size = len(train_annotations)\n",
    "        self.train_dataset, self.train_gt = self.generate_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if len(self.train_dataset) == len(self.train_gt):\n",
    "            return len(self.train_gt)\n",
    "        else:\n",
    "            return \"Dataset generation failure: Size of training images does not match the existing ground truths.\"\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.train_dataset) == len(self.train_gt):\n",
    "            img_tensor = self.train_dataset[idx]\n",
    "            img_gt = self.train_gt[idx]\n",
    "            return img_tensor, img_gt\n",
    "        else:\n",
    "            return \"The dataset hasn't been constructed properly. Generate again!\"\n",
    "    \n",
    "    # Returns original image size for the dataset    \n",
    "    def get_image_size(self):\n",
    "        return self.image_size\n",
    "        \n",
    "    # Partition dataset according to input subset size and dynamically generate the train/val splits\n",
    "    def generate_dataset(self):\n",
    "        train_set = []\n",
    "        \n",
    "        complete_idx = [idx for idx in range(0, self.complete_size + 1)]\n",
    "        target_samples = int(self.complete_size * self.subset)\n",
    "        # val_samples = int(len(target_samples) * self.val_size)\n",
    "        shuffled = random.sample(complete_idx,len(complete_idx))\n",
    "        \n",
    "        # Pick n (target samples no) idx from the shuffled dataset\n",
    "        dataset_idxs = [shuffled[idx] for idx in range(0, target_samples)]\n",
    "        train_gt = [self.complete_gt[idx] for idx in dataset_idxs]\n",
    "        \n",
    "        # Load images, resize inputs, transform to tensors and generate dataset (or subset)\n",
    "        for gt in train_gt:\n",
    "            img_path = gt['raw_file']\n",
    "            train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                                   transforms.Resize(size = self.resize)])\n",
    "            image = cv2.imread(os.path.join(self.train_dir, img_path))\n",
    "            img_tensor = train_transforms(image)\n",
    "            train_set.append(img_tensor)\n",
    "        \n",
    "        return train_set, train_gt   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
