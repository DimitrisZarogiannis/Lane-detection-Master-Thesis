{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn((1,576,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks transformer class\n",
    "class MaskTransformer(nn.Module):\n",
    "    def __init__(self, image_size = (640,640) ,n_classes = 2, patch_size = 16, depth = 6 ,heads = 8, dim_enc = 768, dim_dec = 512, mlp_dim = 1024, dropout = 0.1):\n",
    "        super(MaskTransformer, self).__init__()\n",
    "        self.dim = dim_enc\n",
    "        self.patch_size = patch_size\n",
    "        self.depth = depth\n",
    "        self.class_n = n_classes\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "        self.d_model = dim_dec\n",
    "        self.scale = self.d_model ** -0.5\n",
    "        self.att_heads = heads\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Define the transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(dim_dec, heads, mlp_dim, dropout)\n",
    "            for _ in range(self.depth)\n",
    "            ])\n",
    "        \n",
    "        # Learnable Class embedding parameter\n",
    "        self.cls_emb = nn.Parameter(torch.randn(1, n_classes,dim_dec))\n",
    "        \n",
    "        # Projection layers for patch embeddings and class embeddings\n",
    "        self.proj_dec = nn.Linear(dim_enc,dim_dec)\n",
    "        self.proj_patch = nn.Parameter(self.scale * torch.randn(dim_dec, dim_dec))\n",
    "        self.proj_classes = nn.Parameter(self.scale * torch.randn(dim_dec, dim_dec))\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.decoder_norm = nn.LayerNorm(dim_dec)\n",
    "        self.mask_norm = nn.LayerNorm(n_classes)\n",
    "        \n",
    "        \n",
    "        # Initialize weights from a random normal distribution for all layers and the class embedding parameter\n",
    "        self.apply(self.init_weights)\n",
    "        init.normal_(self.cls_emb, std=0.02)\n",
    "    \n",
    "    # Init weights method\n",
    "    @staticmethod\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_in')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        H, W = self.image_size\n",
    "        GS = H // self.patch_size\n",
    "\n",
    "        # Project embeddings to mask transformer dim size and expand class embedding(by adding the batch dim) to match these \n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        \n",
    "        # Add the learnable class embedding to the patch embeddings and pass through the transformer blocks\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for blk in self.transformer_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # Split output tensor into patch embeddings and the transformer patch level class embeddings\n",
    "        patches, cls_seg_feat = x[:, : -self.class_n], x[:, -self.class_n :]\n",
    "        patches = patches @ self.proj_patch\n",
    "        cls_seg_feat = cls_seg_feat @ self.proj_classes\n",
    "\n",
    "        # Perform L2 Normalizations over the two tensors\n",
    "        patches = patches / patches.norm(dim=-1, keepdim=True)\n",
    "        cls_seg_feat = cls_seg_feat / cls_seg_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 1. Calculate patch level class scores(as per dot product) by between the normalized patch tensors and the normalized class embeddings\n",
    "        # 2. Reshape the output from (batch,number of patches, classes) to (batch size, classes, height, width)\n",
    "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
    "        masks = self.mask_norm(masks)\n",
    "        masks = rearrange(masks, \"b (h w) n -> b n h w\", h=int(GS))\n",
    "\n",
    "        return masks       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskTransformer(image_size=(384,384),n_classes=1, dim_dec= 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 24, 24])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-5.6671e-08, -7.3756e-07,  2.7428e-08, -4.7659e-08, -3.9736e-08,\n",
       "            2.5518e-08, -2.2287e-08, -3.3461e-07, -3.7178e-07, -9.1061e-07,\n",
       "            3.0523e-09, -3.9563e-08, -1.7503e-08, -1.0877e-07, -1.0289e-07,\n",
       "            9.1736e-08, -9.2361e-08, -2.2415e-07,  4.1944e-07, -4.2552e-08,\n",
       "           -4.0916e-07,  1.9419e-07, -8.4540e-07,  4.8652e-08],\n",
       "          [-1.9286e-07,  4.4176e-07,  9.4099e-07, -1.7902e-08,  8.0209e-08,\n",
       "            1.0372e-07, -1.5885e-06,  6.3817e-08, -1.8562e-07,  5.9067e-08,\n",
       "            3.2308e-07, -3.3988e-07,  8.6113e-08,  1.0192e-06, -1.3037e-07,\n",
       "            9.3491e-08, -1.1120e-07,  8.7559e-08, -7.0695e-09,  7.3977e-08,\n",
       "           -2.3963e-07, -2.5006e-07, -5.3607e-08,  8.0832e-07],\n",
       "          [-8.4898e-08,  1.0473e-07, -1.6218e-08, -1.6943e-07,  8.9449e-10,\n",
       "            8.4814e-07,  1.3022e-08,  4.6543e-07, -2.4182e-07,  1.9075e-07,\n",
       "            5.7920e-08,  7.2318e-07, -3.3806e-07,  1.6051e-06, -6.4804e-07,\n",
       "           -2.8490e-07,  5.3466e-08, -1.1493e-07, -1.9586e-07, -7.0617e-08,\n",
       "            9.4364e-07, -8.3546e-07, -2.3093e-07, -3.7324e-07],\n",
       "          [ 1.6232e-07,  7.7616e-07, -8.2076e-08, -1.1307e-07,  2.4863e-07,\n",
       "           -8.9404e-07, -3.0458e-07,  7.1560e-08,  1.4958e-07, -8.7265e-07,\n",
       "           -4.5341e-07, -1.7000e-07,  4.1908e-07, -2.0934e-08, -3.9981e-08,\n",
       "            1.1685e-07,  1.5368e-07,  2.2832e-08, -4.6424e-07, -2.2168e-07,\n",
       "           -2.2091e-07,  4.4079e-07,  4.0843e-09,  6.0998e-07],\n",
       "          [-8.3426e-07,  1.8406e-08, -1.7320e-07, -8.0036e-08,  7.9847e-08,\n",
       "           -3.4926e-07,  4.7402e-08, -4.1478e-09,  8.6568e-07, -3.9794e-07,\n",
       "           -8.8324e-07, -5.4461e-08, -2.2722e-08,  2.5034e-07, -5.5006e-07,\n",
       "           -3.0126e-08, -4.5855e-07, -1.5690e-06,  1.4827e-07,  1.1102e-07,\n",
       "            3.1063e-07,  4.1415e-07, -5.4405e-08, -1.1694e-07],\n",
       "          [-1.7870e-08,  2.3014e-07, -8.1554e-09,  1.0109e-07, -5.4996e-08,\n",
       "           -1.2095e-07, -5.0434e-09,  1.5181e-06,  1.8456e-07, -7.8151e-07,\n",
       "           -2.5334e-07, -1.9862e-07, -1.9489e-08, -4.6632e-08,  2.3982e-07,\n",
       "           -1.5366e-07,  6.8469e-08,  7.6585e-08, -4.5187e-08,  1.8396e-07,\n",
       "            9.5058e-07, -5.4772e-08, -8.0308e-08,  2.2640e-07],\n",
       "          [-4.7636e-07,  3.0638e-07,  4.4133e-07,  3.0903e-07, -3.0469e-07,\n",
       "            6.1393e-07,  8.8999e-09, -2.9638e-08,  3.4940e-08,  2.8393e-07,\n",
       "           -1.0407e-07, -1.8519e-07,  5.8953e-07, -1.7512e-07,  3.4128e-08,\n",
       "            3.8782e-07, -1.3536e-07, -8.8500e-07, -3.2032e-07,  3.3835e-07,\n",
       "           -1.2333e-07, -6.9444e-07, -9.7098e-08, -3.9283e-07],\n",
       "          [ 3.1857e-07, -2.2585e-07, -2.7918e-07,  5.9299e-10,  1.5275e-07,\n",
       "           -6.6569e-07,  5.2328e-08,  6.6959e-07,  9.0498e-08, -1.0145e-07,\n",
       "            2.1643e-07, -2.3175e-07,  4.0372e-09,  1.1923e-07, -1.3227e-07,\n",
       "            5.5729e-09, -9.5715e-09,  5.8969e-08,  8.2030e-07, -1.3085e-07,\n",
       "            5.1325e-07,  1.3103e-07, -4.2439e-07,  1.3144e-07],\n",
       "          [-2.7516e-07, -4.2124e-07,  1.1075e-07, -2.5315e-08,  2.1100e-07,\n",
       "           -1.4465e-07, -4.5054e-07,  1.1602e-07,  3.8715e-07, -1.7289e-06,\n",
       "            1.8521e-07,  1.4086e-07,  3.1780e-08, -1.6844e-08,  7.8110e-08,\n",
       "            1.6789e-07,  8.8302e-08, -1.9861e-08,  2.5132e-07, -1.6058e-07,\n",
       "           -4.5994e-07,  7.4491e-07,  5.5988e-07, -5.0588e-08],\n",
       "          [-9.4309e-08,  8.2361e-07, -4.2418e-07, -1.4990e-07,  1.7246e-07,\n",
       "           -9.0587e-08, -4.8417e-07, -3.8736e-07,  6.7903e-07,  3.7236e-07,\n",
       "           -5.0935e-07,  2.1604e-07,  1.0359e-07, -7.9104e-08,  3.3860e-08,\n",
       "            1.6756e-08,  9.0011e-07,  2.3252e-07,  4.2587e-07, -1.7957e-07,\n",
       "            2.2298e-07,  1.8369e-07, -4.0295e-09,  1.9432e-08],\n",
       "          [ 1.7089e-07, -5.7883e-07,  3.2366e-07,  1.2278e-11, -1.0998e-07,\n",
       "            3.1990e-07, -3.5109e-07, -6.2629e-08,  9.8780e-08,  2.6159e-07,\n",
       "           -1.3813e-07,  1.4988e-09,  1.6507e-09,  2.8105e-07,  6.6284e-07,\n",
       "            8.5229e-07,  6.6084e-07, -1.1795e-07, -2.7028e-07, -1.1893e-07,\n",
       "            1.8877e-07, -7.8975e-07, -9.4136e-07,  2.4464e-07],\n",
       "          [-7.7653e-08,  5.1139e-07,  1.3408e-06, -2.9531e-07,  7.3680e-07,\n",
       "           -5.1069e-07, -9.4032e-08, -1.6666e-08,  4.3474e-07,  8.9808e-07,\n",
       "            1.3312e-07,  7.9236e-08,  1.0145e-07, -1.8688e-08, -2.9493e-08,\n",
       "            2.0032e-07,  2.7836e-07, -1.0266e-07,  3.4646e-07,  9.7147e-08,\n",
       "           -2.6220e-07,  4.7604e-07,  1.2070e-07, -8.1783e-07],\n",
       "          [ 4.6407e-09,  8.2448e-08,  2.2284e-07, -4.5843e-07,  1.9649e-07,\n",
       "            5.0334e-07, -4.1455e-07,  5.4707e-07, -1.6470e-07,  2.5014e-07,\n",
       "            2.1530e-07,  3.5056e-08, -6.2231e-08, -4.3836e-07, -3.2071e-07,\n",
       "           -1.8200e-07,  2.5806e-07, -1.6090e-07, -6.0589e-08,  3.0100e-09,\n",
       "            9.4566e-08,  6.6548e-09, -1.5856e-07,  1.9891e-07],\n",
       "          [ 6.1623e-07, -1.2139e-07,  1.5262e-07,  1.2854e-07, -2.9875e-07,\n",
       "           -2.7820e-08,  3.0499e-08,  1.5988e-06, -6.2578e-08, -2.3903e-07,\n",
       "            1.0549e-07,  4.5346e-08, -6.1453e-07,  7.7737e-08, -7.5461e-08,\n",
       "            8.9053e-09, -9.1222e-07, -4.8869e-07,  9.7429e-08, -2.8415e-07,\n",
       "            7.6353e-08,  1.0995e-07,  1.9967e-07, -7.6468e-08],\n",
       "          [ 2.7311e-07,  7.5676e-07, -8.7310e-08,  2.4460e-08, -6.4697e-07,\n",
       "           -7.5853e-07,  8.9222e-07, -1.2509e-07, -1.1706e-07, -1.5164e-09,\n",
       "           -2.5555e-07, -8.0399e-09, -1.3083e-07,  2.2244e-07,  9.8344e-07,\n",
       "           -5.9584e-07,  8.5277e-07, -8.5956e-09, -1.0156e-07,  9.4799e-07,\n",
       "            2.6041e-08,  1.0739e-08,  7.3968e-07,  8.7786e-08],\n",
       "          [ 6.0682e-07, -1.8594e-07,  9.7420e-08,  1.9558e-08,  2.6503e-09,\n",
       "           -7.0068e-07,  1.7244e-07,  2.5420e-07, -1.8271e-07, -1.3168e-08,\n",
       "            2.1269e-08, -1.0631e-08,  1.3380e-07,  2.3597e-07, -1.8280e-07,\n",
       "            1.5411e-07,  2.3133e-08,  3.7236e-07,  1.3694e-07, -2.0465e-07,\n",
       "            3.4502e-08,  8.6748e-08,  2.8755e-07,  2.3728e-07],\n",
       "          [-8.7913e-07,  2.9011e-08, -2.8035e-09, -5.9836e-07,  3.4481e-07,\n",
       "           -3.4235e-08,  7.5038e-08, -9.5315e-09,  4.4041e-07, -1.0640e-08,\n",
       "            3.4949e-08, -1.5917e-07,  2.2689e-07, -3.1287e-07,  9.9999e-08,\n",
       "           -2.2980e-07, -6.1263e-07,  4.5573e-07,  1.0390e-07, -1.1356e-08,\n",
       "            8.2650e-10, -4.1975e-08, -2.1828e-07,  3.3905e-07],\n",
       "          [-6.4659e-08, -6.8103e-08, -1.9387e-07, -2.2340e-07, -9.0098e-08,\n",
       "            3.9247e-07, -4.6803e-07, -1.0057e-07,  3.8153e-07,  3.5686e-07,\n",
       "            1.8693e-07, -1.8250e-08, -2.4288e-07,  1.6227e-07,  5.6010e-08,\n",
       "           -2.5744e-08,  4.3611e-07, -3.6277e-07,  2.1413e-07,  4.4005e-07,\n",
       "           -8.1355e-07,  9.3740e-08, -2.1789e-07,  1.1558e-08],\n",
       "          [-2.3600e-07,  1.6396e-07,  6.7215e-07, -2.0430e-07, -2.5779e-07,\n",
       "           -9.7602e-08, -7.0850e-09,  4.6684e-07, -7.9913e-09,  9.8872e-08,\n",
       "            2.1950e-07,  3.3798e-07, -2.1352e-07, -8.3768e-08, -1.1422e-07,\n",
       "            6.6149e-08, -1.9228e-07,  1.0322e-07, -8.6992e-07, -2.5522e-08,\n",
       "           -2.1581e-07,  1.1363e-08, -1.3182e-07,  5.0436e-07],\n",
       "          [ 1.4487e-07,  8.8013e-07,  3.7965e-07,  1.1178e-07, -4.7685e-07,\n",
       "           -1.1848e-07,  8.6877e-07,  3.2296e-07, -2.2191e-07,  2.5523e-08,\n",
       "           -2.5624e-07, -3.7482e-07,  5.6418e-08, -1.7459e-07,  1.3212e-06,\n",
       "            5.2019e-09,  2.6545e-07, -3.2670e-08, -6.3734e-08, -1.4177e-07,\n",
       "            2.7313e-07,  3.8324e-07, -2.9464e-08,  1.6709e-07],\n",
       "          [-2.7044e-07,  6.6256e-07, -1.6398e-07,  9.1502e-08,  1.8470e-07,\n",
       "            4.6022e-07,  1.0890e-06,  3.3583e-07,  2.9275e-07, -1.0167e-07,\n",
       "           -4.1606e-07,  5.2489e-08,  2.9740e-07, -2.1992e-07, -5.9768e-08,\n",
       "           -2.0920e-07, -4.0594e-08, -2.3393e-07,  1.4744e-07, -2.4651e-07,\n",
       "            4.5839e-09,  8.8557e-08,  6.5588e-07,  6.6207e-09],\n",
       "          [-7.6278e-07,  1.3478e-07, -1.1288e-07, -2.4379e-09, -2.7005e-07,\n",
       "            5.2099e-08, -2.0633e-07,  1.8928e-07, -4.3972e-07, -9.4449e-08,\n",
       "           -2.5726e-07,  4.0865e-08, -2.3546e-07,  3.9081e-07,  3.8015e-07,\n",
       "            1.4974e-07, -6.7351e-07, -7.4204e-08, -4.3590e-07, -2.4658e-07,\n",
       "            3.4549e-07, -4.0100e-07, -3.1592e-08, -1.2869e-09],\n",
       "          [-8.0852e-07,  2.6424e-08,  7.7512e-07,  7.0804e-09,  7.9583e-07,\n",
       "            4.6777e-08, -1.9274e-07, -1.0929e-08,  4.2276e-07, -8.0431e-08,\n",
       "           -8.9160e-08, -3.4797e-08,  3.4483e-07,  2.2744e-07, -8.7403e-08,\n",
       "            1.7190e-07, -4.3276e-07, -1.2393e-08,  2.9480e-07,  7.3643e-08,\n",
       "            1.8961e-07,  6.4749e-07,  4.7985e-07, -1.7458e-07],\n",
       "          [-1.4741e-08, -1.5814e-07,  2.1231e-07, -2.0049e-07, -4.6254e-07,\n",
       "            6.4712e-08, -7.0406e-07, -1.2527e-07,  4.0116e-07,  1.3067e-06,\n",
       "            1.3848e-07, -9.8322e-07,  7.5856e-07, -1.3533e-09, -6.3541e-07,\n",
       "           -2.3447e-07,  1.8388e-06, -2.9759e-07,  1.8231e-07, -1.9172e-07,\n",
       "           -1.2597e-07,  2.5444e-08,  3.4354e-07, -2.9009e-08]]]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
