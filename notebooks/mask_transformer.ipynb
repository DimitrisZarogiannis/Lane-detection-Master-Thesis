{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count pipeline trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks transformer class\n",
    "class MaskTransformer(nn.Module):\n",
    "    def __init__(self, image_size = (640,640) ,n_classes = 1, patch_size = 16, depth = 2 ,heads = 8, dim_enc = 768, dim_dec = 768, mlp_dim = 3072, dropout = 0.1, pre_train = False):\n",
    "        super(MaskTransformer, self).__init__()\n",
    "        self.dim = dim_enc\n",
    "        self.patch_size = patch_size\n",
    "        self.depth = depth\n",
    "        self.class_n = n_classes\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "        self.d_model = dim_dec\n",
    "        self.scale = self.d_model ** -0.5\n",
    "        self.att_heads = heads\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Define the transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(dim_dec, heads, mlp_dim, dropout)\n",
    "            for _ in range(self.depth)\n",
    "            ])\n",
    "        \n",
    "        # Learnable Class embedding parameter\n",
    "        self.cls_emb = nn.Parameter(torch.randn(1, n_classes,dim_dec))\n",
    "        \n",
    "        # Projection layers for patch embeddings and class embeddings\n",
    "        self.proj_dec = nn.Linear(dim_enc,dim_dec)\n",
    "        self.proj_patch = nn.Parameter(self.scale * torch.randn(dim_dec, dim_dec))\n",
    "        self.proj_classes = nn.Parameter(self.scale * torch.randn(dim_dec, dim_dec))\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.decoder_norm = nn.LayerNorm(dim_dec)\n",
    "        self.mask_norm = nn.LayerNorm(n_classes)\n",
    "        \n",
    "        \n",
    "        # Initialize weights from a random normal distribution for all layers and the class embedding parameter\n",
    "        if pre_train:\n",
    "            self.load_pretrained_weights()\n",
    "            init.normal_(self.cls_emb, std=0.02)\n",
    "            init.xavier_uniform_(self.proj_dec.weight)\n",
    "            init.normal_(self.proj_classes, std=0.02)\n",
    "            init.normal_(self.proj_patch)\n",
    "            self.decoder_norm.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            self.decoder_norm.bias.data.zero_()\n",
    "            self.mask_norm.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            self.mask_norm.bias.data.zero_()\n",
    "        else:\n",
    "            self.apply(self.init_weights)\n",
    "        \n",
    "    \n",
    "    # Init weights method\n",
    "    @staticmethod\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_in')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        H, W = self.image_size\n",
    "        GS = H // self.patch_size\n",
    "\n",
    "        # Project embeddings to mask transformer dim size and expand class embedding(by adding the batch dim) to match these \n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        \n",
    "        # Add the learnable class embedding to the patch embeddings and pass through the transformer blocks\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for blk in self.transformer_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # Split output tensor into patch embeddings and the transformer patch level class embeddings\n",
    "        patches, cls_seg_feat = x[:, : -self.class_n], x[:, -self.class_n :]\n",
    "        patches = patches @ self.proj_patch\n",
    "        cls_seg_feat = cls_seg_feat @ self.proj_classes\n",
    "\n",
    "        # Perform L2 Normalizations over the two tensors\n",
    "        patches = patches / patches.norm(dim=-1, keepdim=True)\n",
    "        cls_seg_feat = cls_seg_feat / cls_seg_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 1. Calculate patch level class scores(as per dot product) by between the normalized patch tensors and the normalized class embeddings\n",
    "        # 2. Reshape the output from (batch,number of patches, classes) to (batch size, classes, height, width)\n",
    "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
    "        masks = self.mask_norm(masks)\n",
    "        masks = rearrange(masks, \"b (h w) n -> b n h w\", h=int(GS))\n",
    "\n",
    "        return masks       \n",
    "    \n",
    "     # Load pre-trained weights method\n",
    "    def load_pretrained_weights(self):\n",
    "        map_dict = self.generate_mapping_dict()\n",
    "        pretrained = torch.load('../pre-trained/pretrained_mask_trans_2.pth')\n",
    "        model_state_dict = self.state_dict()\n",
    "        \n",
    "    \n",
    "        # create new state dict with mapped keys\n",
    "        new_state_dict = {}\n",
    "        for key in pretrained['model'].keys():\n",
    "            if key in map_dict:\n",
    "                new_state_dict[map_dict[key]] = pretrained['model'][key]\n",
    "            else:\n",
    "                if key in model_state_dict:\n",
    "                    new_state_dict[key] = pretrained['model'][key]\n",
    "        \n",
    "        \n",
    "        # Load the mapped weights into our ViT model\n",
    "        self.load_state_dict(new_state_dict, strict= False)\n",
    "        self.freeze_transformer_layers(list(map_dict.values()))\n",
    "        print('Succesfully created Mask Transformer with pre-trained weights...!')\n",
    "        print('Froze transformer layers for training..!')\n",
    "        \n",
    "    def freeze_transformer_layers(self, transformer_layers_dict):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in transformer_layers_dict:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    # Generate the mapping dict renaming the pretrained weights layers names to the desired format\n",
    "    def generate_mapping_dict(self):\n",
    "        mapping = {}\n",
    "        for i in range(2):\n",
    "            prefix = f'decoder.blocks.{i}.'\n",
    "\n",
    "            mapping[f'{prefix}norm1.bias'] = f'transformer_blocks.{i}.norm1.bias'\n",
    "            mapping[f'{prefix}norm1.weight'] = f'transformer_blocks.{i}.norm1.weight'\n",
    "            mapping[f'{prefix}norm2.bias'] = f'transformer_blocks.{i}.norm2.bias'\n",
    "            mapping[f'{prefix}norm2.weight'] = f'transformer_blocks.{i}.norm2.weight'\n",
    "            mapping[f'{prefix}mlp.fc1.bias'] = f'transformer_blocks.{i}.linear1.bias'\n",
    "            mapping[f'{prefix}mlp.fc1.weight'] = f'transformer_blocks.{i}.linear1.weight'\n",
    "            mapping[f'{prefix}mlp.fc2.bias'] = f'transformer_blocks.{i}.linear2.bias' \n",
    "            mapping[f'{prefix}mlp.fc2.weight'] = f'transformer_blocks.{i}.linear2.weight'\n",
    "            mapping[f'{prefix}attn.proj.bias'] = f'transformer_blocks.{i}.self_attn.out_proj.bias'\n",
    "            mapping[f'{prefix}attn.proj.weight'] = f'transformer_blocks.{i}.self_attn.out_proj.weight'\n",
    "            mapping[f'{prefix}attn.qkv.bias'] = f'transformer_blocks.{i}.self_attn.in_proj_bias'\n",
    "            mapping[f'{prefix}attn.qkv.weight'] = f'transformer_blocks.{i}.self_attn.in_proj_weight'\n",
    "        return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created Mask Transformer with pre-trained weights...!\n",
      "Froze transformer layers for training..!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1772546"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MaskTransformer(image_size=(640,640),n_classes=1, dim_dec= 768, depth= 2, pre_train=True)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.002, val_size= 0.2)\n",
    "\n",
    "# Create train and validation splits / Always use del dataset to free memory after this\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created ViT with pre-trained weights...!\n"
     ]
    }
   ],
   "source": [
    "from vit import ViT\n",
    "import utils\n",
    "\n",
    "encoder = ViT(image_size=640, patch_size=16, num_classes=1, dim=768, depth=12, heads=12, \n",
    "            mlp_dim=3072, dropout=0.1,load_pre= True, pre_trained_path= '../pre-trained/jx_vit_base_p16_224-80ecf9dd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "img,gt = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1600, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = encoder(img.unsqueeze(0))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 40, 40])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = model(sample)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
