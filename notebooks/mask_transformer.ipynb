{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks transformer class\n",
    "class MaskTransformer(nn.Module):\n",
    "    def __init__(self, image_size = (640,640) ,n_classes = 2, patch_size = 16, depth = 6 ,heads = 8, dim_enc = 768, dim_dec = 512, mlp_dim = 1024, dropout = 0.1):\n",
    "        self.dim = dim_enc\n",
    "        self.patch_size = patch_size\n",
    "        self.depth = depth\n",
    "        self.class_n = n_classes\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "        self.d_model = dim_dec\n",
    "        self.scale = self.d_model ** -0.5\n",
    "        self.att_heads = heads\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Define the transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(dim_dec, heads, mlp_dim, dropout)\n",
    "            for _ in range(self.depth)\n",
    "            ])\n",
    "        \n",
    "        # Learnable Class embedding parameter\n",
    "        self.cls_emb = nn.Parameter(torch.randn(1, n_classes,dim_dec))\n",
    "        \n",
    "        # Projection layers for patch embeddings and class embeddings\n",
    "        self.proj_dec = nn.Linear(dim_enc,dim_dec)\n",
    "        self.proj_patch = nn.Parameter(self.scale * torch.randn(dim_dec, dim_dec))\n",
    "        self.proj_classes = nn.Parameter(self.scale * torch.randn(dim_dec, dim_dec))\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.decoder_norm = nn.LayerNorm(dim_dec)\n",
    "        self.mask_norm = nn.LayerNorm(n_classes)\n",
    "        \n",
    "        \n",
    "        # Initialize weights from a random normal distribution for all layers and the class embedding parameter\n",
    "        self.apply(self.init_weights)\n",
    "        init.normal_(self.cls_emb, std=0.02)\n",
    "    \n",
    "    # Init weights method\n",
    "    @staticmethod\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_in')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        H, W = self.image_size\n",
    "        GS = H // self.patch_size\n",
    "\n",
    "        # Project embeddings to mask transformer dim size and expand class embedding(by adding the batch dim) to match these \n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        \n",
    "        # Add the learnable class embedding to the patch embeddings and pass through the transformer blocks\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # Split output tensor into patch embeddings and the transformer patch level class embeddings\n",
    "        patches, cls_seg_feat = x[:, : -self.n_cls], x[:, -self.n_cls :]\n",
    "        patches = patches @ self.proj_patch\n",
    "        cls_seg_feat = cls_seg_feat @ self.proj_classes\n",
    "\n",
    "        # Perform L2 Normalizations over the two tensors\n",
    "        patches = patches / patches.norm(dim=-1, keepdim=True)\n",
    "        cls_seg_feat = cls_seg_feat / cls_seg_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 1. Calculate patch level class scores(as per dot product) by between the normalized patch tensors and the normalized class embeddings\n",
    "        # 2. Reshape the output from (batch,number of patches, classes) to (batch size, classes, height, width)\n",
    "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
    "        masks = self.mask_norm(masks)\n",
    "        masks = rearrange(masks, \"b (h w) n -> b n h w\", h=int(GS))\n",
    "\n",
    "        return masks       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
