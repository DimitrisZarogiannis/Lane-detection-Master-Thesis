{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from vit import ViT\n",
    "from mlp_decoder import DecoderMLP\n",
    "from segnet_backbone import SegNet\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-End pipeline (CNN + ViT + MLP)\n",
    "class Pipeline(nn.Module):\n",
    "    def __init__(self, feat_extractor, vit_variant, mlp_head, image_size = (448,448)):\n",
    "        super().__init__()\n",
    "        self.cnn = feat_extractor\n",
    "        self.transformer = vit_variant\n",
    "        self.mlp = mlp_head\n",
    "        self.image_size = image_size\n",
    "        self.lane_threshold = 0.5\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "        # create the upsampling layers\n",
    "        self.upsample_layers = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # CNN branch for feature extraction\n",
    "        x,_ = self.cnn(im)\n",
    "        \n",
    "        # Transform standardized feature maps using the ViT\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Remove the learnable class token before patch classification\n",
    "        x = x[:, 1:]\n",
    "        \n",
    "        # Perform patch level classification (0 for background/ 1 for lane)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        # Interpolate patch level class annotatations to pixel level and transform to original image size\n",
    "        # logits = F.interpolate(x, size=(H, W), mode=\"bilinear\")\n",
    "        # upsample the predictions using the ConvTranspose2d layers\n",
    "        for upsample_layer in self.upsample_layers:\n",
    "            x = upsample_layer(x)\n",
    "        \n",
    "        logits = x\n",
    "        probs = self.activation(logits)\n",
    "\n",
    "        return logits, probs\n",
    "        \n",
    "        \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Load trained model\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the pipeline with schedule and augmentations\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, weight_decay=0, SGD_momentum = 0.9, lr_scheduler=False, lane_weight = None, save_path = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "\n",
    "    #create seperate parameter groups for the different networks\n",
    "    segnet_params = [p for p in model.cnn.parameters() if p.requires_grad]\n",
    "    mlp_params = [p for p in model.mlp.parameters() if p.requires_grad]\n",
    "    vit_params = [p for p in model.transformer.parameters() if p.requires_grad]\n",
    "    \n",
    "    # Define your learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        # Define the optimizer with different learning rates for the parameter groups\n",
    "        optimizer = optim.SGD([\n",
    "            {'params' : segnet_params, 'lr' : lr},\n",
    "            {'params' : mlp_params, 'lr' : lr},\n",
    "            {'params' : vit_params, 'lr' : 0.001}\n",
    "            ], momentum=SGD_momentum, weight_decay= weight_decay)\n",
    "        # Set the scheduler\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=1e-4, min_lr=[0.0001,0.0001, 0.001])\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(),lr = lr, momentum= SGD_momentum, weight_decay= weight_decay)\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    gt_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "  \n",
    "    train_augmentations = transforms.Compose([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                                            transforms.ColorJitter(brightness=0.35, contrast=0.2, saturation=0.4, hue=0.1)])\n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Metrics collection for plotting\n",
    "    train_losses = []\n",
    "    train_f1_scores = []\n",
    "    train_iou_scores = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    val_iou_scores = []\n",
    "    \n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = gt_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)]\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "\n",
    "            inputs = train_augmentations(inputs).to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "        \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    logits,outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    val_loss = criterion(logits.to(device),targets)\n",
    "                    val_loss += val_loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "    \n",
    "        # Collect metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1_scores.append(train_f1.cpu().item())\n",
    "        train_iou_scores.append(train_iou.cpu().item())\n",
    "    \n",
    "        if val_loader:\n",
    "            val_losses.append(val_loss.cpu().item())\n",
    "            val_f1_scores.append(val_f1.cpu().item())\n",
    "            val_iou_scores.append(val_iou.cpu().item())\n",
    "            \n",
    "                \n",
    "        # Check if currect val_loss is the best and save the weights\n",
    "        if val_loader and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Save the model weights\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), '../models/best_pipeline.pth')\n",
    "        \n",
    "        # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))\n",
    "            if val_loader:\n",
    "                print('Val_Loss: {} - Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_loss,val_f1,val_iou))\n",
    "        \n",
    "        # Save the model after 15 epochs\n",
    "        # if epoch + 1 == 15:\n",
    "        #     torch.save(model.state_dict(), '../models/pipeline_15.pth')\n",
    "    \n",
    "    if val_loader:\n",
    "        \n",
    "        # Log metrics\n",
    "        metrics_dict = {'train_l': train_losses, 'train_f1': train_f1_scores, 'train_iou': train_iou_scores,\n",
    "                    'val_l': val_losses, 'val_f1_scores': val_f1_scores, 'val_iou': val_iou_scores}\n",
    "        \n",
    "        # Save log_dict to a JSON file\n",
    "        with open('../logs/plot_metrics.json', 'w') as f:\n",
    "            json.dump(metrics_dict, f)\n",
    "            \n",
    "        return train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores\n",
    "    else:\n",
    "        return train_losses,train_f1_scores,train_iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 448, 448])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = torch.rand(2,3,448,448)\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for ViT : 5966400\n"
     ]
    }
   ],
   "source": [
    "# Initialize ViT Tiny\n",
    "vit_tiny = ViT(image_size=448, patch_size=16, num_classes=1, dim=192, depth=6, heads=3, \n",
    "                      mlp_dim=768, dropout=0.1,load_pre= False)\n",
    "print(f'Number of trainable parameters for ViT : {vit_tiny.count_parameters()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 785, 192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_tiny.eval()\n",
    "test = vit_tiny(test_img)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for MLP : 234313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize MLP\n",
    "patch_classifier = DecoderMLP(n_classes = 4, d_encoder = 192, image_size=(448,448))\n",
    "print(f'Number of trainable parameters for MLP : {patch_classifier.count_parameters()}')\n",
    "\n",
    "test2 = patch_classifier(test[:,1:])\n",
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "pixel_preds = torch.nn.functional.interpolate(test2, size=(448, 448), mode='bilinear')\n",
    "print(pixel_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for SegNet : 29443587\n",
      "Number of trainable parameters for ViT : 5966400\n",
      "Number of trainable parameters for MLP : 233539\n"
     ]
    }
   ],
   "source": [
    "# Initialize SegNet\n",
    "cnn = SegNet()\n",
    "print(f'Number of trainable parameters for SegNet : {cnn.count_parameters()}')\n",
    "\n",
    "# Initialize ViT Tiny\n",
    "vit_tiny = ViT(image_size=448, patch_size=16, num_classes=1, dim=192, depth=6, heads=3, \n",
    "                      mlp_dim=768, dropout=0.1,load_pre= False)\n",
    "print(f'Number of trainable parameters for ViT : {vit_tiny.count_parameters()}')\n",
    "\n",
    "# Initialize MLP\n",
    "patch_classifier = DecoderMLP(n_classes = 16, d_encoder = 192, image_size=(448,448))\n",
    "print(f'Number of trainable parameters for MLP : {patch_classifier.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for Pipeline : 35643546\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(cnn, vit_tiny, patch_classifier, image_size= (448,448))\n",
    "print(f'Number of trainable parameters for Pipeline : {model.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 28, 28])\n",
      "torch.Size([2, 1, 56, 56])\n",
      "torch.Size([2, 1, 112, 112])\n",
      "torch.Size([2, 1, 224, 224])\n",
      "torch.Size([2, 1, 448, 448])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sigmoid(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model(test_img)\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb Cell 11\u001b[0m in \u001b[0;36mPipeline.forward\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(logits)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits, probs\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/activation.py:295\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49msigmoid(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: sigmoid(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "model(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "# annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "# test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# test_annotated = os.listdir(annotated_dir_test)\n",
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple test dataset + ground truth dictionary\n",
    "# test_annotations = list()\n",
    "# for gt_file in test_annotated:\n",
    "#     path = os.path.join(annotated_dir_test,gt_file)\n",
    "#     json_gt = [json.loads(line) for line in open(path)]\n",
    "#     test_annotations.append(json_gt)\n",
    "    \n",
    "# test_annotations = [a for f in test_annotations for a in f]\n",
    "\n",
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.001)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for train and validation \n",
    "train_loader = DataLoader(train_set, batch_size= 1,shuffle= True, drop_last= True, num_workers= 4) \n",
    "validation_loader = DataLoader(validation_set,batch_size= 1, shuffle= True, drop_last= True, num_workers= 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Train Loss: 1.0259 - Train_IoU: 0.09748 - Train_F1: 0.17720\n",
      "Val_Loss: 2.835956573486328 - Val_F1: 0.06259  - Val_IoU: 0.03231 \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores = train(model, train_loader,val_loader= validation_loader , num_epochs= 1, \n",
    "                                                                                        lane_weight = pos_weight, lr = 0.01, SGD_momentum= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
