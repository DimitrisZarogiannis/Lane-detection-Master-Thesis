{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from vit import ViT\n",
    "from mlp_decoder import DecoderMLP\n",
    "from segnet_backbone import SegNet\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-End pipeline (CNN + ViT + MLP)\n",
    "class Pipeline(nn.Module):\n",
    "    def __init__(self, feat_extractor, vit_variant, mlp_head, image_size = (448,448)):\n",
    "        super().__init__()\n",
    "        self.cnn = feat_extractor\n",
    "        self.transformer = vit_variant\n",
    "        self.mlp = mlp_head\n",
    "        self.image_size = image_size\n",
    "        self.lane_threshold = 0.5\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # CNN branch for feature extraction\n",
    "        x,_ = self.cnn(im)\n",
    "        \n",
    "        # Standardize featmaps to prevent exploding gradients and help the ViT perform better\n",
    "        x = self.standarize_layer(x)\n",
    "        \n",
    "        # Transform standardized feature maps using the ViT\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Perform patch level classification (0 for background/ 1 for lane)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        # Interpolate patch level class annotatations to pixel level and transform to original image size\n",
    "        logits = F.interpolate(x, size=(H, W), mode=\"bilinear\")\n",
    "        \n",
    "        probs = self.activation(logits)\n",
    "\n",
    "        return logits, probs\n",
    "        \n",
    "        \n",
    "    # Standardize feature maps from SegNet layer\n",
    "    def standarize_layer(self,featmaps):\n",
    "        # Compute mean and standard deviation of each channel\n",
    "        mean = torch.mean(featmaps, dim=[0, 2, 3], keepdim=True)\n",
    "        std = torch.std(featmaps, dim=[0, 2, 3], keepdim=True)\n",
    "\n",
    "        # Normalize each channel to have zero mean and unit variance\n",
    "        normal_featmaps = (featmaps - mean) / std\n",
    "        return normal_featmaps\n",
    "        \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Load trained model\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the pipeline with schedule and augmentations\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, weight_decay=0, SGD_momentum = 0.9, lr_scheduler=False, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "\n",
    "    #create seperate parameter groups for the different networks\n",
    "    segnet_params = [p for p in model.cnn.parameters() if p.requires_grad]\n",
    "    mlp_params = [p for p in model.mlp.parameters() if p.requires_grad]\n",
    "    vit_params = [p for p in model.transformer.parameters() if p.requires_grad]\n",
    "    \n",
    "    # Define your learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        # Define the optimizer with different learning rates for the parameter groups\n",
    "        optimizer = optim.SGD([\n",
    "            {'params' : segnet_params, 'lr' : lr},\n",
    "            {'params' : mlp_params, 'lr' : lr},\n",
    "            {'params' : vit_params, 'lr' : 0.001}\n",
    "            ], momentum=SGD_momentum, weight_decay= weight_decay)\n",
    "        # Set the scheduler\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=1e-4, min_lr=[0.0001,0.0001, 0.001])\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(),lr = lr, momentum= SGD_momentum, weight_decay= weight_decay)\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    gt_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "  \n",
    "    train_augmentations = transforms.Compose([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                                            transforms.ColorJitter(brightness=0.35, contrast=0.2, saturation=0.4, hue=0.1)])\n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Metrics collection for plotting\n",
    "    train_losses = []\n",
    "    train_f1_scores = []\n",
    "    train_iou_scores = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    val_iou_scores = []\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = gt_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)]\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "\n",
    "            inputs = train_augmentations(inputs).to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "        \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    logits,outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    val_loss = criterion(logits.to(device),targets)\n",
    "                    val_loss += val_loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "        # Collect metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        train_iou_scores.append(train_iou)\n",
    "    \n",
    "        if val_loader:\n",
    "            val_losses.append(val_loss)\n",
    "            val_f1_scores.append(val_f1)\n",
    "            val_iou_scores.append(val_iou)\n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            # scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))\n",
    "            \n",
    "            if val_loader:\n",
    "                print('Val_Loss: {} - Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_loss,val_f1,val_iou))\n",
    "    \n",
    "    if val_loader:\n",
    "        return train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores\n",
    "    else:\n",
    "        return train_losses,train_f1_scores,train_iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 448, 448])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = torch.rand(1,3,448,448)\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for SegNet : 29443587\n",
      "Number of trainable parameters for ViT : 2869440\n",
      "Number of trainable parameters for MLP : 233537\n"
     ]
    }
   ],
   "source": [
    "# Initialize SegNet\n",
    "cnn = SegNet()\n",
    "print(f'Number of trainable parameters for SegNet : {cnn.count_parameters()}')\n",
    "\n",
    "# Initialize ViT Tiny\n",
    "vit_tiny = ViT(image_size=448, patch_size=16, num_classes=1, dim=192, depth=6, heads=3, \n",
    "                      mlp_dim=768, dropout=0.1,load_pre= False)\n",
    "print(f'Number of trainable parameters for ViT : {vit_tiny.count_parameters()}')\n",
    "\n",
    "# Initialize MLP\n",
    "patch_classifier = DecoderMLP(n_classes = 1, d_encoder = 192, image_size=(448,448))\n",
    "print(f'Number of trainable parameters for MLP : {patch_classifier.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for Pipeline : 32546564\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(cnn, vit_tiny, patch_classifier, image_size= (448,448))\n",
    "print(f'Number of trainable parameters for Pipeline : {model.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "# annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "# test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# test_annotated = os.listdir(annotated_dir_test)\n",
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple test dataset + ground truth dictionary\n",
    "# test_annotations = list()\n",
    "# for gt_file in test_annotated:\n",
    "#     path = os.path.join(annotated_dir_test,gt_file)\n",
    "#     json_gt = [json.loads(line) for line in open(path)]\n",
    "#     test_annotations.append(json_gt)\n",
    "    \n",
    "# test_annotations = [a for f in test_annotations for a in f]\n",
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.001)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for train and validation \n",
    "train_loader = DataLoader(train_set, batch_size= 1,shuffle= True, drop_last= True, num_workers= 4) \n",
    "validation_loader = DataLoader(validation_set,batch_size= 1, shuffle= True, drop_last= True, num_workers= 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores = train(model, train_loader,val_loader= validation_loader , num_epochs= 1, \n",
    "                                                                                        lane_weight = pos_weight, lr = 0.01, SGD_momentum= 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
