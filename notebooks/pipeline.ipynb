{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from vit import ViT\n",
    "from mlp_decoder import DecoderMLP\n",
    "from segnet_backbone import SegNet\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mload()\n",
      "\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'f'"
     ]
    }
   ],
   "source": [
    "torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-End pipeline (CNN + ViT + MLP)\n",
    "class Pipeline(nn.Module):\n",
    "    def __init__(self, feat_extractor, vit_variant, mlp_head, image_size = (448,448)):\n",
    "        super().__init__()\n",
    "        self.cnn = feat_extractor\n",
    "        self.transformer = vit_variant\n",
    "        self.mlp = mlp_head\n",
    "        self.image_size = image_size\n",
    "        self.lane_threshold = 0.5\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # CNN branch for feature extraction\n",
    "        x,_ = self.cnn(im)\n",
    "        \n",
    "        # Transform standardized feature maps using the ViT\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Remove the learnable class token before patch classification\n",
    "        x = x[:, 1:]\n",
    "        \n",
    "        # Perform patch level classification (0 for background/ 1 for lane)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # Interpolate patch level class annotatations to pixel level and transform to original image size\n",
    "        logits = F.interpolate(x, size=(H, W), mode=\"bilinear\")\n",
    "        \n",
    "        probs = self.activation(logits)\n",
    "\n",
    "        return logits, probs\n",
    "\n",
    "    # Make a single prediction \n",
    "    def predict(self,x):\n",
    "        self.eval()\n",
    "        logits,probs = self.forward(x)\n",
    "        prediction = torch.where(probs > self.lane_threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
    "        return prediction\n",
    "    \n",
    "        \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Load trained model\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')), strict= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the pipeline with schedule and augmentations\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, weight_decay=0, SGD_momentum = 0.9, lr_scheduler=False, lane_weight = None, save_path = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "\n",
    "    #create seperate parameter groups for the different networks\n",
    "    segnet_params = [p for p in model.cnn.parameters() if p.requires_grad]\n",
    "    mlp_params = [p for p in model.mlp.parameters() if p.requires_grad]\n",
    "    vit_params = [p for p in model.transformer.parameters() if p.requires_grad]\n",
    "    \n",
    "    # Define your learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        # Define the optimizer with different learning rates for the parameter groups\n",
    "        optimizer = optim.SGD([\n",
    "            {'params' : segnet_params, 'lr' : lr},\n",
    "            {'params' : mlp_params, 'lr' : lr},\n",
    "            {'params' : vit_params, 'lr' : 0.001}\n",
    "            ], momentum=SGD_momentum, weight_decay= weight_decay)\n",
    "        # Set the scheduler\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=1e-4, min_lr=[0.0001,0.0001, 0.001])\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(),lr = lr, momentum= SGD_momentum, weight_decay= weight_decay)\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    gt_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "  \n",
    "    train_augmentations = transforms.Compose([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                                            transforms.ColorJitter(brightness=0.35, contrast=0.2, saturation=0.4, hue=0.1)])\n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Metrics collection for plotting\n",
    "    train_losses = []\n",
    "    train_f1_scores = []\n",
    "    train_iou_scores = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    val_iou_scores = []\n",
    "    \n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = gt_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)]\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "\n",
    "            inputs = train_augmentations(inputs).to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "        \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    logits,outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    val_loss = criterion(logits.to(device),targets)\n",
    "                    val_loss += val_loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "    \n",
    "        # Collect metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1_scores.append(train_f1.cpu().item())\n",
    "        train_iou_scores.append(train_iou.cpu().item())\n",
    "    \n",
    "        if val_loader:\n",
    "            val_losses.append(val_loss.cpu().item())\n",
    "            val_f1_scores.append(val_f1.cpu().item())\n",
    "            val_iou_scores.append(val_iou.cpu().item())\n",
    "            \n",
    "                \n",
    "        # Check if currect val_loss is the best and save the weights\n",
    "        if val_loader and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Save the model weights\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), '../models/best_pipeline.pth')\n",
    "        \n",
    "        # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))\n",
    "            if val_loader:\n",
    "                print('Val_Loss: {} - Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_loss,val_f1,val_iou))\n",
    "        \n",
    "    \n",
    "    if val_loader:\n",
    "        return train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores\n",
    "    else:\n",
    "        return train_losses,train_f1_scores,train_iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dict succesfully!\n",
      "Number of trainable parameters for SegNet : 0\n",
      "Number of trainable parameters for ViT : 5966400\n",
      "Number of trainable parameters for MLP : 233539\n"
     ]
    }
   ],
   "source": [
    "# Initialize SegNet\n",
    "cnn = SegNet()\n",
    "cnn.load_weights('../models/best_segnet.pth')\n",
    "cnn.freeze_all_but_some([])\n",
    "print(f'Number of trainable parameters for SegNet : {cnn.count_parameters()}')\n",
    "\n",
    "# Initialize ViT Tiny\n",
    "vit_tiny = ViT(image_size=448, patch_size=16, num_classes=1, dim=192, depth=6, heads=3, \n",
    "                      mlp_dim=768, dropout=0.1,load_pre= False)\n",
    "print(f'Number of trainable parameters for ViT : {vit_tiny.count_parameters()}')\n",
    "\n",
    "# Initialize MLP\n",
    "patch_classifier = DecoderMLP(n_classes = 1, d_encoder = 192, image_size=(448,448))\n",
    "print(f'Number of trainable parameters for MLP : {patch_classifier.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Pipeline:\n\tsize mismatch for transformer.pos_embedding: copying a param with shape torch.Size([1, 785, 384]) from checkpoint, the shape in current model is torch.Size([1, 785, 192]).\n\tsize mismatch for transformer.cls_token: copying a param with shape torch.Size([1, 1, 384]) from checkpoint, the shape in current model is torch.Size([1, 1, 192]).\n\tsize mismatch for transformer.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.patch_embedding.proj.weight: copying a param with shape torch.Size([384, 64, 16, 16]) from checkpoint, the shape in current model is torch.Size([192, 64, 16, 16]).\n\tsize mismatch for transformer.patch_embedding.proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.0.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.0.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.1.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.1.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.2.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.2.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.3.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.3.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.4.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.4.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.5.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.5.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for mlp.mlp.0.weight: copying a param with shape torch.Size([512, 384]) from checkpoint, the shape in current model is torch.Size([512, 192]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Pipeline(cnn, vit_tiny, patch_classifier, image_size\u001b[39m=\u001b[39m (\u001b[39m448\u001b[39m,\u001b[39m448\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_weights(\u001b[39m'\u001b[39;49m\u001b[39m../models/best_pipeline.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumber of trainable parameters for Pipeline : \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mcount_parameters()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb Cell 6\u001b[0m in \u001b[0;36mPipeline.load_weights\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_weights\u001b[39m(\u001b[39mself\u001b[39m,path): \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path,map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)), strict\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Pipeline:\n\tsize mismatch for transformer.pos_embedding: copying a param with shape torch.Size([1, 785, 384]) from checkpoint, the shape in current model is torch.Size([1, 785, 192]).\n\tsize mismatch for transformer.cls_token: copying a param with shape torch.Size([1, 1, 384]) from checkpoint, the shape in current model is torch.Size([1, 1, 192]).\n\tsize mismatch for transformer.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.patch_embedding.proj.weight: copying a param with shape torch.Size([384, 64, 16, 16]) from checkpoint, the shape in current model is torch.Size([192, 64, 16, 16]).\n\tsize mismatch for transformer.patch_embedding.proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.0.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.0.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.0.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.1.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.1.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.1.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.2.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.2.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.2.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.3.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.3.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.3.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.4.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.4.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.4.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.4.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for transformer.transformer.layers.5.self_attn.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.linear1.weight: copying a param with shape torch.Size([768, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for transformer.transformer.layers.5.linear2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for transformer.transformer.layers.5.linear2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer.transformer.layers.5.norm2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for mlp.mlp.0.weight: copying a param with shape torch.Size([512, 384]) from checkpoint, the shape in current model is torch.Size([512, 192])."
     ]
    }
   ],
   "source": [
    "model = Pipeline(cnn, vit_tiny, patch_classifier, image_size= (448,448))\n",
    "model.load_weights('../models/best_pipeline.pth')\n",
    "print(f'Number of trainable parameters for Pipeline : {model.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "\n",
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.001)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "test_annotated = os.listdir(annotated_dir_test)\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple test dataset + ground truth dictionary\n",
    "test_annotations = list()\n",
    "for gt_file in test_annotated:\n",
    "    path = os.path.join(annotated_dir_test,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    test_annotations.append(json_gt)\n",
    "    \n",
    "test_annotations = [a for f in test_annotations for a in f]\n",
    "\n",
    "test_dataset = TuSimple(train_annotations = test_annotations, train_img_dir = test_clips_dir, resize_to = (448,448), subset_size = 0.001, test = True, previous= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def results_orig_masked(test, predicted_mask, name):\n",
    "    # convert the predicted binary mask tensor to a numpy array\n",
    "    predicted_mask_n = predicted_mask.squeeze().cpu().detach().numpy()  # remove batch and channel dimensions and convert to numpy array\n",
    "    predicted_mask_n = np.array(Image.fromarray(predicted_mask_n).resize((448, 448)))  # resize to original image size\n",
    "    original_image = test.cpu().detach().numpy()  # convert to numpy array\n",
    "    original_image = np.transpose(original_image, (1, 2, 0))[:, :, ::-1]  # transpose to match channel order and convert from RGB to BGR\n",
    "    mask = np.zeros(original_image.shape[:2])  # create an empty mask of the same size as the original image\n",
    "    mask[predicted_mask_n == 1] = 1  # set the predicted mask to be one in the mask\n",
    "    # Blend the original image and the mask together, where the mask is green and the original image is in the background.\n",
    "    alpha = 0.2\n",
    "    green_mask = np.zeros_like(original_image)\n",
    "    green_mask[:, :, 1] = 255\n",
    "    blended_image = (1 - alpha) * original_image + alpha * green_mask * mask[..., None]\n",
    "\n",
    "    # Convert the blended image back to a PyTorch tensor and return it.\n",
    "    blended_image = np.transpose(blended_image, (2, 0, 1))\n",
    "    blended_image = torch.from_numpy(blended_image).float().to(\"cpu\")\n",
    "    utils.save_image(blended_image, name + '.png', save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for train and validation \n",
    "train_loader = DataLoader(train_set, batch_size= 1,shuffle= True, drop_last= True, num_workers= 4) \n",
    "validation_loader = DataLoader(validation_set,batch_size= 1, shuffle= True, drop_last= True, num_workers= 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores = train(model, train_loader,val_loader= validation_loader , num_epochs= 1, \n",
    "                                                                                        lane_weight = pos_weight, lr = 0.01, SGD_momentum= 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
