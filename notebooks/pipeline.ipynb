{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torch_poly_lr_decay import PolynomialLRDecay\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from vit import ViT\n",
    "from mlp_decoder import DecoderMLP\n",
    "from segnet_backbone import SegNet\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-End pipeline (CNN + ViT + MLP)\n",
    "class Pipeline(nn.Module):\n",
    "    def __init__(self, feat_extractor, vit_variant, mlp_head, image_size = (448,448)):\n",
    "        super().__init__()\n",
    "        self.cnn = feat_extractor\n",
    "        self.transformer = vit_variant\n",
    "        self.mlp = mlp_head\n",
    "        self.image_size = image_size\n",
    "        self.lane_threshold = 0.5\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # CNN branch for feature extraction\n",
    "        x,_ = self.cnn(im)\n",
    "        \n",
    "        # Standardize featmaps to prevent exploding gradients and help the ViT perform better\n",
    "        x = self.standarize_layer(x)\n",
    "        \n",
    "        # Transform standardized feature maps using the ViT\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Perform patch level classification (0 for background/ 1 for lane)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        # Interpolate patch level class annotatations to pixel level and transform to original image size\n",
    "        logits = F.interpolate(x, size=(H, W), mode=\"bilinear\")\n",
    "        \n",
    "        probs = self.activation(logits)\n",
    "\n",
    "        return logits, probs\n",
    "        \n",
    "        \n",
    "    # Standardize feature maps from SegNet layer\n",
    "    def standarize_layer(self,featmaps):\n",
    "        # Compute mean and standard deviation of each channel\n",
    "        mean = torch.mean(featmaps, dim=[0, 2, 3], keepdim=True)\n",
    "        std = torch.std(featmaps, dim=[0, 2, 3], keepdim=True)\n",
    "\n",
    "        # Normalize each channel to have zero mean and unit variance\n",
    "        normal_featmaps = (featmaps - mean) / std\n",
    "        return normal_featmaps\n",
    "        \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Load trained model\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the pipeline with schedule and augmentations\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, weight_decay=0, SGD_momentum = 0.9, lr_scheduler=False, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "\n",
    "    #create seperate parameter groups for the different networks\n",
    "    segnet_params = [p for p in model.cnn.parameters() if p.requires_grad]\n",
    "    mlp_params = [p for p in model.mlp.parameters() if p.requires_grad]\n",
    "    vit_params = [p for p in model.transformer.parameters() if p.requires_grad]\n",
    "\n",
    "    #define the optimizer with different learning rates for the parameter groups\n",
    "    optimizer = optim.SGD([\n",
    "        {'params' : segnet_params, 'lr' : 0.01},\n",
    "        {'params' : mlp_params, 'lr' : 0.01},\n",
    "        {'params' : vit_params, 'lr' : 0.001}\n",
    "    ], momentum=SGD_momentum)\n",
    "    \n",
    "    # Define your learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=4, verbose=True)\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        pass\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    gt_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "  \n",
    "    train_augmentations = transforms.Compose([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                                            transforms.ColorJitter(brightness=0.35, contrast=0.2, saturation=0.4, hue=0.1)])\n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Metrics collection for plotting\n",
    "    train_losses = []\n",
    "    train_f1_scores = []\n",
    "    train_iou_scores = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    val_iou_scores = []\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = gt_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)]\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "\n",
    "            inputs = train_augmentations(inputs).to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "        \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    logits,outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    val_loss = criterion(logits.to(device),targets)\n",
    "                    val_loss += val_loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "        # Collect metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        train_iou_scores.append(train_iou)\n",
    "    \n",
    "        if val_loader:\n",
    "            val_losses.append(val_loss)\n",
    "            val_f1_scores.append(val_f1)\n",
    "            val_iou_scores.append(val_iou)\n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            # scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))\n",
    "            \n",
    "            if val_loader:\n",
    "                print('Val_Loss: {} - Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_loss,val_f1,val_iou))\n",
    "    \n",
    "    if val_loader:\n",
    "        return train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores\n",
    "    else:\n",
    "        return train_losses,train_f1_scores,train_iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 448, 448])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = torch.rand(1,3,448,448)\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for SegNet : 29443587\n",
      "Number of trainable parameters for ViT : 2869440\n",
      "Number of trainable parameters for MLP : 233537\n"
     ]
    }
   ],
   "source": [
    "# Initialize SegNet\n",
    "cnn = SegNet()\n",
    "print(f'Number of trainable parameters for SegNet : {cnn.count_parameters()}')\n",
    "\n",
    "# Initialize ViT Tiny\n",
    "vit_tiny = ViT(image_size=448, patch_size=16, num_classes=1, dim=192, depth=6, heads=3, \n",
    "                      mlp_dim=768, dropout=0.1,load_pre= False)\n",
    "print(f'Number of trainable parameters for ViT : {vit_tiny.count_parameters()}')\n",
    "\n",
    "# Initialize MLP\n",
    "patch_classifier = DecoderMLP(n_classes = 1, d_encoder = 192, image_size=(448,448))\n",
    "print(f'Number of trainable parameters for MLP : {patch_classifier.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for Pipeline : 32546564\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(cnn, vit_tiny, patch_classifier, image_size= (448,448))\n",
    "print(f'Number of trainable parameters for Pipeline : {model.count_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = model(test_img)\n",
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "# annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "# test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# test_annotated = os.listdir(annotated_dir_test)\n",
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple test dataset + ground truth dictionary\n",
    "# test_annotations = list()\n",
    "# for gt_file in test_annotated:\n",
    "#     path = os.path.join(annotated_dir_test,gt_file)\n",
    "#     json_gt = [json.loads(line) for line in open(path)]\n",
    "#     test_annotations.append(json_gt)\n",
    "    \n",
    "# test_annotations = [a for f in test_annotations for a in f]\n",
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.001)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\pipeline.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_set, batch_size\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,shuffle\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(model, train_loader, val_loader \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, lane_weight \u001b[39m=\u001b[39;49m pos_weight)\n",
      "\u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\pipeline.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr, weight_decay, SGD_momentum, lr_scheduler, lane_weight)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m inputs \u001b[39m=\u001b[39m train_augmentations(inputs)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m outputs, eval_out \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mto(device), targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\pipeline.ipynb Cell 11\u001b[0m in \u001b[0;36mPipeline.forward\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m H, W \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# CNN branch for feature extraction\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x,_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(im)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Standardize featmaps to prevent exploding gradients and help the ViT perform better\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/Lane-detection-Master-Thesis/notebooks/pipeline.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstandarize_layer(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\steli\\Lane-detection-Master-Thesis\\notebooks\\../resources\\segnet_backbone.py:260\u001b[0m, in \u001b[0;36mSegNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 260\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mlocal_response_norm(x, size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m    262\u001b[0m     \u001b[39m#ENCODE LAYERS\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[39m#Stage 1\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBNEn11(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConvEn11(x))) \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:2540\u001b[0m, in \u001b[0;36mlocal_response_norm\u001b[1;34m(input, size, alpha, beta, k)\u001b[0m\n\u001b[0;32m   2537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2538\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n\u001b[1;32m-> 2540\u001b[0m div \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mmul(\u001b[39minput\u001b[39;49m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m   2542\u001b[0m     div \u001b[39m=\u001b[39m pad(div, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, (size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size= 1,shuffle= True, drop_last= True, num_workers= 4) \n",
    "train(model, train_loader, val_loader = None, num_epochs=1, lane_weight = pos_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
