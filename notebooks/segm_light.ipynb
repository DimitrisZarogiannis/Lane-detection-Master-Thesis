{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch_poly_lr_decay import PolynomialLRDecay\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from mask_transformer import MaskTransformer\n",
    "from vit import ViT\n",
    "import utils\n",
    "from linear import DecoderLinear\n",
    "from mlp_decoder import DecoderMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.001, val_size= 0.2)\n",
    "\n",
    "# Create train and validation splits / Always use del dataset to free memory after this\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenter pipeline class for training with Dice loss and lightning wrapper\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self,encoder, decoder, image_size = (640,640), output_act = nn.Sigmoid()):\n",
    "        super().__init__()\n",
    "        self.patch_size = encoder.patch_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_size = image_size\n",
    "        self.lane_threshold = 0.5\n",
    "        self.output_act = output_act\n",
    "        \n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # Pass through the pre-trained vit backbone\n",
    "        x = self.encoder(im, return_features=True)\n",
    "        \n",
    "        # Pass through the masks transformer\n",
    "        masks = self.decoder(x)\n",
    "        \n",
    "\n",
    "        # Interpolate patch level class annotatations to pixel level and transform to original image size\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        \n",
    "        # Training time\n",
    "        if self.training:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(masks)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            predictions = predictions.float().requires_grad_()\n",
    "            return predictions\n",
    "        # Evaluation time\n",
    "        else:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(masks)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            return predictions\n",
    "        \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Load trained model\n",
    "    def load_segmenter(self):\n",
    "        self.load_state_dict(torch.load(\"path/to/save/model.pth\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningSegmenter(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, loss_fn, lr):\n",
    "        super().__init__()\n",
    "        self.model = Segmenter(encoder, decoder)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "        self.f1 = F1Score(task=\"binary\")\n",
    "        self.iou_score = JaccardIndex(task= 'binary')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        f1_train = self.f1(y_hat,y)\n",
    "        iou_train = self.iou_score(y_hat,y)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('f1_train', f1_train)\n",
    "        self.log('iou_train', iou_train)\n",
    "        return {'loss': loss, 'f1_train': f1_train, 'iou_train': iou_train}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        f1_val = self.f1(y_hat,y)\n",
    "        iou_val = self.iou_score(y_hat,y)\n",
    "        self.log('f1_val', f1_val)\n",
    "        self.log('iou_val', iou_val)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_f1 = torch.stack([x['f1_train'] for x in outputs]).mean()\n",
    "        avg_iou = torch.stack([x['iou_train'] for x in outputs]).mean()\n",
    "        self.log('avg_train_loss', avg_loss, prog_bar=True)\n",
    "        self.log('avg_f1_train', avg_f1, prog_bar=True)\n",
    "        self.log('avg_iou_train', avg_iou, prog_bar=True)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=2, drop_last=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=2,drop_last=True)\n",
    "\n",
    "encoder = ViT(image_size=640, patch_size=16, num_classes=1, dim=768, depth=12, heads=12, \n",
    "            mlp_dim=3072, dropout=0.1, load_pre=True, pre_trained_path='../pre-trained/jx_vit_base_p16_224-80ecf9dd.pth')\n",
    "encoder.freeze_all_but_some([])\n",
    "decoder = DecoderLinear(n_classes=1, d_encoder=768)\n",
    "\n",
    "model = LightningSegmenter(encoder, decoder, loss_fn=utils.dice_loss, lr=0.001)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=4, gpus=None)# set gpus to None if you're not using a GPU\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
