{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch_poly_lr_decay import PolynomialLRDecay\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from mask_transformer import MaskTransformer\n",
    "from vit import ViT\n",
    "import utils\n",
    "from linear import DecoderLinear\n",
    "from mlp_decoder import DecoderMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "    \n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.001, val_size= 0.2)\n",
    "\n",
    "# Create train and validation splits / Always use del dataset to free memory after this\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.plot_img_gt(train_set[0][0],train_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1899, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)\n",
    "print(pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the transformer pipeline with schedule and SGD optimizer\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, momentum=0.9, weight_decay=0, lr_scheduler=True, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        scheduler = PolynomialLRDecay(optimizer, max_decay_steps=100, end_learning_rate=0.0001, power=0.9)\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "                   \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "            \n",
    "        if val_loader:\n",
    "            for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                model.eval()\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                val_iou += iou_score(outputs.to(device), targets)\n",
    "                val_f1 += f1_score(outputs.to(device),targets)\n",
    "        \n",
    "            val_iou /= len(val_loader)\n",
    "            val_f1 /= len(val_loader)\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "        \n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenter pipeline class (ViT + Masks transformer end-to-end)\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self,encoder, decoder, image_size = (640,640), output_act = nn.Sigmoid()):\n",
    "        super().__init__()\n",
    "        self.patch_size = encoder.patch_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_size = image_size\n",
    "        self.lane_threshold = 0.5\n",
    "        self.output_act = output_act\n",
    "        \n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # Pass through the pre-trained vit backbone\n",
    "        x = self.encoder(im, return_features=True)\n",
    "        \n",
    "        # Pass through the masks transformer\n",
    "        masks = self.decoder(x)\n",
    "        \n",
    "\n",
    "        # Interpolate patch level class annotatations to pixel level and transform to original image size\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        \n",
    "        # Training time\n",
    "        if self.training:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(masks)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            return masks, predictions\n",
    "        # Evaluation time\n",
    "        else:\n",
    "            act = self.output_act\n",
    "            class_prob_masks = act(masks)\n",
    "            predictions = torch.where(class_prob_masks > self.lane_threshold, torch.ones_like(class_prob_masks), torch.zeros_like(class_prob_masks))\n",
    "            return predictions\n",
    "        \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Load trained model\n",
    "    def load_segmenter(self):\n",
    "        self.load_state_dict(torch.load(\"path/to/save/model.pth\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created ViT with pre-trained weights...!\n",
      "Number of trainable parameters : 236289\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=2, shuffle= True, drop_last= True) \n",
    "validation_loader = DataLoader(validation_set,batch_size=2, shuffle= True, drop_last= True) \n",
    "encoder = ViT(image_size=640, patch_size=16, num_classes=1, dim=768, depth=12, heads=12, \n",
    "            mlp_dim=3072, dropout=0.1,load_pre= True, pre_trained_path= '../pre-trained/jx_vit_base_p16_224-80ecf9dd.pth')\n",
    "encoder.freeze_all_but_some([])\n",
    "# decoder = MaskTransformer(n_classes= 1)\n",
    "decoder = DecoderMLP(n_classes= 1, d_encoder= 768)\n",
    "model = Segmenter(encoder, decoder)\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# train(model, train_loader,val_loader= validation_loader,num_epochs= 15, lane_weight = pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236289"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count pipeline trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.pos_embedding', 'encoder.norm.weight', 'encoder.norm.bias', 'encoder.patch_embedding.proj.weight', 'encoder.patch_embedding.proj.bias', 'encoder.transformer.layers.0.self_attn.in_proj_weight', 'encoder.transformer.layers.0.self_attn.in_proj_bias', 'encoder.transformer.layers.0.self_attn.out_proj.weight', 'encoder.transformer.layers.0.self_attn.out_proj.bias', 'encoder.transformer.layers.0.linear1.weight', 'encoder.transformer.layers.0.linear1.bias', 'encoder.transformer.layers.0.linear2.weight', 'encoder.transformer.layers.0.linear2.bias', 'encoder.transformer.layers.0.norm1.weight', 'encoder.transformer.layers.0.norm1.bias', 'encoder.transformer.layers.0.norm2.weight', 'encoder.transformer.layers.0.norm2.bias', 'encoder.transformer.layers.1.self_attn.in_proj_weight', 'encoder.transformer.layers.1.self_attn.in_proj_bias', 'encoder.transformer.layers.1.self_attn.out_proj.weight', 'encoder.transformer.layers.1.self_attn.out_proj.bias', 'encoder.transformer.layers.1.linear1.weight', 'encoder.transformer.layers.1.linear1.bias', 'encoder.transformer.layers.1.linear2.weight', 'encoder.transformer.layers.1.linear2.bias', 'encoder.transformer.layers.1.norm1.weight', 'encoder.transformer.layers.1.norm1.bias', 'encoder.transformer.layers.1.norm2.weight', 'encoder.transformer.layers.1.norm2.bias', 'encoder.transformer.layers.2.self_attn.in_proj_weight', 'encoder.transformer.layers.2.self_attn.in_proj_bias', 'encoder.transformer.layers.2.self_attn.out_proj.weight', 'encoder.transformer.layers.2.self_attn.out_proj.bias', 'encoder.transformer.layers.2.linear1.weight', 'encoder.transformer.layers.2.linear1.bias', 'encoder.transformer.layers.2.linear2.weight', 'encoder.transformer.layers.2.linear2.bias', 'encoder.transformer.layers.2.norm1.weight', 'encoder.transformer.layers.2.norm1.bias', 'encoder.transformer.layers.2.norm2.weight', 'encoder.transformer.layers.2.norm2.bias', 'encoder.transformer.layers.3.self_attn.in_proj_weight', 'encoder.transformer.layers.3.self_attn.in_proj_bias', 'encoder.transformer.layers.3.self_attn.out_proj.weight', 'encoder.transformer.layers.3.self_attn.out_proj.bias', 'encoder.transformer.layers.3.linear1.weight', 'encoder.transformer.layers.3.linear1.bias', 'encoder.transformer.layers.3.linear2.weight', 'encoder.transformer.layers.3.linear2.bias', 'encoder.transformer.layers.3.norm1.weight', 'encoder.transformer.layers.3.norm1.bias', 'encoder.transformer.layers.3.norm2.weight', 'encoder.transformer.layers.3.norm2.bias', 'encoder.transformer.layers.4.self_attn.in_proj_weight', 'encoder.transformer.layers.4.self_attn.in_proj_bias', 'encoder.transformer.layers.4.self_attn.out_proj.weight', 'encoder.transformer.layers.4.self_attn.out_proj.bias', 'encoder.transformer.layers.4.linear1.weight', 'encoder.transformer.layers.4.linear1.bias', 'encoder.transformer.layers.4.linear2.weight', 'encoder.transformer.layers.4.linear2.bias', 'encoder.transformer.layers.4.norm1.weight', 'encoder.transformer.layers.4.norm1.bias', 'encoder.transformer.layers.4.norm2.weight', 'encoder.transformer.layers.4.norm2.bias', 'encoder.transformer.layers.5.self_attn.in_proj_weight', 'encoder.transformer.layers.5.self_attn.in_proj_bias', 'encoder.transformer.layers.5.self_attn.out_proj.weight', 'encoder.transformer.layers.5.self_attn.out_proj.bias', 'encoder.transformer.layers.5.linear1.weight', 'encoder.transformer.layers.5.linear1.bias', 'encoder.transformer.layers.5.linear2.weight', 'encoder.transformer.layers.5.linear2.bias', 'encoder.transformer.layers.5.norm1.weight', 'encoder.transformer.layers.5.norm1.bias', 'encoder.transformer.layers.5.norm2.weight', 'encoder.transformer.layers.5.norm2.bias', 'encoder.transformer.layers.6.self_attn.in_proj_weight', 'encoder.transformer.layers.6.self_attn.in_proj_bias', 'encoder.transformer.layers.6.self_attn.out_proj.weight', 'encoder.transformer.layers.6.self_attn.out_proj.bias', 'encoder.transformer.layers.6.linear1.weight', 'encoder.transformer.layers.6.linear1.bias', 'encoder.transformer.layers.6.linear2.weight', 'encoder.transformer.layers.6.linear2.bias', 'encoder.transformer.layers.6.norm1.weight', 'encoder.transformer.layers.6.norm1.bias', 'encoder.transformer.layers.6.norm2.weight', 'encoder.transformer.layers.6.norm2.bias', 'encoder.transformer.layers.7.self_attn.in_proj_weight', 'encoder.transformer.layers.7.self_attn.in_proj_bias', 'encoder.transformer.layers.7.self_attn.out_proj.weight', 'encoder.transformer.layers.7.self_attn.out_proj.bias', 'encoder.transformer.layers.7.linear1.weight', 'encoder.transformer.layers.7.linear1.bias', 'encoder.transformer.layers.7.linear2.weight', 'encoder.transformer.layers.7.linear2.bias', 'encoder.transformer.layers.7.norm1.weight', 'encoder.transformer.layers.7.norm1.bias', 'encoder.transformer.layers.7.norm2.weight', 'encoder.transformer.layers.7.norm2.bias', 'encoder.transformer.layers.8.self_attn.in_proj_weight', 'encoder.transformer.layers.8.self_attn.in_proj_bias', 'encoder.transformer.layers.8.self_attn.out_proj.weight', 'encoder.transformer.layers.8.self_attn.out_proj.bias', 'encoder.transformer.layers.8.linear1.weight', 'encoder.transformer.layers.8.linear1.bias', 'encoder.transformer.layers.8.linear2.weight', 'encoder.transformer.layers.8.linear2.bias', 'encoder.transformer.layers.8.norm1.weight', 'encoder.transformer.layers.8.norm1.bias', 'encoder.transformer.layers.8.norm2.weight', 'encoder.transformer.layers.8.norm2.bias', 'encoder.transformer.layers.9.self_attn.in_proj_weight', 'encoder.transformer.layers.9.self_attn.in_proj_bias', 'encoder.transformer.layers.9.self_attn.out_proj.weight', 'encoder.transformer.layers.9.self_attn.out_proj.bias', 'encoder.transformer.layers.9.linear1.weight', 'encoder.transformer.layers.9.linear1.bias', 'encoder.transformer.layers.9.linear2.weight', 'encoder.transformer.layers.9.linear2.bias', 'encoder.transformer.layers.9.norm1.weight', 'encoder.transformer.layers.9.norm1.bias', 'encoder.transformer.layers.9.norm2.weight', 'encoder.transformer.layers.9.norm2.bias', 'encoder.transformer.layers.10.self_attn.in_proj_weight', 'encoder.transformer.layers.10.self_attn.in_proj_bias', 'encoder.transformer.layers.10.self_attn.out_proj.weight', 'encoder.transformer.layers.10.self_attn.out_proj.bias', 'encoder.transformer.layers.10.linear1.weight', 'encoder.transformer.layers.10.linear1.bias', 'encoder.transformer.layers.10.linear2.weight', 'encoder.transformer.layers.10.linear2.bias', 'encoder.transformer.layers.10.norm1.weight', 'encoder.transformer.layers.10.norm1.bias', 'encoder.transformer.layers.10.norm2.weight', 'encoder.transformer.layers.10.norm2.bias', 'encoder.transformer.layers.11.self_attn.in_proj_weight', 'encoder.transformer.layers.11.self_attn.in_proj_bias', 'encoder.transformer.layers.11.self_attn.out_proj.weight', 'encoder.transformer.layers.11.self_attn.out_proj.bias', 'encoder.transformer.layers.11.linear1.weight', 'encoder.transformer.layers.11.linear1.bias', 'encoder.transformer.layers.11.linear2.weight', 'encoder.transformer.layers.11.linear2.bias', 'encoder.transformer.layers.11.norm1.weight', 'encoder.transformer.layers.11.norm1.bias', 'encoder.transformer.layers.11.norm2.weight', 'encoder.transformer.layers.11.norm2.bias', 'decoder.head.weight', 'decoder.head.bias'])\n"
     ]
    }
   ],
   "source": [
    "x =torch.load('../models/segmenter.pth',map_location=torch.device('cpu'))\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1600, 2])\n",
      "tensor([0.3014, 0.3048], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with with spatial dimensions of [2] and output size of (640, 640). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, train_loader,val_loader\u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,num_epochs\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, lane_weight \u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0.0005\u001b[39;49m,\u001b[39m1.\u001b[39;49m]), lr_scheduler\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,lr\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, momentum, weight_decay, lr_scheduler, lane_weight)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m outputs, eval_out \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mto(device), targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb Cell 9\u001b[0m in \u001b[0;36mSegmenter.forward\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(masks[\u001b[39m1\u001b[39m][\u001b[39m2\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Interpolate patch level class annotatations to pixel level and transform to original image size\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m masks \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49minterpolate(masks, size\u001b[39m=\u001b[39;49m(H, W), mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(masks\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segmenter.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Training time\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/functional.py:3854\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3852\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   3853\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m dim:\n\u001b[0;32m-> 3854\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3855\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3856\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput with with spatial dimensions of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:])\u001b[39m}\u001b[39;00m\u001b[39m and output size of \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3857\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3858\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3859\u001b[0m \n\u001b[1;32m   3860\u001b[0m         )\n\u001b[1;32m   3861\u001b[0m     output_size \u001b[39m=\u001b[39m size\n\u001b[1;32m   3862\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with with spatial dimensions of [2] and output size of (640, 640). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "train(model, train_loader,val_loader= None,num_epochs= 1, lane_weight = torch.tensor([0.0005,1.]), lr_scheduler= False,lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on a single sample (for now)\n",
    "model.eval()\n",
    "img_tens, gt = train_set[1]\n",
    "img_tens = img_tens.unsqueeze(0)\n",
    "test = model(img_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 640])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.squeeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mask = utils.toImagearr(test)\n",
    "base_img = utils.toImagearr(img_tens.squeeze(0))\n",
    "utils.disp_img(image = base_img, name = 'Original Image')\n",
    "utils.disp_img(image = predicted_mask, name = 'Predicted Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot image and prediction simultaneously\n",
    "utils.plot_img_pred(img_tens.squeeze(0),test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
