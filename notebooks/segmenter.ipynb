{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from mask_transformer import MaskTransformer\n",
    "from vit import ViT\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "    \n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.01,val_size= 0.1)\n",
    "\n",
    "# Create train and validation splits / Always use del dataset to free memory after this\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenter pipeline class (ViT + Masks transformer end-to-end)\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self,encoder, mask_trans, image_size = (640,640)):\n",
    "        super().__init__()\n",
    "        self.patch_size = encoder.patch_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = mask_trans\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    # Forward pass of the pipeline\n",
    "    def forward(self, im):\n",
    "        H, W = self.image_size\n",
    "        \n",
    "        # Pass through the pre-trained vit backbone\n",
    "        x = self.encoder(im, return_features=True)\n",
    "        \n",
    "        # Pass through the masks transformer\n",
    "        masks = self.decoder(x)\n",
    "\n",
    "        # Interpolate patch level class annotatations to pixel level and transform to original image size\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        predicted_masks = torch.argmax(masks, dim=1).float()\n",
    "        \n",
    "        # expand the output mask tensor along the channel dimension to match the ground truth tensor (maybe needs removal)\n",
    "        predicted_masks = predicted_masks.unsqueeze(0).expand(3, -1, -1, -1).transpose(0, 1).squeeze(0)\n",
    "\n",
    "        return predicted_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created ViT with pre-trained weights...!\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=2, shuffle= True)\n",
    "\n",
    "encoder = ViT(image_size=640, patch_size=16, num_classes=2, dim=768, depth=12, heads=12, \n",
    "            mlp_dim=3072, dropout=0.1,load_pre= True, pre_trained_path= '../pre-trained/jx_vit_base_p16_224-80ecf9dd.pth')\n",
    "decoder = MaskTransformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmenter(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.9964\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the Segmenter Transformer model and move it to the device\n",
    "model = Segmenter(encoder, decoder).to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop through the training dataset and perform the training\n",
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move the input images and the target masks to the same device as the model\n",
    "        inputs, targets = inputs.to(device), targets[\"gt_tensor\"].to(device)\n",
    "        \n",
    "        # Zero the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass the input images through the model to get the predictions\n",
    "        outputs = model(inputs)\n",
    "        outputs.requires_grad = True\n",
    "        \n",
    "        # Compute the loss between the predictions and the target masks\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward propagate the loss through the model to compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    #Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{10} - Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test = model(train_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mask = utils.toImagearr(test)\n",
    "base_img = utils.toImagearr(train_set[0][0])\n",
    "utils.disp_img(image = base_img, name = 'Original Image')\n",
    "utils.disp_img(image = predicted_mask, name = 'Predicted Mask')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
