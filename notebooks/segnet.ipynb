{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "import json\n",
    "from torchmetrics import F1Score,JaccardIndex, Accuracy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from torchmetrics.classification import BinaryStatScores\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "import utils\n",
    "from vit import ViT\n",
    "from mlp_decoder import DecoderMLP\n",
    "\n",
    "\n",
    "\n",
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "\n",
    "test_annotated = os.listdir(annotated_dir_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple test dataset + ground truth dictionary\n",
    "# test_annotations = list()\n",
    "# for gt_file in test_annotated:\n",
    "#     path = os.path.join(annotated_dir_test,gt_file)\n",
    "#     json_gt = [json.loads(line) for line in open(path)]\n",
    "#     test_annotations.append(json_gt)\n",
    "    \n",
    "# test_annotations = [a for f in test_annotations for a in f]\n",
    "\n",
    "# test_dataset = TuSimple(train_annotations = test_annotations, train_img_dir = test_clips_dir, resize_to = (448,448), subset_size = 0.0005, test = True, previous= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from : https://github.com/vinceecws/SegNet_PyTorch/blob/master/Pavements/SegNet.py\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chn=3, out_chn=1, BN_momentum=0.5):\n",
    "        super(SegNet, self).__init__()\n",
    "        \n",
    "        #SegNet Architecture\n",
    "        #Takes input of size in_chn = 3 (RGB images have 3 channels)\n",
    "        #Outputs size label_chn (N # of classes)\n",
    "\n",
    "        self.lane_threshold = 0.5\n",
    "        self.in_chn = in_chn\n",
    "        self.out_chn = out_chn\n",
    "\n",
    "        self.MaxEn = nn.MaxPool2d(2, stride=2, return_indices=True) \n",
    "\n",
    "        self.ConvEn11 = nn.Conv2d(self.in_chn, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn11 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvEn12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn21 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvEn22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn31 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn41 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "\n",
    "        #DECODING consists of 5 stages\n",
    "        #Each stage corresponds to their respective counterparts in ENCODING\n",
    "\n",
    "        #General Max Pool 2D/Upsampling for DECODING layers\n",
    "        self.MaxDe = nn.MaxUnpool2d(2, stride=2) \n",
    "\n",
    "        self.ConvDe53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe41 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe41 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe31 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvDe21 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe21 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvDe11 = nn.Conv2d(64, self.out_chn, kernel_size=3, padding=1)\n",
    "        self.BNDe11 = nn.BatchNorm2d(self.out_chn, momentum=BN_momentum)\n",
    "        \n",
    "\n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.local_response_norm(x, size=3)\n",
    "\n",
    "        #ENCODE LAYERS\n",
    "        #Stage 1\n",
    "        x = F.relu(self.BNEn11(self.ConvEn11(x))) \n",
    "        x = F.relu(self.BNEn12(self.ConvEn12(x)))\n",
    "         \n",
    "        # edges1 = self.canny_edge_detection(x)\n",
    "        \n",
    "        # x = torch.add(x, edges1)\n",
    "        \n",
    "        \n",
    "        x, ind1 = self.MaxEn(x)\n",
    "        size1 = x.size()\n",
    "\n",
    "\n",
    "        #Stage 2\n",
    "        x = F.relu(self.BNEn21(self.ConvEn21(x))) \n",
    "        x = F.relu(self.BNEn22(self.ConvEn22(x))) \n",
    "        \n",
    "        # edges2 = self.canny_edge_detection(x)\n",
    "        # x = torch.add(x, edges2)\n",
    "        \n",
    "        x, ind2 = self.MaxEn(x)\n",
    "        size2 = x.size()\n",
    "\n",
    "        #Stage 3\n",
    "        x = F.relu(self.BNEn31(self.ConvEn31(x))) \n",
    "        x = F.relu(self.BNEn32(self.ConvEn32(x))) \n",
    "        x = F.relu(self.BNEn33(self.ConvEn33(x)))\n",
    "        \n",
    "        # edges3 = self.canny_edge_detection(x)\n",
    "        # x = torch.add(x, edges3)\n",
    "        \n",
    "        x, ind3 = self.MaxEn(x)\n",
    "        size3 = x.size()\n",
    "        \n",
    "        #Stage 4\n",
    "        x = F.relu(self.BNEn41(self.ConvEn41(x))) \n",
    "        x = F.relu(self.BNEn42(self.ConvEn42(x))) \n",
    "        x = F.relu(self.BNEn43(self.ConvEn43(x)))   \n",
    "        \n",
    "        # edges4 = self.canny_edge_detection(x)\n",
    "        # x = torch.add(x, edges4)\n",
    "         \n",
    "        x, ind4 = self.MaxEn(x)\n",
    "        size4 = x.size()\n",
    "\n",
    "        #Stage 5\n",
    "        x = F.relu(self.BNEn51(self.ConvEn51(x))) \n",
    "        x = F.relu(self.BNEn52(self.ConvEn52(x))) \n",
    "        x = F.relu(self.BNEn53(self.ConvEn53(x)))\n",
    "        \n",
    "        # edges5 = self.canny_edge_detection(x)\n",
    "        # x = torch.add(x, edges5)\n",
    "            \n",
    "        x, ind5 = self.MaxEn(x)\n",
    "        size5 = x.size()\n",
    "\n",
    "        #DECODE LAYERS\n",
    "        #Stage 5\n",
    "        x = self.MaxDe(x, ind5, output_size=size4)\n",
    "        x = F.relu(self.BNDe53(self.ConvDe53(x)))\n",
    "        x = F.relu(self.BNDe52(self.ConvDe52(x)))\n",
    "        x = F.relu(self.BNDe51(self.ConvDe51(x)))\n",
    "\n",
    "        #Stage 4\n",
    "        x = self.MaxDe(x, ind4, output_size=size3)\n",
    "        x = F.relu(self.BNDe43(self.ConvDe43(x)))\n",
    "        x = F.relu(self.BNDe42(self.ConvDe42(x)))\n",
    "        x = F.relu(self.BNDe41(self.ConvDe41(x)))\n",
    "\n",
    "        #Stage 3\n",
    "        x = self.MaxDe(x, ind3, output_size=size2)\n",
    "        x = F.relu(self.BNDe33(self.ConvDe33(x)))\n",
    "        x = F.relu(self.BNDe32(self.ConvDe32(x)))\n",
    "        x = F.relu(self.BNDe31(self.ConvDe31(x)))\n",
    "\n",
    "        #Stage 2\n",
    "        x = self.MaxDe(x, ind2, output_size=size1)\n",
    "        x = F.relu(self.BNDe22(self.ConvDe22(x)))\n",
    "        x = F.relu(self.BNDe21(self.ConvDe21(x)))\n",
    "\n",
    "        #Stage 1\n",
    "        x = self.MaxDe(x, ind1)\n",
    "        x = F.relu(self.BNDe12(self.ConvDe12(x)))\n",
    "        x = self.ConvDe11(x)\n",
    "        \n",
    "        probs = F.sigmoid(x)\n",
    "        \n",
    "        return x,probs\n",
    "\n",
    "    # Make a single prediction \n",
    "    def predict(self,x):\n",
    "        self.eval()\n",
    "        cnn_features,probs = self.forward(x)\n",
    "        prediction = torch.where(probs > self.lane_threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
    "        return prediction,cnn_features\n",
    "    \n",
    "    def canny_edge_detection(self,x, low_threshold=50, high_threshold=150):\n",
    "        feature_dims = x.shape[1]\n",
    "        # Convert tensor to numpy array and transpose\n",
    "        x = x.detach().cpu().permute(0,2,3,1).numpy()\n",
    "    \n",
    "        # Calculate mean of channels\n",
    "        gray = np.mean(x, axis=3)\n",
    "\n",
    "        # Reshape to (batch_size, height, width)\n",
    "        gray = gray.squeeze().astype(np.uint8)\n",
    "        \n",
    "        # Apply Canny edge detection\n",
    "        edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "        # Convert back to tensor\n",
    "        edges = torch.Tensor(edges).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Expand edge_feature_map to have the same number of channels as conv_feature_map\n",
    "        edges = edges.repeat(1, feature_dims, 1, 1)\n",
    "    \n",
    "        return edges\n",
    "\n",
    "    # Predict with temporal post-processing\n",
    "    def predict_temporal (self,x):\n",
    "        self.eval()\n",
    "        self.lane_threshold = 0.5\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        with torch.no_grad():\n",
    "            for clip in x:\n",
    "                base_frame = clip.to(device)\n",
    "                # Predict mask probs for base frame\n",
    "                _,base_mask_prob = self.forward(base_frame.unsqueeze(0))\n",
    "            \n",
    "                previous_masks = []\n",
    "            \n",
    "                # Predict masks probs for previous frames\n",
    "                for i in range(1,len(x)):\n",
    "                    prev_frame = x[i].to(device)\n",
    "                    _,prev_mask = self.forward(prev_frame.unsqueeze(0))\n",
    "                    previous_masks.append(prev_mask)\n",
    "                \n",
    "                # Define the decay factor for the previous frames weights\n",
    "                decay_factor = 0.8\n",
    "\n",
    "                # Calculate the weights for each previous frame\n",
    "                weights = [decay_factor**i for i in range(len(clip[0])- 1)]\n",
    "            \n",
    "                # Normalize the weights so that they sum to 1\n",
    "                weights /= np.sum(weights)\n",
    "\n",
    "                # Loop over the previous frames and update the class probabilities for the target frame\n",
    "                for i, prev_mask in enumerate(previous_masks):\n",
    "                    weight = weights[i]\n",
    "                    update_mask = (base_mask_prob < 0.85)\n",
    "                    base_mask_prob[update_mask] = (1 - weight) * base_mask_prob[update_mask] + weight * prev_mask[update_mask]\n",
    "                # # Loop over the previous frames and update the class probabilities for the target frame\n",
    "                # for i, prev_mask in enumerate(previous_masks):\n",
    "                #     weight = weights[i]\n",
    "                #     base_mask_prob[:, 0, :, :] = (1 - weight) * base_mask_prob[:, 0, :, :] + weight * prev_mask[:, 0, :, :]\n",
    "                \n",
    "            prediction = torch.where(base_mask_prob > self.lane_threshold, torch.ones_like(base_mask_prob), torch.zeros_like(base_mask_prob))\n",
    "            \n",
    "        return prediction\n",
    "            \n",
    "    # Load trained model weights\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n",
    "        print('Loaded state dict succesfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.1)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set function (with temporal post-processing)\n",
    "def evaluate(model, test_set):\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    stat_scores = BinaryStatScores().to(device)\n",
    "    accuracy = Accuracy(task=\"binary\").to(device)\n",
    "        \n",
    "    test_f1 = 0\n",
    "    test_iou = 0\n",
    "    test_accuracy = 0\n",
    "    fps_temp = 0\n",
    "    \n",
    "    \n",
    "    no_temp_f1 = 0\n",
    "    no_temp_iou = 0\n",
    "    no_temp_accuracy = 0\n",
    "    fps_no = 0\n",
    "    \n",
    "    all_stats_no = []\n",
    "    all_stats_temp = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for clip in test_set:\n",
    "            gt = clip[1].to(device)\n",
    "            base_frame = clip[0][0].to(device)\n",
    "            \n",
    "            start_time1 = time.time()\n",
    "            \n",
    "            start_time2 = time.time()\n",
    "            \n",
    "            # Predict mask probs for base frame\n",
    "            _,base_mask_prob = model.forward(base_frame.unsqueeze(0))\n",
    "            \n",
    "            end_time1 = time.time()\n",
    "            \n",
    "            # Get F1 scores,IoU and Accuracy before temporal post process\n",
    "            no_temp_f1 += f1_score(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            no_temp_iou += iou_score(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            no_temp_accuracy += accuracy(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            \n",
    "            # Measure FPS without temporal post process\n",
    "            processing_time = end_time1 - start_time1\n",
    "            fps_no += (1 / processing_time)\n",
    "            \n",
    "            # Get stats to calculate FPR/FNR\n",
    "            all_stats_no.append(stat_scores(base_mask_prob.to(device), gt.unsqueeze(0)))\n",
    "            \n",
    "            previous_masks = []\n",
    "            \n",
    "            # Predict masks probs for previous frames\n",
    "            for i in range(1,len(clip[0])):\n",
    "                prev_frame = clip[0][i].to(device)\n",
    "                _,prev_mask = model.forward(prev_frame.unsqueeze(0))\n",
    "                previous_masks.append(prev_mask)\n",
    "            \n",
    "            # Define the decay factor for the previous frames weights\n",
    "            decay_factor = 0.8\n",
    "\n",
    "            # Calculate the weights for each previous frame\n",
    "            weights = [decay_factor**i for i in range(len(clip[0])- 1)]\n",
    "            \n",
    "            # Normalize the weights so that they sum to 1\n",
    "            weights /= np.sum(weights)\n",
    "            \n",
    "            \n",
    "            # Loop over the previous frames and update the class probabilities for the target frame\n",
    "            for i, prev_mask in enumerate(previous_masks):\n",
    "                weight = weights[i]\n",
    "                base_mask_prob[:, 0, :, :] = (1 - weight) * base_mask_prob[:, 0, :, :] + weight * prev_mask[:, 0, :, :]\n",
    "                \n",
    "            end_time2 = time.time()\n",
    "            \n",
    "            # Measure FPS with temporal post process\n",
    "            processing_time = end_time2 - start_time2\n",
    "            fps_temp += (1 / processing_time)\n",
    "            \n",
    "            # Compare new mask with temporal post process with gt and get evaluation scores\n",
    "            test_f1 += f1_score(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            test_iou += iou_score (base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            test_accuracy += accuracy(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            \n",
    "            # Get stats for FNR/FPR with temporal\n",
    "            all_stats_temp.append(stat_scores(base_mask_prob.to(device), gt.unsqueeze(0)))\n",
    "            \n",
    "        # Get FPR and FNR for test set (with and without temporal)\n",
    "        \n",
    "        fp1_sum = torch.stack([s[1] for s in all_stats_no]).sum()\n",
    "        fn1_sum = torch.stack([s[3] for s in all_stats_no]).sum()\n",
    "        tn1_sum = torch.stack([s[2] for s in all_stats_no]).sum()\n",
    "        tp1_sum = torch.stack([s[0] for s in all_stats_no]).sum()\n",
    "        \n",
    "        # Average fpr without temporal\n",
    "        fpr_no = fp1_sum / (tn1_sum + fp1_sum)\n",
    "        fnr_no = fn1_sum / (tp1_sum + fn1_sum)\n",
    "        \n",
    "        fp2_sum = torch.stack([s[1] for s in all_stats_temp]).sum()\n",
    "        fn2_sum = torch.stack([s[3] for s in all_stats_temp]).sum()\n",
    "        tn2_sum = torch.stack([s[2] for s in all_stats_temp]).sum()\n",
    "        tp2_sum = torch.stack([s[0] for s in all_stats_temp]).sum()\n",
    "        \n",
    "        # Average fpr and fnr with temporal\n",
    "        fpr_temp = fp2_sum / (tn2_sum + fp2_sum)\n",
    "        fnr_temp = fn2_sum / (tp2_sum + fn2_sum)\n",
    "        \n",
    "        # Calculate test set average metrics\n",
    "        test_f1 /= len(test_set)\n",
    "        test_iou /= len(test_set)\n",
    "        no_temp_f1 /= len(test_set)\n",
    "        no_temp_iou /= len(test_set)\n",
    "        no_temp_accuracy /= len(test_set)\n",
    "        test_accuracy /= len(test_set)\n",
    "        fps_temp /= len(test_set)\n",
    "        fps_no /= len(test_set)\n",
    "        \n",
    "        # Create lists of metrics with and without temporal post process\n",
    "        \n",
    "        without = [round(fpr_no.item(),4), round(fnr_no.item(),4), round(no_temp_f1.item(),3), round(no_temp_iou.item(),3), round(no_temp_accuracy.item(),3) * 100, round(float(fps_no),3)]\n",
    "        \n",
    "        temporal = [round(fpr_temp.item(),4), round(fnr_temp.item(),4), round(test_f1.item(),3), round(test_iou.item(),3),round(test_accuracy.item(),3) * 100 ,round(float(fps_temp),3)]\n",
    "        \n",
    "        return without,temporal\n",
    "            \n",
    "\n",
    "# Plot metrics function \n",
    "def plot_metrics(train_losses, val_losses, train_f1, val_f1, train_iou, val_iou):\n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation')\n",
    "    plt.xlabel('Epochs (bins of 5)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(0, len(train_losses) + 1, 5))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation F1 scores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_f1) + 1), train_f1, label='Train')\n",
    "    plt.plot(range(1, len(val_f1) + 1), val_f1, label='Validation')\n",
    "    plt.xlabel('Epochs (bins of 5)')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(range(0, len(train_f1) + 1, 5))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation IoU scores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_iou) + 1), train_iou, label='Train')\n",
    "    plt.plot(range(1, len(val_iou) + 1), val_iou, label='Validation')\n",
    "    plt.xlabel('Epochs (bins of 5)')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.xticks(range(0, len(train_iou) + 1, 5))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Custom training function for the pipeline with schedule and augmentations\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, weight_decay=0, SGD_momentum = 0.9, lr_scheduler=False, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=SGD_momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        pass\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    gt_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "  \n",
    "    train_augmentations = transforms.Compose([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                                            transforms.ColorJitter(brightness=0.35, contrast=0.2, saturation=0.4, hue=0.1)])\n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Metrics collection for plotting\n",
    "    train_losses = []\n",
    "    train_f1_scores = []\n",
    "    train_iou_scores = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    val_iou_scores = []\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = gt_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)]\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "\n",
    "            inputs = train_augmentations(inputs).to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "        \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    logits,outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    val_loss = criterion(logits.to(device),targets)\n",
    "                    val_loss += val_loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Collect metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        train_iou_scores.append(train_iou)\n",
    "    \n",
    "        if val_loader:\n",
    "            val_losses.append(val_loss)\n",
    "            val_f1_scores.append(val_f1)\n",
    "            val_iou_scores.append(val_iou)\n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            # scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))\n",
    "            \n",
    "            if val_loader:\n",
    "                print('Val_Loss: {} - Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_loss,val_f1,val_iou))\n",
    "    \n",
    "    if val_loader:\n",
    "        return train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores\n",
    "    else:\n",
    "        return train_losses,train_f1_scores,train_iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 29443587\n",
      "Loaded state dict succesfully!\n"
     ]
    }
   ],
   "source": [
    "model = SegNet()\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "\n",
    "# Create dataloaders and train the model\n",
    "# train_loader = DataLoader(train_set, batch_size= 2,shuffle= True, drop_last= True, num_workers= 4) \n",
    "# validation_loader = DataLoader(validation_set,batch_size= 1, shuffle= True, drop_last= True, num_workers= 4) \n",
    "model.load_weights('../models/best_segnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.jpg', '1.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '35.jpg', '36.jpg', '37.jpg', '38.jpg', '39.jpg', '40.jpg', '41.jpg', '42.jpg', '43.jpg', '44.jpg', '45.jpg', '46.jpg', '47.jpg', '48.jpg', '49.jpg', '50.jpg', '51.jpg', '52.jpg', '53.jpg', '54.jpg', '55.jpg', '56.jpg', '57.jpg', '58.jpg', '59.jpg', '60.jpg', '61.jpg', '62.jpg', '63.jpg', '64.jpg', '65.jpg', '66.jpg', '67.jpg', '68.jpg', '69.jpg', '70.jpg', '71.jpg', '72.jpg', '73.jpg', '74.jpg', '75.jpg', '76.jpg', '77.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Greek road video\n",
    "import cv2\n",
    "\n",
    "def load_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (448, 448)) # replace with the size of your input image\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def make_video(frames_dir, output_file):\n",
    "    \n",
    "    # get the list of frame filenames\n",
    "    frame_filenames = [f for f in os.listdir(frames_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    # sort the filenames based on the integer value of the filename\n",
    "    frame_filenames = sorted(frame_filenames, key=lambda x: int(x.split(\".\")[0]))\n",
    "\n",
    "    # get the first frame to use as a reference for the video dimensions\n",
    "    first_frame_path = os.path.join(frames_dir, frame_filenames[0])\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    height, width, channels = first_frame.shape\n",
    "    \n",
    "    video_length = len(frame_filenames) * 0.1\n",
    "    fps = len(frame_filenames) / video_length\n",
    "\n",
    "    # initialize the video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(output_file, fourcc, fps , (width, height))\n",
    "    \n",
    "    # loop over the frames directory and process each frame\n",
    "    for filename in frame_filenames:\n",
    "        image_path = os.path.join(frames_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # write the image with the predicted lane points overlaid to the video\n",
    "        video_writer.write(image)\n",
    "\n",
    "    # release the video writer object\n",
    "    video_writer.release()\n",
    "\n",
    "\n",
    "frames_dir = '../clips/good_lanes_clip'\n",
    "\n",
    "frames_list = os.listdir(frames_dir)\n",
    "# sort the filenames based on the integer value of the filename\n",
    "frames_list = sorted(frames_list, key=lambda x: int(x.split(\".\")[0]))\n",
    "print(frames_list)\n",
    "no_prev = 2\n",
    "\n",
    "\n",
    "# loop over the frames directory and process each frame\n",
    "for filename in frames_list:\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "        frame_id = frames_list.index(filename)\n",
    "\n",
    "        # load the image\n",
    "        image_path = os.path.join(frames_dir, filename)\n",
    "        image = torch.from_numpy(load_image(image_path).transpose(2,0,1))\n",
    "        \n",
    "        if frame_id >= no_prev:\n",
    "            previous_frames = []\n",
    "            previous_frames.append(image)\n",
    "            for i in range (frame_id - 1,(frame_id - no_prev) - 1, -1):\n",
    "                previous_fname = frames_list[i]\n",
    "                previous_path = os.path.join(frames_dir, filename)\n",
    "                previous = torch.from_numpy(load_image(image_path).transpose(2,0,1))\n",
    "                previous_frames.append(previous)\n",
    "                \n",
    "            pred = model.predict_temporal(previous_frames)\n",
    "            mask = pred.squeeze(0).detach().cpu().numpy()\n",
    "       \n",
    "            # convert the binary mask to green dots on the original frame\n",
    "            original_image = cv2.imread(image_path)\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            green_dots = np.zeros_like(original_image)\n",
    "            green_dots[:,:,1] = 255 * mask\n",
    "            overlay_image = cv2.addWeighted(original_image, 0.7, green_dots, 0.3, 0)\n",
    "\n",
    "            # save the image with green masked predicted lane points \n",
    "            cv2.imwrite(f'../clips/pred_frames/{filename}', overlay_image)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            pred,_ = model.predict(image.unsqueeze(0))\n",
    "            mask = pred.squeeze(0).cpu().numpy()\n",
    "       \n",
    "            # convert the binary mask to green dots on the original frame\n",
    "            original_image = cv2.imread(image_path)\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            green_dots = np.zeros_like(original_image)\n",
    "            green_dots[:,:,1] = 255 * mask\n",
    "            overlay_image = cv2.addWeighted(original_image, 0.7, green_dots, 0.3, 0)\n",
    "\n",
    "            # save the image with green masked predicted lane points \n",
    "            cv2.imwrite(f'../clips/pred_frames/{filename}', overlay_image)\n",
    "                \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted marked frames dir\n",
    "pred_frames_dir = '../clips/pred_frames'\n",
    "output_file = '../clips/pred_vid_good_temp.mp4'\n",
    "make_video(pred_frames_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,probs = model(train_set[0][0].unsqueeze(0))\n",
    "# x = x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "without, temporal = evaluate(model,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for train and validation \n",
    "train_loader = DataLoader(train_set, batch_size= 1,shuffle= True, drop_last= True, num_workers= 4) \n",
    "validation_loader = DataLoader(validation_set,batch_size= 1, shuffle= True, drop_last= True, num_workers= 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayshape:(448, 448)\n",
      "add1 shape: torch.Size([1, 64, 448, 448])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores \u001b[39m=\u001b[39m train(model, train_loader,val_loader\u001b[39m=\u001b[39;49m validation_loader , num_epochs\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                                                         lane_weight \u001b[39m=\u001b[39;49m pos_weight, lr \u001b[39m=\u001b[39;49m \u001b[39m0.01\u001b[39;49m, SGD_momentum\u001b[39m=\u001b[39;49m \u001b[39m0.9\u001b[39;49m)\n",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb Cell 12\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, weight_decay, SGD_momentum, lr_scheduler, lane_weight)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m inputs \u001b[39m=\u001b[39m train_augmentations(inputs)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m outputs, eval_out \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=220'>221</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mto(device), targets)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=221'>222</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb Cell 12\u001b[0m in \u001b[0;36mSegNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m#Stage 2\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBNEn21(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConvEn21(x))) \n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBNEn22(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mConvEn22(x))) \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m edges2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanny_edge_detection(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dimitris/Downloads/Lane-detection-Master-Thesis/notebooks/segnet.ipynb#X13sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39madd(x, edges2)\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores = train(model, train_loader,val_loader= validation_loader , num_epochs= 1, \n",
    "                                                                                        lane_weight = pos_weight, lr = 0.01, SGD_momentum= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics after training for train and validation sets (bins of 5 epochs)\n",
    "plot_metrics(train_losses,val_losses,train_f1_scores,val_f1_scores,train_iou_scores,val_iou_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses,train_f1_scores,train_iou_scores = train(model, train_loader,val_loader= None , num_epochs= 10, \n",
    "                                                     lane_weight = pos_weight, lr = 0.005, SGD_momentum= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 448, 448]) torch.Size([1, 448, 448])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "img_tns,gt = validation_set[0]\n",
    "\n",
    "model.eval()\n",
    "test_pred, test_features = model.predict(img_tns.unsqueeze(0))\n",
    "\n",
    "test_pred = test_pred.squeeze(0)\n",
    "test_features = test_features.squeeze(0)\n",
    "print(test_pred.shape,test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_temporal(test_dataset[0][0],max_prev_frames=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f74cedf3fe8df28562f21d2c7dd20a528944420e9979c6ce3cb4240ded655862"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
