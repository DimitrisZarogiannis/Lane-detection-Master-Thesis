{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "import json\n",
    "from torchmetrics import F1Score,JaccardIndex, Accuracy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from torchmetrics.classification import BinaryStatScores\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "import utils\n",
    "from vit import ViT\n",
    "from mlp_decoder import DecoderMLP\n",
    "\n",
    "\n",
    "\n",
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "\n",
    "test_annotated = os.listdir(annotated_dir_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "# annotations = list()\n",
    "# for gt_file in annotated:\n",
    "#     path = os.path.join(annotated_dir,gt_file)\n",
    "#     json_gt = [json.loads(line) for line in open(path)]\n",
    "#     annotations.append(json_gt)\n",
    "    \n",
    "# annotations = [a for f in annotations for a in f]\n",
    "\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple test dataset + ground truth dictionary\n",
    "test_annotations = list()\n",
    "for gt_file in test_annotated:\n",
    "    path = os.path.join(annotated_dir_test,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    test_annotations.append(json_gt)\n",
    "    \n",
    "test_annotations = [a for f in test_annotations for a in f]\n",
    "\n",
    "test_dataset = TuSimple(train_annotations = test_annotations, train_img_dir = test_clips_dir, resize_to = (448,448), subset_size = 0.001, test = True, previous= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from : https://github.com/vinceecws/SegNet_PyTorch/blob/master/Pavements/SegNet.py\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chn=3, out_chn=1, BN_momentum=0.5):\n",
    "        super(SegNet, self).__init__()\n",
    "        \n",
    "        #SegNet Architecture\n",
    "        #Takes input of size in_chn = 3 (RGB images have 3 channels)\n",
    "        #Outputs size label_chn (N # of classes)\n",
    "\n",
    "        self.lane_threshold = 0.5\n",
    "        self.in_chn = in_chn\n",
    "        self.out_chn = out_chn\n",
    "\n",
    "        self.MaxEn = nn.MaxPool2d(2, stride=2, return_indices=True) \n",
    "\n",
    "        self.ConvEn11 = nn.Conv2d(self.in_chn, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn11 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvEn12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn21 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvEn22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn31 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn41 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "\n",
    "        #DECODING consists of 5 stages\n",
    "        #Each stage corresponds to their respective counterparts in ENCODING\n",
    "\n",
    "        #General Max Pool 2D/Upsampling for DECODING layers\n",
    "        self.MaxDe = nn.MaxUnpool2d(2, stride=2) \n",
    "\n",
    "        self.ConvDe53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe41 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe41 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe31 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvDe21 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe21 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvDe11 = nn.Conv2d(64, self.out_chn, kernel_size=3, padding=1)\n",
    "        self.BNDe11 = nn.BatchNorm2d(self.out_chn, momentum=BN_momentum)\n",
    "        \n",
    "\n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.local_response_norm(x, size=3)\n",
    "\n",
    "        #ENCODE LAYERS\n",
    "        #Stage 1\n",
    "        x = F.relu(self.BNEn11(self.ConvEn11(x))) \n",
    "        x = F.relu(self.BNEn12(self.ConvEn12(x)))\n",
    "         \n",
    "        x, ind1 = self.MaxEn(x)\n",
    "        size1 = x.size()\n",
    "\n",
    "\n",
    "        #Stage 2\n",
    "        x = F.relu(self.BNEn21(self.ConvEn21(x))) \n",
    "        x = F.relu(self.BNEn22(self.ConvEn22(x))) \n",
    "        \n",
    "        x, ind2 = self.MaxEn(x)\n",
    "        size2 = x.size()\n",
    "\n",
    "        #Stage 3\n",
    "        x = F.relu(self.BNEn31(self.ConvEn31(x))) \n",
    "        x = F.relu(self.BNEn32(self.ConvEn32(x))) \n",
    "        x = F.relu(self.BNEn33(self.ConvEn33(x)))\n",
    "        \n",
    "        x, ind3 = self.MaxEn(x)\n",
    "        size3 = x.size()\n",
    "        \n",
    "        #Stage 4\n",
    "        x = F.relu(self.BNEn41(self.ConvEn41(x))) \n",
    "        x = F.relu(self.BNEn42(self.ConvEn42(x))) \n",
    "        x = F.relu(self.BNEn43(self.ConvEn43(x)))   \n",
    "         \n",
    "        x, ind4 = self.MaxEn(x)\n",
    "        size4 = x.size()\n",
    "\n",
    "        #Stage 5\n",
    "        x = F.relu(self.BNEn51(self.ConvEn51(x))) \n",
    "        x = F.relu(self.BNEn52(self.ConvEn52(x))) \n",
    "        x = F.relu(self.BNEn53(self.ConvEn53(x)))\n",
    "            \n",
    "        x, ind5 = self.MaxEn(x)\n",
    "        size5 = x.size()\n",
    "\n",
    "        #DECODE LAYERS\n",
    "        #Stage 5\n",
    "        x = self.MaxDe(x, ind5, output_size=size4)\n",
    "        x = F.relu(self.BNDe53(self.ConvDe53(x)))\n",
    "        x = F.relu(self.BNDe52(self.ConvDe52(x)))\n",
    "        x = F.relu(self.BNDe51(self.ConvDe51(x)))\n",
    "\n",
    "        #Stage 4\n",
    "        x = self.MaxDe(x, ind4, output_size=size3)\n",
    "        x = F.relu(self.BNDe43(self.ConvDe43(x)))\n",
    "        x = F.relu(self.BNDe42(self.ConvDe42(x)))\n",
    "        x = F.relu(self.BNDe41(self.ConvDe41(x)))\n",
    "\n",
    "        #Stage 3\n",
    "        x = self.MaxDe(x, ind3, output_size=size2)\n",
    "        x = F.relu(self.BNDe33(self.ConvDe33(x)))\n",
    "        x = F.relu(self.BNDe32(self.ConvDe32(x)))\n",
    "        x = F.relu(self.BNDe31(self.ConvDe31(x)))\n",
    "\n",
    "        #Stage 2\n",
    "        x = self.MaxDe(x, ind2, output_size=size1)\n",
    "        x = F.relu(self.BNDe22(self.ConvDe22(x)))\n",
    "        x = F.relu(self.BNDe21(self.ConvDe21(x)))\n",
    "\n",
    "        #Stage 1\n",
    "        x = self.MaxDe(x, ind1)\n",
    "        x = F.relu(self.BNDe12(self.ConvDe12(x)))\n",
    "        x = self.ConvDe11(x)\n",
    "        \n",
    "        probs = F.sigmoid(x)\n",
    "        \n",
    "        return x,probs\n",
    "\n",
    "    # Make a single prediction \n",
    "    def predict(self,x):\n",
    "        self.eval()\n",
    "        cnn_features,probs = self.forward(x)\n",
    "        prediction = torch.where(probs > self.lane_threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
    "        return prediction,cnn_features\n",
    "\n",
    "    def canny_edge_detection_batch(self, x, low_threshold=50, high_threshold=150):\n",
    "        batch_size, feature_dims, height, width = x.shape\n",
    "        edges = []\n",
    "        for i in range(batch_size):\n",
    "            # Convert tensor to numpy array and transpose\n",
    "            img = x[i].detach().cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "            # Calculate mean of channels\n",
    "            gray = np.mean(img, axis=2)\n",
    "\n",
    "            # Apply Canny edge detection\n",
    "            edges_img = cv2.Canny(gray.astype(np.uint8), low_threshold, high_threshold)\n",
    "    \n",
    "            # Convert back to tensor\n",
    "            edges_img = torch.Tensor(edges_img).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "            # Expand edge_feature_map to have the same number of channels as conv_feature_map\n",
    "            edges_img = edges_img.repeat(1, feature_dims, 1, 1)\n",
    "        \n",
    "            edges.append(edges_img)\n",
    "    \n",
    "        edges = torch.cat(edges, dim=0)\n",
    "    \n",
    "        return edges\n",
    "    \n",
    "    # Predict with temporal post-processing\n",
    "    def predict_temporal (self,x):\n",
    "        self.eval()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        with torch.no_grad():\n",
    "            for clip in x:\n",
    "                base_frame = clip.to(device)\n",
    "                # Predict mask probs for base frame\n",
    "                _,base_mask_prob = self.forward(base_frame.unsqueeze(0))\n",
    "            \n",
    "                previous_masks = []\n",
    "            \n",
    "                # Predict masks probs for previous frames\n",
    "                for i in range(1,len(x)):\n",
    "                    prev_frame = x[i].to(device)\n",
    "                    _,prev_mask = self.forward(prev_frame.unsqueeze(0))\n",
    "                    previous_masks.append(prev_mask)\n",
    "                \n",
    "                # Define the decay factor for the previous frames weights\n",
    "                decay_factor = 0.8\n",
    "\n",
    "                # Calculate the weights for each previous frame\n",
    "                weights = [decay_factor**i for i in range(len(clip[0])- 1)]\n",
    "            \n",
    "                # Normalize the weights so that they sum to 1\n",
    "                weights /= np.sum(weights)\n",
    "\n",
    "                # Loop over the previous frames and update the class probabilities for the target frame\n",
    "                for i, prev_mask in enumerate(previous_masks):\n",
    "                    weight = weights[i]\n",
    "                    update_mask = (base_mask_prob < 0.85)\n",
    "                    base_mask_prob[update_mask] = (1 - weight) * base_mask_prob[update_mask] + weight * prev_mask[update_mask]\n",
    "                # # Loop over the previous frames and update the class probabilities for the target frame\n",
    "                # for i, prev_mask in enumerate(previous_masks):\n",
    "                #     weight = weights[i]\n",
    "                #     base_mask_prob[:, 0, :, :] = (1 - weight) * base_mask_prob[:, 0, :, :] + weight * prev_mask[:, 0, :, :]\n",
    "                \n",
    "            prediction = torch.where(base_mask_prob > self.lane_threshold, torch.ones_like(base_mask_prob), torch.zeros_like(base_mask_prob))\n",
    "            \n",
    "        return prediction\n",
    "            \n",
    "    # Load trained model weights\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n",
    "        print('Loaded state dict succesfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.1)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set function (with temporal post-processing)\n",
    "def evaluate(model, test_set):\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    stat_scores = BinaryStatScores().to(device)\n",
    "    accuracy = Accuracy(task=\"binary\").to(device)\n",
    "        \n",
    "    test_f1 = 0\n",
    "    test_iou = 0\n",
    "    test_accuracy = 0\n",
    "    fps_temp = 0\n",
    "    \n",
    "    \n",
    "    no_temp_f1 = 0\n",
    "    no_temp_iou = 0\n",
    "    no_temp_accuracy = 0\n",
    "    fps_no = 0\n",
    "    \n",
    "    all_stats_no = []\n",
    "    all_stats_temp = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for clip in test_set:\n",
    "            gt = clip[1].to(device)\n",
    "            base_frame = clip[0][0].to(device)\n",
    "            \n",
    "            start_time1 = time.time()\n",
    "            \n",
    "            start_time2 = time.time()\n",
    "            \n",
    "            # Predict mask probs for base frame\n",
    "            _,base_mask_prob = model.forward(base_frame.unsqueeze(0))\n",
    "            \n",
    "            end_time1 = time.time()\n",
    "            \n",
    "            # Get F1 scores,IoU and Accuracy before temporal post process\n",
    "            no_temp_f1 += f1_score(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            no_temp_iou += iou_score(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            no_temp_accuracy += accuracy(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            \n",
    "            # Measure FPS without temporal post process\n",
    "            processing_time = end_time1 - start_time1\n",
    "            fps_no += (1 / processing_time)\n",
    "            \n",
    "            # Get stats to calculate FPR/FNR\n",
    "            all_stats_no.append(stat_scores(base_mask_prob.to(device), gt.unsqueeze(0)))\n",
    "            \n",
    "            previous_masks = []\n",
    "            \n",
    "            # Predict masks probs for previous frames\n",
    "            for i in range(1,len(clip[0])):\n",
    "                prev_frame = clip[0][i].to(device)\n",
    "                _,prev_mask = model.forward(prev_frame.unsqueeze(0))\n",
    "                previous_masks.append(prev_mask)\n",
    "            \n",
    "            # Define the decay factor for the previous frames weights\n",
    "            decay_factor = 0.8\n",
    "\n",
    "            # Calculate the weights for each previous frame\n",
    "            weights = [decay_factor**i for i in range(len(clip[0])- 1)]\n",
    "            \n",
    "            # Normalize the weights so that they sum to 1\n",
    "            weights /= np.sum(weights)\n",
    "            \n",
    "            \n",
    "            # Loop over the previous frames and update the class probabilities for the target frame\n",
    "            for i, prev_mask in enumerate(previous_masks):\n",
    "                weight = weights[i]\n",
    "                base_mask_prob[:, 0, :, :] = (1 - weight) * base_mask_prob[:, 0, :, :] + weight * prev_mask[:, 0, :, :]\n",
    "                \n",
    "            end_time2 = time.time()\n",
    "            \n",
    "            # Measure FPS with temporal post process\n",
    "            processing_time = end_time2 - start_time2\n",
    "            fps_temp += (1 / processing_time)\n",
    "            \n",
    "            # Compare new mask with temporal post process with gt and get evaluation scores\n",
    "            test_f1 += f1_score(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            test_iou += iou_score (base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            test_accuracy += accuracy(base_mask_prob.to(device), gt.unsqueeze(0))\n",
    "            \n",
    "            # Get stats for FNR/FPR with temporal\n",
    "            all_stats_temp.append(stat_scores(base_mask_prob.to(device), gt.unsqueeze(0)))\n",
    "            \n",
    "        # Get FPR and FNR for test set (with and without temporal)\n",
    "        \n",
    "        fp1_sum = torch.stack([s[1] for s in all_stats_no]).sum()\n",
    "        fn1_sum = torch.stack([s[3] for s in all_stats_no]).sum()\n",
    "        tn1_sum = torch.stack([s[2] for s in all_stats_no]).sum()\n",
    "        tp1_sum = torch.stack([s[0] for s in all_stats_no]).sum()\n",
    "        \n",
    "        # Average fpr without temporal\n",
    "        fpr_no = fp1_sum / (tn1_sum + fp1_sum)\n",
    "        fnr_no = fn1_sum / (tp1_sum + fn1_sum)\n",
    "        \n",
    "        fp2_sum = torch.stack([s[1] for s in all_stats_temp]).sum()\n",
    "        fn2_sum = torch.stack([s[3] for s in all_stats_temp]).sum()\n",
    "        tn2_sum = torch.stack([s[2] for s in all_stats_temp]).sum()\n",
    "        tp2_sum = torch.stack([s[0] for s in all_stats_temp]).sum()\n",
    "        \n",
    "        # Average fpr and fnr with temporal\n",
    "        fpr_temp = fp2_sum / (tn2_sum + fp2_sum)\n",
    "        fnr_temp = fn2_sum / (tp2_sum + fn2_sum)\n",
    "        \n",
    "        # Calculate test set average metrics\n",
    "        test_f1 /= len(test_set)\n",
    "        test_iou /= len(test_set)\n",
    "        no_temp_f1 /= len(test_set)\n",
    "        no_temp_iou /= len(test_set)\n",
    "        no_temp_accuracy /= len(test_set)\n",
    "        test_accuracy /= len(test_set)\n",
    "        fps_temp /= len(test_set)\n",
    "        fps_no /= len(test_set)\n",
    "        \n",
    "        # Create lists of metrics with and without temporal post process\n",
    "        \n",
    "        without = [round(fpr_no.item(),4), round(fnr_no.item(),4), round(no_temp_f1.item(),3), round(no_temp_iou.item(),3), round(no_temp_accuracy.item(),3) * 100, round(float(fps_no),3)]\n",
    "        \n",
    "        temporal = [round(fpr_temp.item(),4), round(fnr_temp.item(),4), round(test_f1.item(),3), round(test_iou.item(),3),round(test_accuracy.item(),3) * 100 ,round(float(fps_temp),3)]\n",
    "        \n",
    "        return without,temporal\n",
    "            \n",
    "\n",
    "# Plot metrics function \n",
    "def plot_metrics(train_losses, val_losses, train_f1, val_f1, train_iou, val_iou):\n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation')\n",
    "    plt.xlabel('Epochs (bins of 5)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(0, len(train_losses) + 1, 5))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation F1 scores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_f1) + 1), train_f1, label='Train')\n",
    "    plt.plot(range(1, len(val_f1) + 1), val_f1, label='Validation')\n",
    "    plt.xlabel('Epochs (bins of 5)')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(range(0, len(train_f1) + 1, 5))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation IoU scores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_iou) + 1), train_iou, label='Train')\n",
    "    plt.plot(range(1, len(val_iou) + 1), val_iou, label='Validation')\n",
    "    plt.xlabel('Epochs (bins of 5)')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.xticks(range(0, len(train_iou) + 1, 5))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Custom training function for the pipeline with schedule and augmentations\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, weight_decay=0, SGD_momentum = 0.9, lr_scheduler=False, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=SGD_momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        pass\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    gt_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "  \n",
    "    train_augmentations = transforms.Compose([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                                            transforms.ColorJitter(brightness=0.35, contrast=0.2, saturation=0.4, hue=0.1)])\n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Metrics collection for plotting\n",
    "    train_losses = []\n",
    "    train_f1_scores = []\n",
    "    train_iou_scores = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    val_iou_scores = []\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = gt_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)]\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "\n",
    "            inputs = train_augmentations(inputs).to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "        \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    logits,outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    val_loss = criterion(logits.to(device),targets)\n",
    "                    val_loss += val_loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Collect metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        train_iou_scores.append(train_iou)\n",
    "    \n",
    "        if val_loader:\n",
    "            val_losses.append(val_loss)\n",
    "            val_f1_scores.append(val_f1)\n",
    "            val_iou_scores.append(val_iou)\n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            # scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))\n",
    "            \n",
    "            if val_loader:\n",
    "                print('Val_Loss: {} - Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_loss,val_f1,val_iou))\n",
    "    \n",
    "    if val_loader:\n",
    "        return train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores\n",
    "    else:\n",
    "        return train_losses,train_f1_scores,train_iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 29443587\n",
      "Loaded state dict succesfully!\n"
     ]
    }
   ],
   "source": [
    "model = SegNet()\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "model.load_weights('../models/best_segnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greek road video\n",
    "import cv2\n",
    "\n",
    "def load_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (448, 448)) # replace with the size of your input image\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def make_video(frames_dir, output_file):\n",
    "    \n",
    "    # get the list of frame filenames\n",
    "    frame_filenames = [f for f in os.listdir(frames_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    # sort the filenames based on the integer value of the filename\n",
    "    frame_filenames = sorted(frame_filenames, key=lambda x: int(x.split(\".\")[0]))\n",
    "\n",
    "    # get the first frame to use as a reference for the video dimensions\n",
    "    first_frame_path = os.path.join(frames_dir, frame_filenames[0])\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    height, width, channels = first_frame.shape\n",
    "    \n",
    "    video_length = len(frame_filenames) * 0.1\n",
    "    fps = len(frame_filenames) / video_length\n",
    "\n",
    "    # initialize the video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(output_file, fourcc, fps , (width, height))\n",
    "    \n",
    "    # loop over the frames directory and process each frame\n",
    "    for filename in frame_filenames:\n",
    "        image_path = os.path.join(frames_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # write the image with the predicted lane points overlaid to the video\n",
    "        video_writer.write(image)\n",
    "\n",
    "    # release the video writer object\n",
    "    video_writer.release()\n",
    "\n",
    "\n",
    "frames_dir = '../clips/good_lanes_clip'\n",
    "\n",
    "frames_list = os.listdir(frames_dir)\n",
    "# sort the filenames based on the integer value of the filename\n",
    "frames_list = sorted(frames_list, key=lambda x: int(x.split(\".\")[0]))\n",
    "print(frames_list)\n",
    "no_prev = 2\n",
    "\n",
    "\n",
    "# loop over the frames directory and process each frame\n",
    "for filename in frames_list:\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "        frame_id = frames_list.index(filename)\n",
    "\n",
    "        # load the image\n",
    "        image_path = os.path.join(frames_dir, filename)\n",
    "        image = torch.from_numpy(load_image(image_path).transpose(2,0,1))\n",
    "        \n",
    "        if frame_id >= no_prev:\n",
    "            previous_frames = []\n",
    "            previous_frames.append(image)\n",
    "            for i in range (frame_id - 1,(frame_id - no_prev) - 1, -1):\n",
    "                previous_fname = frames_list[i]\n",
    "                previous_path = os.path.join(frames_dir, filename)\n",
    "                previous = torch.from_numpy(load_image(image_path).transpose(2,0,1))\n",
    "                previous_frames.append(previous)\n",
    "                \n",
    "            pred = model.predict_temporal(previous_frames)\n",
    "            mask = pred.squeeze(0).detach().cpu().numpy()\n",
    "       \n",
    "            # convert the binary mask to green dots on the original frame\n",
    "            original_image = cv2.imread(image_path)\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            green_dots = np.zeros_like(original_image)\n",
    "            green_dots[:,:,1] = 255 * mask\n",
    "            overlay_image = cv2.addWeighted(original_image, 0.7, green_dots, 0.3, 0)\n",
    "\n",
    "            # save the image with green masked predicted lane points \n",
    "            cv2.imwrite(f'../clips/pred_frames/{filename}', overlay_image)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            pred,_ = model.predict(image.unsqueeze(0))\n",
    "            mask = pred.squeeze(0).cpu().numpy()\n",
    "       \n",
    "            # convert the binary mask to green dots on the original frame\n",
    "            original_image = cv2.imread(image_path)\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            green_dots = np.zeros_like(original_image)\n",
    "            green_dots[:,:,1] = 255 * mask\n",
    "            overlay_image = cv2.addWeighted(original_image, 0.7, green_dots, 0.3, 0)\n",
    "\n",
    "            # save the image with green masked predicted lane points \n",
    "            cv2.imwrite(f'../clips/pred_frames/{filename}', overlay_image)\n",
    "                \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted marked frames dir\n",
    "pred_frames_dir = '../clips/pred_frames'\n",
    "output_file = '../clips/pred_vid_good_temp.mp4'\n",
    "make_video(pred_frames_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "without, temporal = evaluate(model,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for train and validation \n",
    "train_loader = DataLoader(train_set, batch_size= 2,shuffle= True, drop_last= True, num_workers= 4) \n",
    "validation_loader = DataLoader(validation_set,batch_size= 2, shuffle= True, drop_last= True, num_workers= 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses,train_f1_scores,train_iou_scores,val_losses,val_f1_scores,val_iou_scores = train(model, train_loader,val_loader= validation_loader , num_epochs= 2, \n",
    "                                                                                        lane_weight = pos_weight, lr = 0.01, SGD_momentum= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics after training for train and validation sets (bins of 5 epochs)\n",
    "plot_metrics(train_losses,val_losses,train_f1_scores,val_f1_scores,train_iou_scores,val_iou_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 448, 448]) torch.Size([1, 448, 448])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "img_tns,gt = validation_set[0]\n",
    "\n",
    "model.eval()\n",
    "test_pred, test_features = model.predict(img_tns.unsqueeze(0))\n",
    "\n",
    "test_pred = test_pred.squeeze(0)\n",
    "test_features = test_features.squeeze(0)\n",
    "print(test_pred.shape,test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_temporal(test_dataset[1][0]).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 448, 448])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.disp_img(utils.toImagearr(pred), name = 'Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.disp_img(utils.toImagearr(test_dataset[1][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Plot the clustering results with one line passing from each cluster center\n",
    "    # fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    # ax.scatter(points[:,0], points[:,1], c=labels, cmap='viridis')\n",
    "\n",
    "    # # Plot a line for each cluster center\n",
    "    # for center in cluster_centers:\n",
    "    #     angle = np.arctan2(center[1], center[0])\n",
    "    #     rho = center[0]*np.cos(angle) + center[1]*np.sin(angle)\n",
    "    #     x = np.linspace(-1000, 1000, 2)\n",
    "    #     y = (rho - x*np.cos(angle))/np.sin(angle)\n",
    "    #     ax.plot(x,y, c='r', linewidth=2)\n",
    "\n",
    "    # ax.axis('equal')\n",
    "    # plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  [0 0 1 0 1 1 1 1 0 2 1 2 2 1]\n",
      "Cluster centers:  [array([271.5      ,   0.4537856], dtype=float32), array([-71.14286 ,   2.435981], dtype=float32), array([39.333332 ,  2.0362175], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def post_process_segmentation_output(segmentation_output):\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(segmentation_output.astype(np.uint8), 100, 200)\n",
    "    \n",
    "    utils.disp_img(edges)\n",
    "\n",
    "    lines = cv2.HoughLines(edges,rho = 1, theta=np.pi/180, threshold = 80)\n",
    "    \n",
    "    # Create a black image with the same size as the original image\n",
    "    disp_lines = np.zeros_like(segmentation_output)\n",
    "\n",
    "    # Draw each line on the mask\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            rho, theta = line[0]\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            x0 = a * rho\n",
    "            y0 = b * rho\n",
    "            x1 = int(x0 + 1000 * (-b))\n",
    "            y1 = int(y0 + 1000 * (a))\n",
    "            x2 = int(x0 - 1000 * (-b))\n",
    "            y2 = int(y0 - 1000 * (a))\n",
    "            cv2.line(disp_lines, (x1, y1), (x2, y2), 255, 1)\n",
    "\n",
    "    # Display the binary mask\n",
    "    utils.disp_img(disp_lines)\n",
    "    \n",
    "    points = lines.squeeze()\n",
    "    \n",
    "    # Perform mean shift clustering\n",
    "    dbs = DBSCAN(eps=25, min_samples=3)\n",
    "    # Fit the model and get the labels\n",
    "    labels = dbs.fit_predict(points)\n",
    "\n",
    "    # Get the unique labels and their counts\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    # Compute the mean of each cluster to get the cluster centers\n",
    "    cluster_centers = []\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            continue\n",
    "        cluster_center = np.mean(points[labels == label], axis=0)\n",
    "        cluster_centers.append(cluster_center)\n",
    "        \n",
    "    # Print the labels and cluster centers\n",
    "    print(\"Labels: \", labels)\n",
    "    print(\"Cluster centers: \", cluster_centers)\n",
    "\n",
    "    # Plot the mean shift clustering results\n",
    "    plt.scatter(points[:,0], points[:,1], c=labels, cmap='viridis')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Angle')\n",
    "    plt.show()\n",
    "    \n",
    "    # Convert cluster centers to Cartesian coordinates\n",
    "    cart_centers = []\n",
    "    for center in cluster_centers:\n",
    "        rho = center[0]\n",
    "        theta = center[1]\n",
    "        x = rho * np.cos(theta)\n",
    "        y = rho * np.sin(theta)\n",
    "        cart_centers.append((x, y))\n",
    "    return 1\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "vp = post_process_segmentation_output(utils.toImagearr(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f74cedf3fe8df28562f21d2c7dd20a528944420e9979c6ce3cb4240ded655862"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
