{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Set imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "import json\n",
    "from torchmetrics import F1Score,JaccardIndex\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "import utils\n",
    "from vit import ViT\n",
    "from mlp_decoder import DecoderMLP\n",
    "\n",
    "\n",
    "\n",
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated_dir_test = os.path.join(root_dir,'datasets/tusimple/test_set/annotations/')\n",
    "test_clips_dir = os.path.join(root_dir,'datasets/tusimple/test_set/')\n",
    "\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "test_annotated = os.listdir(annotated_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path directories for clips and annotations for the TUSimple training  dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "\n",
    "# Get path directories for clips and annotations for the TUSimple test dataset + ground truth dictionary\n",
    "test_annotations = list()\n",
    "for gt_file in test_annotated:\n",
    "    path = os.path.join(annotated_dir_test,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    test_annotations.append(json_gt)\n",
    "    \n",
    "test_annotations = [a for f in test_annotations for a in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chn=3, out_chn=1, BN_momentum=0.5):\n",
    "        super(SegNet, self).__init__()\n",
    "\n",
    "        #SegNet Architecture\n",
    "        #Takes input of size in_chn = 3 (RGB images have 3 channels)\n",
    "        #Outputs size label_chn (N # of classes)\n",
    "\n",
    "        #ENCODING consists of 5 stages\n",
    "        #Stage 1, 2 has 2 layers of Convolution + Batch Normalization + Max Pool respectively\n",
    "        #Stage 3, 4, 5 has 3 layers of Convolution + Batch Normalization + Max Pool respectively\n",
    "\n",
    "        #General Max Pool 2D for ENCODING layers\n",
    "        #Pooling indices are stored for Upsampling in DECODING layers\n",
    "\n",
    "        self.lane_threshold = 0.5\n",
    "        self.in_chn = in_chn\n",
    "        self.out_chn = out_chn\n",
    "\n",
    "        self.MaxEn = nn.MaxPool2d(2, stride=2, return_indices=True) \n",
    "\n",
    "        self.ConvEn11 = nn.Conv2d(self.in_chn, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn11 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvEn12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn21 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvEn22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn31 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn41 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "\n",
    "        #DECODING consists of 5 stages\n",
    "        #Each stage corresponds to their respective counterparts in ENCODING\n",
    "\n",
    "        #General Max Pool 2D/Upsampling for DECODING layers\n",
    "        self.MaxDe = nn.MaxUnpool2d(2, stride=2) \n",
    "\n",
    "        self.ConvDe53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe41 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe41 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe31 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvDe21 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe21 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvDe11 = nn.Conv2d(64, self.out_chn, kernel_size=3, padding=1)\n",
    "        self.BNDe11 = nn.BatchNorm2d(self.out_chn, momentum=BN_momentum)\n",
    "    \n",
    "    # Count pipeline trainable parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #ENCODE LAYERS\n",
    "        #Stage 1\n",
    "        x = F.relu(self.BNEn11(self.ConvEn11(x))) \n",
    "        x = F.relu(self.BNEn12(self.ConvEn12(x))) \n",
    "        x, ind1 = self.MaxEn(x)\n",
    "        size1 = x.size()\n",
    "\n",
    "        #Stage 2\n",
    "        x = F.relu(self.BNEn21(self.ConvEn21(x))) \n",
    "        x = F.relu(self.BNEn22(self.ConvEn22(x))) \n",
    "        x, ind2 = self.MaxEn(x)\n",
    "        size2 = x.size()\n",
    "\n",
    "        #Stage 3\n",
    "        x = F.relu(self.BNEn31(self.ConvEn31(x))) \n",
    "        x = F.relu(self.BNEn32(self.ConvEn32(x))) \n",
    "        x = F.relu(self.BNEn33(self.ConvEn33(x)))   \n",
    "        x, ind3 = self.MaxEn(x)\n",
    "        size3 = x.size()\n",
    "\n",
    "        #Stage 4\n",
    "        x = F.relu(self.BNEn41(self.ConvEn41(x))) \n",
    "        x = F.relu(self.BNEn42(self.ConvEn42(x))) \n",
    "        x = F.relu(self.BNEn43(self.ConvEn43(x)))   \n",
    "        x, ind4 = self.MaxEn(x)\n",
    "        size4 = x.size()\n",
    "\n",
    "        #Stage 5\n",
    "        x = F.relu(self.BNEn51(self.ConvEn51(x))) \n",
    "        x = F.relu(self.BNEn52(self.ConvEn52(x))) \n",
    "        x = F.relu(self.BNEn53(self.ConvEn53(x)))   \n",
    "        x, ind5 = self.MaxEn(x)\n",
    "        size5 = x.size()\n",
    "\n",
    "        #DECODE LAYERS\n",
    "        #Stage 5\n",
    "        x = self.MaxDe(x, ind5, output_size=size4)\n",
    "        x = F.relu(self.BNDe53(self.ConvDe53(x)))\n",
    "        x = F.relu(self.BNDe52(self.ConvDe52(x)))\n",
    "        x = F.relu(self.BNDe51(self.ConvDe51(x)))\n",
    "\n",
    "        #Stage 4\n",
    "        x = self.MaxDe(x, ind4, output_size=size3)\n",
    "        x = F.relu(self.BNDe43(self.ConvDe43(x)))\n",
    "        x = F.relu(self.BNDe42(self.ConvDe42(x)))\n",
    "        x = F.relu(self.BNDe41(self.ConvDe41(x)))\n",
    "\n",
    "        #Stage 3\n",
    "        x = self.MaxDe(x, ind3, output_size=size2)\n",
    "        x = F.relu(self.BNDe33(self.ConvDe33(x)))\n",
    "        x = F.relu(self.BNDe32(self.ConvDe32(x)))\n",
    "        x = F.relu(self.BNDe31(self.ConvDe31(x)))\n",
    "\n",
    "        #Stage 2\n",
    "        x = self.MaxDe(x, ind2, output_size=size1)\n",
    "        x = F.relu(self.BNDe22(self.ConvDe22(x)))\n",
    "        x = F.relu(self.BNDe21(self.ConvDe21(x)))\n",
    "\n",
    "        #Stage 1\n",
    "        x = self.MaxDe(x, ind1)\n",
    "        x = F.relu(self.BNDe12(self.ConvDe12(x)))\n",
    "        x = self.ConvDe11(x)\n",
    "        \n",
    "        probs = F.sigmoid(x)\n",
    "        \n",
    "        return x,probs\n",
    "\n",
    "    # Make a single prediction \n",
    "    def predict(self,x):\n",
    "        self.eval()\n",
    "        cnn_features,probs = self.forward(x)\n",
    "        prediction = torch.where(probs > self.lane_threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
    "        return prediction,cnn_features\n",
    "        \n",
    "    # Load trained model weights\n",
    "    def load_weights(self,path): \n",
    "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n",
    "        print('Loaded state dict succesfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset / Calculate pos weight\n",
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (448,448), subset_size = 0.001,val_size= 0.1)\n",
    "train_set, validation_set = dataset.train_val_split()\n",
    "del dataset\n",
    "\n",
    "# Lane weight\n",
    "pos_weight = utils.calculate_class_weight(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function for the transformer pipeline with schedule and augmentations\n",
    "def train(model, train_loader, val_loader = None, num_epochs=10, lr=0.01, weight_decay=0, SGD_momentum = 0.9, lr_scheduler=False, lane_weight = None):\n",
    "    # Set up loss function and optimizer\n",
    "    criterion =  nn.BCEWithLogitsLoss(pos_weight= lane_weight)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=SGD_momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        pass\n",
    "\n",
    "    # Set up device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_score = F1Score(task=\"binary\").to(device)\n",
    "    iou_score = JaccardIndex(task= 'binary').to(device)\n",
    "    \n",
    "    gt_augmentations = transforms.Compose([transforms.RandomRotation(degrees=(10, 30)),\n",
    "                                              transforms.RandomHorizontalFlip()])\n",
    "  \n",
    "    train_augmentations = transforms.Compose([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                                            transforms.ColorJitter(brightness=0.35, contrast=0.2, saturation=0.4, hue=0.1)])\n",
    "    # Set a seed for augmentations\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_f1 = 0\n",
    "        \n",
    "        val_iou = 0\n",
    "        val_f1 = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Combine the inputs and targets into a single tensor\n",
    "            data = torch.cat((inputs, targets), dim=1)\n",
    "            # Apply the same augmentations to the combined tensor\n",
    "            augmented_data = gt_augmentations(data)    \n",
    "    \n",
    "            # Split the augmented data back into individual inputs and targets\n",
    "            inputs = augmented_data[:, :inputs.size(1)]\n",
    "            targets = augmented_data[:, inputs.size(1):].to(device)\n",
    "\n",
    "            inputs = train_augmentations(inputs).to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            outputs, eval_out = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.to(device), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iou += iou_score(eval_out.to(device).detach(), targets)\n",
    "            train_f1 += f1_score(eval_out.to(device).detach(),targets)\n",
    "        \n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader): \n",
    "                \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    logits,outputs = model(inputs)\n",
    "                \n",
    "                    val_iou += iou_score(outputs.to(device), targets)\n",
    "                    val_f1 += f1_score(outputs.to(device),targets)\n",
    "        \n",
    "                val_iou /= len(val_loader)\n",
    "                val_f1 /= len(val_loader)\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_f1 /= len(train_loader)\n",
    "        \n",
    "        \n",
    "        \n",
    "     # Print progress\n",
    "        if lr_scheduler:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Learning Rate: {:.6f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss,optimizer.param_groups[0]['lr'], train_iou, train_f1))\n",
    "            # scheduler.step()\n",
    "            if val_loader:\n",
    "                print('Val_F1: {:.5f}  - Val_IoU: {:.5f} '.format(val_f1,val_iou))\n",
    "        else:\n",
    "            print('Epoch: {} - Train Loss: {:.4f} - Train_IoU: {:.5f} - Train_F1: {:.5f}'.format(epoch+1, train_loss, train_iou, train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "\t\"batch_size\": 4,\n",
    "\t\"epochs\": 10,\n",
    "\t\"learning_rate\": 0.005,\n",
    "\t\"sgd_momentum\": 0.9,\n",
    "\t\"bn_momentum\": 0.5,\n",
    "\t\"lane_weight\": pos_weight,\n",
    "\t\"no_cuda\": False,\n",
    "\t\"seed\": 42,\n",
    "\t\"in_chn\": 3,\n",
    "\t\"out_chn\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 29443587\n",
      "Loaded state dict succesfully!\n"
     ]
    }
   ],
   "source": [
    "model = SegNet(3,parameters['out_chn'],BN_momentum= parameters['bn_momentum'])\n",
    "print(f'Number of trainable parameters : {model.count_parameters()}')\n",
    "\n",
    "# Create dataloaders and train the model\n",
    "train_loader = DataLoader(train_set, batch_size= 2,shuffle= True, drop_last= True, num_workers= 4) \n",
    "validation_loader = DataLoader(validation_set,batch_size= 1, shuffle= True, drop_last= True, num_workers= 4) \n",
    "model.load_weights('../models/segnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, train_loader,val_loader= None , num_epochs= parameters['epochs'], \n",
    "      lane_weight = pos_weight, lr = parameters['learning_rate'], SGD_momentum= parameters['sgd_momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 448, 448]) torch.Size([1, 448, 448])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "img_tns,gt = validation_set[0]\n",
    "\n",
    "model.eval()\n",
    "test_pred, test_features = model.predict(img_tns.unsqueeze(0))\n",
    "\n",
    "test_pred = test_pred.squeeze(0)\n",
    "test_features = test_features.squeeze(0)\n",
    "print(test_pred.shape,test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.plot_img_pred(img_tns,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 448, 448])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' NOT SURE ABOUT THIS'''\n",
    "# Compute the mean and standard deviation of the foreground pixels\n",
    "foreground = test_features[gt > 0]\n",
    "mean = foreground.mean()\n",
    "std = foreground.std()\n",
    "\n",
    "# Normalize the features\n",
    "features_normalized = (test_features - mean) / std\n",
    "features_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (patch_embedding): PatchEmbedding(\n",
       "    (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_tiny_module = ViT(image_size=448, patch_size=16, num_classes=1, dim=384, depth=6, heads=3, \n",
    "                      mlp_dim=768, dropout=0.1,load_pre= False)\n",
    "\n",
    "vit_tiny_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 384])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_out = vit_tiny_module(features_normalized.unsqueeze(0))\n",
    "vit_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderMLP(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_classifier = DecoderMLP(n_classes = 1, d_encoder = 384, image_size=(448,448))\n",
    "patch_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_classified = patch_classifier(vit_out)\n",
    "patches_classified.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
