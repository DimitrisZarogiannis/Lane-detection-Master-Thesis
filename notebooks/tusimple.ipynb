{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626\n"
     ]
    }
   ],
   "source": [
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "\n",
    "annotated = os.listdir(annotated_dir)\n",
    "\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]\n",
    "\n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate segmentation masks functionality/ Useful for resizing ground truth masks NOTE: dims = (H,W,C)\n",
    "img_path = annotations[11]['raw_file']\n",
    "image = cv2.imread(os.path.join(clips_dir, img_path))\n",
    "\n",
    "\n",
    "# Generate segmentation mask for a given image\n",
    "def generate_seg_mask(ground_truth: dict, image : np.array):\n",
    "    # image_path = ground_truth['raw_file']\n",
    "    # image = cv2.imread(os.path.join(train_img_dir,image_path))\n",
    "    masks = np.zeros_like(image[:,:,0])\n",
    "    nolane_token = -2 \n",
    "    h_vals = ground_truth['h_samples']\n",
    "    lanes = ground_truth['lanes']\n",
    "    lane_val = 255\n",
    "    lane_markings_list = []\n",
    "    for lane in lanes:\n",
    "        x_coords = []\n",
    "        y_coords = []\n",
    "        for i in range(0,len(lane)):             \n",
    "            if lane[i] != nolane_token:\n",
    "                x_coords.append(lane[i])\n",
    "                y_coords.append(h_vals[i])\n",
    "                lane_markings = list(zip(x_coords, y_coords))\n",
    "        lane_markings_list.append(lane_markings)        \n",
    "    for z in lane_markings_list:\n",
    "        for x,y in z:\n",
    "            masks[y,x] = 1\n",
    "    lane_markings_img = cv2.bitwise_and(image, image, mask=masks)\n",
    "    lane_markings_img [lane_markings_img != 0] = lane_val\n",
    "    return lane_markings_img  \n",
    "\n",
    "masked_image = generate_seg_mask(annotations[11], image)\n",
    "\n",
    "# Helper func to display image with OpenCV\n",
    "def disp_img(image, name = 'Image'):\n",
    "    cv2.imshow(name,image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()  \n",
    "    \n",
    "\n",
    "# disp_img(image,'Original Image')\n",
    "disp_img(masked_image,'Original Mask')    \n",
    "\n",
    "resized1 = cv2.resize(image,(640,640), interpolation = cv2.INTER_LINEAR)\n",
    "resized_mask = cv2.resize(masked_image,(640,640), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "# Set all resized pixels color to white (thresholding)\n",
    "resized_mask [resized_mask !=0] = 255\n",
    "\n",
    "# disp_img(resized_mask,'Resized')\n",
    "\n",
    "# concatenate image Horizontally\n",
    "Hori = np.concatenate((resized1, resized_mask), axis=1)\n",
    "\n",
    "disp_img(Hori,'Resized Image/Seg Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lane_pix': [(222, 331), (223, 331), (231, 305), (231, 326), (231, 341), (231, 360), (240, 288), (240, 320), (240, 347), (240, 386), (248, 271), (248, 315), (248, 353), (248, 413), (249, 271), (249, 315), (249, 353), (249, 413), (257, 254), (257, 309), (257, 359), (257, 439), (258, 254), (258, 309), (258, 359), (258, 439), (266, 237), (266, 304), (266, 365), (266, 465), (267, 237), (267, 304), (267, 365), (267, 465), (275, 219), (275, 298), (275, 371), (275, 492), (276, 219), (276, 298), (276, 371), (276, 492), (284, 202), (284, 293), (284, 377), (284, 518), (285, 202), (285, 293), (285, 377), (285, 518), (293, 185), (293, 287), (293, 382), (293, 544), (294, 185), (294, 287), (294, 382), (294, 544), (302, 168), (302, 282), (302, 388), (302, 571), (303, 168), (303, 282), (303, 388), (303, 571), (311, 151), (311, 276), (311, 394), (311, 597), (320, 134), (320, 270), (320, 400), (320, 623), (328, 117), (328, 265), (328, 406), (329, 117), (329, 265), (329, 406), (337, 100), (337, 259), (337, 412), (338, 100), (338, 259), (338, 412), (346, 83), (346, 254), (346, 418), (347, 83), (347, 254), (347, 418), (355, 66), (355, 248), (355, 424), (356, 66), (356, 248), (356, 424), (364, 49), (364, 243), (364, 430), (365, 49), (365, 243), (365, 430), (373, 32), (373, 237), (373, 436), (374, 32), (374, 237), (374, 436), (382, 15), (382, 232), (382, 442), (383, 15), (383, 232), (383, 442), (391, 226), (391, 448), (400, 221), (400, 454), (408, 215), (408, 460), (409, 215), (409, 460), (417, 210), (417, 466), (418, 210), (418, 466), (426, 204), (426, 472), (427, 204), (427, 472), (435, 199), (435, 478), (436, 199), (436, 478), (444, 193), (444, 484), (445, 193), (445, 484), (453, 188), (453, 490), (454, 188), (454, 490), (462, 182), (462, 496), (463, 182), (463, 496), (471, 177), (471, 502), (480, 171), (480, 508), (488, 166), (488, 514), (489, 166), (489, 514), (497, 160), (497, 520), (498, 160), (498, 520), (506, 155), (506, 526), (507, 155), (507, 526), (515, 149), (515, 532), (516, 149), (516, 532), (524, 144), (524, 538), (525, 144), (525, 538), (533, 138), (533, 544), (534, 138), (534, 544), (542, 133), (542, 550), (543, 133), (543, 550), (551, 127), (551, 556), (560, 122), (560, 562), (568, 116), (568, 568), (569, 116), (569, 568), (577, 111), (577, 574), (578, 111), (578, 574), (586, 105), (586, 580), (587, 105), (587, 580), (595, 100), (595, 586), (596, 100), (596, 586), (604, 94), (604, 592), (605, 94), (605, 592), (613, 89), (613, 597), (614, 89), (614, 597), (622, 83), (622, 603), (623, 83), (623, 603), (631, 78), (631, 609)], 'raw_file': 'clips/0531/1492627187263118217/20.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# Get list of lists containing ground truth lane pixel values for all lanes with respect to the original number of lanes in the original gt\n",
    "def get_resized_gt(resized_mask: np.array, original_gt: dict):\n",
    "    lane_pixels = list(np.argwhere(resized_mask == 255)[:,:2])\n",
    "    lane_pixels = [tuple(arr) for arr in lane_pixels]\n",
    "    lane_pixels = list(dict.fromkeys(lane_pixels))\n",
    "    gt_dict = {'lane_pix': lane_pixels ,'raw_file': original_gt['raw_file']}\n",
    "    return gt_dict\n",
    "\n",
    "resized_gt = get_resized_gt(resized_mask, annotations[11])\n",
    "\n",
    "print(resized_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading/Resizing image test scenario\n",
    "img_path = annotations[0]['raw_file']\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize(size=(640,640))])\n",
    "image = cv2.imread(os.path.join(clips_dir, img_path))\n",
    "print(image.shape)\n",
    "image_tensor = train_transforms(image)\n",
    "print(image_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "# EZ convert from normalized tensor to np.array (0,255) scale for RGB images\n",
    "convert = transforms.Compose([transforms.ToPILImage()])\n",
    "array = np.array(convert(image_tensor))\n",
    "print(array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuSimple Dataset loader and pre-processing class\n",
    "# Full Size: Train(3626 clips/ 20 frames per clip/ 20th only is annotated), Test(2782 clips/ 20 frames per clip/ 20th only annotated)\n",
    "# Link: https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection\n",
    "class TuSimple(Dataset):  \n",
    "    def __init__(self, train_annotations : list, train_img_dir: str, resize_to : tuple , subset_size = 0.2, image_size = (1280,720), val_size = 0.15):\n",
    "        self.image_size = image_size\n",
    "        self.resize = resize_to\n",
    "        self.val_size = val_size\n",
    "        self.subset = subset_size\n",
    "        self.train_dir = train_img_dir\n",
    "        self.complete_gt = train_annotations\n",
    "        self.complete_size = len(train_annotations)\n",
    "        self.train_dataset, self.train_gt = self.generate_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if len(self.train_dataset) == len(self.train_gt):\n",
    "            return len(self.train_gt)\n",
    "        else:\n",
    "            return \"Dataset generation failure: Size of training images does not match the existing ground truths.\"\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.train_dataset) == len(self.train_gt):\n",
    "            img_tensor = self.train_dataset[idx]\n",
    "            img_gt = self.train_gt[idx]\n",
    "            return img_tensor, img_gt\n",
    "        else:\n",
    "            return \"The dataset hasn't been constructed properly. Generate again!\"\n",
    "    \n",
    "    # Returns original image size for the dataset    \n",
    "    def get_image_size(self):\n",
    "        return self.image_size\n",
    "        \n",
    "    # Partition dataset according to input subset size and dynamically generate the train/val splits\n",
    "    def generate_dataset(self):\n",
    "        train_set = []\n",
    "        \n",
    "        complete_idx = [idx for idx in range(0, self.complete_size + 1)]\n",
    "        target_samples = int(self.complete_size * self.subset)\n",
    "        # val_samples = int(len(target_samples) * self.val_size)\n",
    "        shuffled = random.sample(complete_idx,len(complete_idx))\n",
    "        \n",
    "        # Pick n (target samples no) idx from the shuffled dataset\n",
    "        dataset_idxs = [shuffled[idx] for idx in range(0, target_samples)]\n",
    "        train_gt = [self.complete_gt[idx] for idx in dataset_idxs]\n",
    "        \n",
    "        # Load images, resize inputs, transform to tensors and generate dataset (or subset)\n",
    "        for gt in train_gt:\n",
    "            img_path = gt['raw_file']\n",
    "            train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                                   transforms.Resize(size = self.resize)])\n",
    "            image = cv2.imread(os.path.join(self.train_dir, img_path))\n",
    "            img_tensor = train_transforms(image)\n",
    "            train_set.append(img_tensor)\n",
    "        \n",
    "        return train_set, train_gt   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
