{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from mask_transformer import MaskTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "    \n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch embedding class\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        try:\n",
    "            B, C, H, W = im.shape\n",
    "        except:\n",
    "            _, H, W = im.shape\n",
    "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "# B-16 ViT Class\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, num_classes=1000, dim=768, depth=12, heads=12, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2\n",
    "\n",
    "        # Define the patch embedding layer\n",
    "        # self.patch_embedding = nn.Conv2d(in_channels=3, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.patch_embedding = PatchEmbedding((self.image_size,self.image_size),self.patch_size,self.dim, 3)\n",
    "        \n",
    "        \n",
    "        # Define the positional embedding layer\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
    "        self.pos_embedding = nn.init.trunc_normal_(self.pos_embedding,std= 0.02)\n",
    "\n",
    "        # Define the transformer layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n",
    "            num_layers=depth\n",
    "        )\n",
    "                \n",
    "        # Initialize patch embedding layer weights\n",
    "        init.normal_(self.patch_embedding.proj.weight, std=0.02)\n",
    "\n",
    "        # Initialize transformer layer weights\n",
    "        for i in range(depth):\n",
    "            layer = self.transformer.layers[i]\n",
    "            # Multi-Head Attention weights\n",
    "            init.normal_(layer.self_attn.in_proj_weight, std=0.02)\n",
    "            init.normal_(layer.self_attn.out_proj.weight, std=0.02)\n",
    "            # Feed-Forward layer weights\n",
    "            init.normal_(layer.linear1.weight, std=0.02)\n",
    "            init.normal_(layer.linear2.weight, std=0.02)\n",
    "            # Layer Normalization weights\n",
    "            init.constant_(layer.norm1.weight, 1)\n",
    "            init.constant_(layer.norm2.weight, 1)\n",
    "\n",
    "    def forward(self, x, return_features = True):\n",
    "        # Apply the patch embedding layer\n",
    "        \n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        # Reshape the patches            \n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Resize positional embeddings\n",
    "        if self.image_size != 224:\n",
    "            resized_size = self.image_size\n",
    "            self.resize_pos_embeds(resized_size)\n",
    "        \n",
    "        \n",
    "        # Add the positional embeddings and use dropout\n",
    "        x = (x.reshape(1, -1, 768) + self.pos_embedding)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply the transformer layers\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Apply layer normalization before returning the transformed features \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "    # Resize pos embeddings functionality for tuning the ViT to accept resized images\n",
    "    def resize_pos_embeds(self, new_image_size):\n",
    "        # Get the original size of the positional embeddings\n",
    "        orig_pos_embeds = self.pos_embedding\n",
    "\n",
    "        # Calculate the number of patches for the new image size\n",
    "        new_num_patches = (new_image_size // self.patch_size) ** 2\n",
    "\n",
    "        # Define the new size of the positional embeddings based on the new number of patches\n",
    "        new_embed_size = (new_num_patches, self.dim)  # Keep the same number of tokens\n",
    "        new_pos_embeds = F.interpolate(orig_pos_embeds.unsqueeze(0), size=new_embed_size).squeeze(0)\n",
    "\n",
    "        # Replace the original positional embeddings with the new ones\n",
    "        self.pos_embedding = nn.Parameter(new_pos_embeds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a ViT model\n",
    "model = ViT(image_size=640, patch_size=16, num_classes=10, dim=768, depth=12, heads=12, mlp_dim=3072, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded pre-trained weights...!\n"
     ]
    }
   ],
   "source": [
    "# Generate the mapping dict renaming the pretrained weights layers names to the desired format\n",
    "def generate_mapping_dict():\n",
    "    mapping = {\n",
    "        'norm.bias' : 'norm.bias',\n",
    "        'norm.weight': 'norm.weight',\n",
    "        'pos_embed': 'pos_embedding',\n",
    "        'patch_embed.proj.bias': 'patch_embedding.proj.bias', \n",
    "        'patch_embed.proj.weight': 'patch_embedding.proj.weight',\n",
    "    }\n",
    "\n",
    "    for i in range(12):\n",
    "        prefix = f'blocks.{i}.'\n",
    "\n",
    "        mapping[f'{prefix}norm1.bias'] = f'transformer.layers.{i}.norm1.weight'\n",
    "        mapping[f'{prefix}norm1.weight'] = f'transformer.layers.{i}.norm1.bias'\n",
    "        mapping[f'{prefix}norm2.bias'] = f'transformer.layers.{i}.norm2.bias'\n",
    "        mapping[f'{prefix}norm2.weight'] = f'transformer.layers.{i}.norm2.weight'\n",
    "        mapping[f'{prefix}mlp.fc1.bias'] = f'transformer.layers.{i}.linear1.bias'\n",
    "        mapping[f'{prefix}mlp.fc1.weight'] = f'transformer.layers.{i}.linear1.weight'\n",
    "        mapping[f'{prefix}mlp.fc2.bias'] = f'transformer.layers.{i}.linear2.bias' \n",
    "        mapping[f'{prefix}mlp.fc2.weight'] = f'transformer.layers.{i}.linear2.weight'\n",
    "        mapping[f'{prefix}attn.proj.bias'] = f'transformer.layers.{i}.self_attn.out_proj.bias'\n",
    "        mapping[f'{prefix}attn.proj.weight'] = f'transformer.layers.{i}.self_attn.out_proj.weight'\n",
    "        mapping[f'{prefix}attn.qkv.bias'] = f'transformer.layers.{i}.self_attn.in_proj_bias'\n",
    "        mapping[f'{prefix}attn.qkv.weight'] = f'transformer.layers.{i}.self_attn.in_proj_weight'\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# Resize the pretrained positional embeddings to desired dimensions\n",
    "def resizing_pos_pretrained(pretrained_dict, model_dict):\n",
    "    pretrained_pos_embedding = pretrained_dict['pos_embedding']\n",
    "    new_pos_embedding = torch.zeros_like(model_dict['pos_embedding'])\n",
    "    new_pos_embedding[:, :pretrained_pos_embedding.shape[1], :] = pretrained_pos_embedding\n",
    "    \n",
    "    # update the state_dict with the resized pos_embedding tensor\n",
    "    pretrained_dict['pos_embedding'] = new_pos_embedding\n",
    "    return pretrained_dict\n",
    "\n",
    "# Load pre-trained weights from file \n",
    "def load_pretrained_weigths(model, pretrained_path):\n",
    "    map_dict = generate_mapping_dict()\n",
    "    pretrained = torch.load(pretrained_path)\n",
    "    model_state_dict = model.state_dict()\n",
    "    \n",
    "    # create new state dict with mapped keys\n",
    "    new_state_dict = {}\n",
    "    for key in pretrained:\n",
    "        if key in map_dict:\n",
    "            new_state_dict[map_dict[key]] = pretrained[key]\n",
    "        else:\n",
    "            if key in model_state_dict:\n",
    "                new_state_dict[key] = pretrained[key]\n",
    "    \n",
    "    # Test and see if resizing these is a good idea else keep the original randomly initialized weights\n",
    "    new_state_dict = resizing_pos_pretrained(new_state_dict,model_state_dict)            \n",
    "\n",
    "    # Load the mapped weights into our ViT model\n",
    "    model_state_dict.update(new_state_dict)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    print('Succesfully loaded pre-trained weights...!')\n",
    "    return model\n",
    "\n",
    "    \n",
    "\n",
    "model_with_pretrained = load_pretrained_weigths(model,'../pre-trained/jx_vit_base_p16_224-80ecf9dd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.05)\n",
    "    \n",
    "img_tns, gt = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt['gt_tensor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1600, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_test = model(img_tns)\n",
    "encoder_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_trans = MaskTransformer()\n",
    "result = masks_trans(encoder_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 40, 40])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = F.interpolate(result, size=(640, 640), mode=\"bilinear\")\n",
    "predicted_mask = torch.argmax(mask, dim=1).float()\n",
    "predicted_mask.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand the output mask tensor along the channel dimension to match the ground truth tensor\n",
    "predicted_mask = predicted_mask.unsqueeze(0).expand(3, -1, -1, -1).transpose(0, 1).squeeze(0)\n",
    "predicted_mask = dataset.toImagearr(predicted_mask)\n",
    "predicted_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot_img_gt(img_tns,predicted_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
