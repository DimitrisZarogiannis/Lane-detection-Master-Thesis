{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/dimitris/anaconda3/envs/py10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Set seed for randomize functions (Ez reproduction of results)\n",
    "random.seed(100)\n",
    "\n",
    "# Import TuSimple loader\n",
    "import sys\n",
    "sys.path.insert(0,'../resources/')\n",
    "from tusimple import TuSimple\n",
    "from mask_transformer import MaskTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT DIRECTORIES\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "annotated_dir = os.path.join(root_dir,'datasets/tusimple/train_set/annotations')\n",
    "clips_dir = os.path.join(root_dir,'datasets/tusimple/train_set/')\n",
    "annotated = os.listdir(annotated_dir)\n",
    "    \n",
    "# Get path directories for clips and annotations for the TUSimple dataset + ground truth dictionary\n",
    "annotations = list()\n",
    "for gt_file in annotated:\n",
    "    path = os.path.join(annotated_dir,gt_file)\n",
    "    json_gt = [json.loads(line) for line in open(path)]\n",
    "    annotations.append(json_gt)\n",
    "    \n",
    "annotations = [a for f in annotations for a in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the mapping dict renaming the pretrained weights layers names to the desired format\n",
    "def generate_mapping_dict():\n",
    "    mapping = {\n",
    "        'norm.bias' : 'norm.bias',\n",
    "        'norm.weight': 'norm.weight',\n",
    "        'pos_embed': 'pos_embedding',\n",
    "        'patch_embed.proj.bias': 'patch_embedding.proj.bias', \n",
    "        'patch_embed.proj.weight': 'patch_embedding.proj.weight',\n",
    "    }\n",
    "\n",
    "    for i in range(12):\n",
    "        prefix = f'blocks.{i}.'\n",
    "\n",
    "        mapping[f'{prefix}norm1.bias'] = f'transformer.layers.{i}.norm1.bias'\n",
    "        mapping[f'{prefix}norm1.weight'] = f'transformer.layers.{i}.norm1.weight'\n",
    "        mapping[f'{prefix}norm2.bias'] = f'transformer.layers.{i}.norm2.bias'\n",
    "        mapping[f'{prefix}norm2.weight'] = f'transformer.layers.{i}.norm2.weight'\n",
    "        mapping[f'{prefix}mlp.fc1.bias'] = f'transformer.layers.{i}.linear1.bias'\n",
    "        mapping[f'{prefix}mlp.fc1.weight'] = f'transformer.layers.{i}.linear1.weight'\n",
    "        mapping[f'{prefix}mlp.fc2.bias'] = f'transformer.layers.{i}.linear2.bias' \n",
    "        mapping[f'{prefix}mlp.fc2.weight'] = f'transformer.layers.{i}.linear2.weight'\n",
    "        mapping[f'{prefix}attn.proj.bias'] = f'transformer.layers.{i}.self_attn.out_proj.bias'\n",
    "        mapping[f'{prefix}attn.proj.weight'] = f'transformer.layers.{i}.self_attn.out_proj.weight'\n",
    "        mapping[f'{prefix}attn.qkv.bias'] = f'transformer.layers.{i}.self_attn.in_proj_bias'\n",
    "        mapping[f'{prefix}attn.qkv.weight'] = f'transformer.layers.{i}.self_attn.in_proj_weight'\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# Resize the pretrained positional embeddings to desired dimensions\n",
    "def resize_pretrained_pos(pretrained_dict, new_num_patches):\n",
    "    pretrained_pos_embedding = pretrained_dict['pos_embedding']\n",
    "\n",
    "    # Scale the positional embeddings by the ratio\n",
    "    scaled_pos_embedding = F.interpolate(pretrained_pos_embedding.unsqueeze(0),\n",
    "                                         size=(new_num_patches, pretrained_pos_embedding.shape[2]),\n",
    "                                         mode='nearest').squeeze(0)\n",
    "\n",
    "    # Create a new dictionary with the updated pos_embedding tensor\n",
    "    pretrained_dict['pos_embedding'] = scaled_pos_embedding\n",
    "\n",
    "    return pretrained_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch embedding class\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        try:\n",
    "            B, C, H, W = im.shape\n",
    "        except:\n",
    "            _, H, W = im.shape\n",
    "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "# B-16 ViT Class\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, num_classes=1000, dim=768, depth=12, heads=12, \n",
    "                 mlp_dim=3072, dropout=0.1,load_pre = False, pre_trained_path = None):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2\n",
    "\n",
    "        # Define the patch embedding layer\n",
    "        # self.patch_embedding = nn.Conv2d(in_channels=3, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.patch_embedding = PatchEmbedding((self.image_size,self.image_size),self.patch_size,self.dim, 3)\n",
    "        \n",
    "        \n",
    "        # Define the positional embedding layer\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
    "        self.pos_embedding = nn.init.trunc_normal_(self.pos_embedding,std= 0.02)\n",
    "        \n",
    "        \n",
    "        # Define the transformer layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        \n",
    "        # Load model with pre-trained weights if flas is true else initiallize randomly\n",
    "        if load_pre:\n",
    "            self.load_pretrained_weights(pre_trained_path)\n",
    "        else:\n",
    "            # Randomly initialize patch embedding layer weights\n",
    "            init.normal_(self.patch_embedding.proj.weight, std=0.02)\n",
    "\n",
    "            # Initialize transformer layer weights\n",
    "            for i in range(depth):\n",
    "                layer = self.transformer.layers[i]\n",
    "                # Multi-Head Attention weights\n",
    "                init.normal_(layer.self_attn.in_proj_weight, std=0.02)\n",
    "                init.normal_(layer.self_attn.out_proj.weight, std=0.02)\n",
    "                # Feed-Forward layer weights\n",
    "                init.normal_(layer.linear1.weight, std=0.02)\n",
    "                init.normal_(layer.linear2.weight, std=0.02)\n",
    "                # Layer Normalization weights\n",
    "                init.constant_(layer.norm1.weight, 1)\n",
    "                init.constant_(layer.norm2.weight, 1)\n",
    "\n",
    "    def forward(self, x, return_features = True):\n",
    "        # Apply the patch embedding layer\n",
    "        \n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        # Reshape the patches            \n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "                    \n",
    "        # Dynamically expand pos embed across batch dimension\n",
    "        if self.training:\n",
    "            pos_embedding = nn.Parameter(self.pos_embedding.expand(x.shape[0], -1, -1))\n",
    "            # Add the positional embeddings and use dropout\n",
    "            x = (x.reshape(x.shape[0], -1, 768) + pos_embedding)\n",
    "            x = self.dropout(x)\n",
    "        else:\n",
    "            pos_embedding = self.pos_embedding\n",
    "            # Add the positional embeddings and use dropout\n",
    "            x = (x.reshape(1, -1, 768) + pos_embedding)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Apply the transformer layers\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Apply layer normalization before returning the transformed features \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "    # Resize pos embeddings functionality for tuning the ViT to accept resized images (probably useless code)\n",
    "    # def resize_pos_embeds(self, new_image_size):\n",
    "    #     # Get the original size of the positional embeddings\n",
    "    #     orig_pos_embeds = self.pos_embedding\n",
    "\n",
    "    #     # Calculate the number of patches for the new image size\n",
    "    #     new_num_patches = (new_image_size // self.patch_size) ** 2\n",
    "\n",
    "    #     # Define the new size of the positional embeddings based on the new number of patches\n",
    "    #     new_embed_size = (new_num_patches, self.dim)  # Keep the same number of tokens\n",
    "    #     new_pos_embeds = F.interpolate(orig_pos_embeds.unsqueeze(0), size=new_embed_size).squeeze(0)\n",
    "\n",
    "    #     # Replace the original positional embeddings with the new ones\n",
    "    #     self.pos_embedding = nn.Parameter(new_pos_embeds)\n",
    "        \n",
    "    # Load pre-trained weights method\n",
    "    def load_pretrained_weights(self, pretrained_path):\n",
    "        map_dict = generate_mapping_dict()\n",
    "        pretrained = torch.load(pretrained_path)\n",
    "        model_state_dict = self.state_dict()\n",
    "    \n",
    "        # create new state dict with mapped keys\n",
    "        new_state_dict = {}\n",
    "        for key in pretrained:\n",
    "            if key in map_dict:\n",
    "                new_state_dict[map_dict[key]] = pretrained[key]\n",
    "            else:\n",
    "                if key in model_state_dict:\n",
    "                    new_state_dict[key] = pretrained[key]\n",
    "\n",
    "         # Test and see if resizing these is a good idea else keep the original randomly initialized weights\n",
    "        new_state_dict = resize_pretrained_pos(new_state_dict, new_num_patches= self.num_patches) \n",
    "        \n",
    "        # Load the mapped weights into our ViT model\n",
    "        self.load_state_dict(new_state_dict, strict= False)\n",
    "        print('Succesfully created ViT with pre-trained weights...!')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a ViT model with pre-trained weights\n",
    "model = ViT(image_size=640, patch_size=16, num_classes=2, dim=768, depth=12, heads=12, \n",
    "            mlp_dim=3072, dropout=0.1, load_pre= True, pre_trained_path= '../pre-trained/jx_vit_base_p16_224-80ecf9dd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TuSimple(train_annotations = annotations, train_img_dir = clips_dir, resize_to = (640,640), subset_size = 0.001)\n",
    "    \n",
    "img_tns, gt = dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
